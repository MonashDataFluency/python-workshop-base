{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to Python Workshop Welcome to Introduction to Python ! Modules 01 - Introduction - the basics of Python 02 - Data analysis in Python with Pandas 03 - Indexing and slicing 04 - Missing Values 05 - Combining DataFrames in Pandas 06 - Repetitive tasks with loops 07 - Plotting with plotnine (ggplot) 08 - Reusable and modular code with functions 09 - Defensive Programming Some of these modules have been adapted from the original versions at Data Carpentry - Python for Ecologists and Software Carpentry - Programming with Python (used under a CC-BY 4.0 license ).","title":"Home"},{"location":"#introduction-to-python-workshop","text":"Welcome to Introduction to Python !","title":"Introduction to Python Workshop"},{"location":"#modules","text":"01 - Introduction - the basics of Python 02 - Data analysis in Python with Pandas 03 - Indexing and slicing 04 - Missing Values 05 - Combining DataFrames in Pandas 06 - Repetitive tasks with loops 07 - Plotting with plotnine (ggplot) 08 - Reusable and modular code with functions 09 - Defensive Programming Some of these modules have been adapted from the original versions at Data Carpentry - Python for Ecologists and Software Carpentry - Programming with Python (used under a CC-BY 4.0 license ).","title":"Modules"},{"location":"fullday/","text":"Introduction to Python Workshop Welcome to Introduction to Python ! Sections 01 - Introduction - the basics of Python 02 - Repetitive tasks with loops 03 - Data analysis in Python with Pandas 04 - Reusable and modular code with functions 05 - Handling Missing Values 06 - Plotting with plotnine (ggplot)","title":"Full Day Course"},{"location":"fullday/#introduction-to-python-workshop","text":"Welcome to Introduction to Python !","title":"Introduction to Python Workshop"},{"location":"fullday/#sections","text":"01 - Introduction - the basics of Python 02 - Repetitive tasks with loops 03 - Data analysis in Python with Pandas 04 - Reusable and modular code with functions 05 - Handling Missing Values 06 - Plotting with plotnine (ggplot)","title":"Sections"},{"location":"halfday/","text":"Introduction to Python Workshop (half-day) Welcome to Introduction to Python ! Sections 01 - Introduction - the basics of Python 02 - Data analysis in Python with Pandas 03 - Missing Values 04 - Repetitive tasks with loops 05 - Plotting with plotnine (ggplot)","title":"Half Day Course"},{"location":"halfday/#introduction-to-python-workshop-half-day","text":"Welcome to Introduction to Python !","title":"Introduction to Python Workshop (half-day)"},{"location":"halfday/#sections","text":"01 - Introduction - the basics of Python 02 - Data analysis in Python with Pandas 03 - Missing Values 04 - Repetitive tasks with loops 05 - Plotting with plotnine (ggplot)","title":"Sections"},{"location":"modules/defensive_programming/","text":".output_label { text-align: right; margin: -1em; padding: 0; font-size: 0.5em; color: grey } Defensive Programming Our previous lessons have introduced the basic tools of programming: variables and lists, file operations, data visualisation, loops, conditionals, and functions. What they haven\u2019t done is show us how to tell whether a program is getting the right answer, and how to tell if it\u2019s still getting the right answer as we make changes to it. To achieve that, we need to: Write programs that check their own operation. Write and run tests for widely-used functions. Make sure we know what \u201ccorrect\u201d actually means. The good news is, doing these things will speed up our programming, not slow it down. As in real carpentry \u2014 the kind done with lumber \u2014 the time saved by measuring carefully before cutting a piece of wood is much greater than the time that measuring takes. Assertions The first step toward getting the right answers from our programs is to assume that mistakes will happen and to guard against them. This is called defensive programming, and the most common way to do it is to add assertions to our code so that it checks itself as it runs. An assertion is simply a statement that something must be true at a certain point in a program. When Python sees one, it evaluates the assertion\u2019s condition. If it\u2019s true, Python does nothing, but if it\u2019s false, Python halts the program immediately and prints the error message if one is provided. For example, this piece of code halts as soon as the loop encounters a value that isn\u2019t positive: numbers = [1.5, 2.3, 0.7, -0.001, 4.4] total = 0.0 for n in numbers: assert n > 0.0, 'Data should only contain positive values' total += n print('total is:', total) --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-1-091518d2f2e2> in <module>() 3 total = 0.0 4 for n in numbers: ----> 5 assert n > 0.0, 'Data should only contain positive values' 6 total += n 7 print('total is:', total) AssertionError: Data should only contain positive values Programs like the Firefox browser are full of assertions: 10-20% of the code they contain are there to check that the other 80\u201390% are working correctly. Broadly speaking, assertions fall into three categories: A precondition is something that must be true at the start of a function in order for it to work correctly. A postcondition is something that the function guarantees is true when it finishes. An invariant is something that is always true at a particular point inside a piece of code. For example, suppose we are representing rectangles using a tuple of four coordinates (x0, y0, x1, y1) , representing the lower left and upper right corners of the rectangle. In order to do some calculations, we need to normalize the rectangle so that the lower left corner is at the origin and the longest side is 1.0 units long. This function does that, but checks that its input is correctly formatted and that its result makes sense: def normalize_rectangle(rect): '''Normalizes a rectangle so that it is at the origin and 1.0 units long on its longest axis. Input should be of the format (x0, y0, x1, y1). (x0, y0) and (x1, y1) define the lower left and upper right corners of the rectangle, respectively.''' assert len(rect) == 4, 'Rectangles must contain 4 coordinates' x0, y0, x1, y1 = rect assert x0 < x1, 'Invalid X coordinates' assert y0 < y1, 'Invalid Y coordinates' dx = x1 - x0 dy = y1 - y0 if dx > dy: scaled = float(dx) / dy upper_x, upper_y = 1.0, scaled else: scaled = float(dx) / dy upper_x, upper_y = scaled, 1.0 assert 0 < upper_x <= 1.0, 'Calculated upper X coordinate invalid' assert 0 < upper_y <= 1.0, 'Calculated upper Y coordinate invalid' return (0, 0, upper_x, upper_y) The preconditions on lines 3, 5, and 6 catch invalid inputs: print(normalize_rectangle( (0.0, 1.0, 2.0) )) # missing the fourth coordinate --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-1b9cd8e18a1f> in <module>() ----> 1 print(normalize_rectangle( (0.0, 1.0, 2.0) )) # missing the fourth coordinate <ipython-input-2-c94cf5b065b9> in normalize_rectangle(rect) 4 (x0, y0) and (x1, y1) define the lower left and upper right corners 5 of the rectangle, respectively.''' ----> 6 assert len(rect) == 4, 'Rectangles must contain 4 coordinates' 7 x0, y0, x1, y1 = rect 8 assert x0 < x1, 'Invalid X coordinates' AssertionError: Rectangles must contain 4 coordinates print(normalize_rectangle( (4.0, 2.0, 1.0, 5.0) )) # X axis inverted --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-4-325036405532> in <module>() ----> 1 print(normalize_rectangle( (4.0, 2.0, 1.0, 5.0) )) # X axis inverted <ipython-input-2-c94cf5b065b9> in normalize_rectangle(rect) 6 assert len(rect) == 4, 'Rectangles must contain 4 coordinates' 7 x0, y0, x1, y1 = rect ----> 8 assert x0 < x1, 'Invalid X coordinates' 9 assert y0 < y1, 'Invalid Y coordinates' 10 AssertionError: Invalid X coordinates The post-conditions on lines 17 and 18 help us catch bugs by telling us when our calculations cannot have been correct. For example, if we normalize a rectangle that is taller than it is wide everything seems OK: print(normalize_rectangle( (0.0, 0.0, 1.0, 5.0) )) output (0, 0, 0.2, 1.0) but if we normalize one that\u2019s wider than it is tall, the assertion is triggered: print(normalize_rectangle( (0.0, 0.0, 5.0, 1.0) )) --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-6-8d4a48f1d068> in <module>() ----> 1 print(normalize_rectangle( (0.0, 0.0, 5.0, 1.0) )) <ipython-input-2-c94cf5b065b9> in normalize_rectangle(rect) 19 20 assert 0 < upper_x <= 1.0, 'Calculated upper X coordinate invalid' ---> 21 assert 0 < upper_y <= 1.0, 'Calculated upper Y coordinate invalid' 22 23 return (0, 0, upper_x, upper_y) AssertionError: Calculated upper Y coordinate invalid Re-reading our function, we realize that line 11 should divide dy by dx rather than dx by dy . If we had left out the assertion at the end of the function, we would have created and returned something that had the right shape as a valid answer, but wasn\u2019t. Detecting and debugging that would almost certainly have taken more time in the long run than writing the assertion. But assertions aren\u2019t just about catching errors: they also help people understand programs. Each assertion gives the person reading the program a chance to check (consciously or otherwise) that their understanding matches what the code is doing. Most good programmers follow two rules when adding assertions to their code. The first is, fail early, fail often. The greater the distance between when and where an error occurs and when it\u2019s noticed, the harder the error will be to debug, so good code catches mistakes as early as possible. The second rule is, turn bugs into assertions or tests. Whenever you fix a bug, write an assertion that catches the mistake should you make it again. If you made a mistake in a piece of code, the odds are good that you have made other mistakes nearby, or will make the same mistake (or a related one) the next time you change it. Writing assertions to check that you haven\u2019t regressed (i.e., haven\u2019t re-introduced an old problem) can save a lot of time in the long run, and helps to warn people who are reading the code (including your future self) that this bit is tricky. Test-Driven Development An assertion checks that something is true at a particular point in the program. The next step is to check the overall behavior of a piece of code, i.e., to make sure that it produces the right output when it\u2019s given a particular input. For example, suppose we need to find where two or more time series overlap. The range of each time series is represented as a pair of numbers, which are the time the interval started and ended. The output is the largest range that they all include: Most novice programmers would solve this problem like this: Write a function range_overlap . Call it interactively on two or three different inputs. If it produces the wrong answer, fix the function and re-run that test. This clearly works \u2014 after all, thousands of scientists are doing it right now \u2014 but there\u2019s a better way: Write a short function for each test. Write a range_overlap function that should pass those tests. If range_overlap produces any wrong answers, fix it and re-run the test functions. Writing the tests before writing the function they exercise is called test-driven development (TDD). Its advocates believe it produces better code faster because: If people write tests after writing the thing to be tested, they are subject to confirmation bias, i.e., they subconsciously write tests to show that their code is correct, rather than to find errors. Writing tests helps programmers figure out what the function is actually supposed to do. Here are three test functions for range_overlap : assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0) assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0) assert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-9-dc16b942c085> in <module>() ----> 1 assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0) 2 assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0) 3 assert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0) NameError: name 'range_overlap' is not defined The error is actually reassuring: we haven\u2019t written range_overlap yet, so if the tests passed, it would be a sign that someone else had and that we were accidentally using their function. And as a bonus of writing these tests, we\u2019ve implicitly defined what our input and output look like: we expect a list of pairs as input, and produce a single pair as output. Something important is missing, though. We don\u2019t have any tests for the case where the ranges don\u2019t overlap at all: assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == ??? What should range_overlap do in this case: fail with an error message, produce a special value like (0.0, 0.0) to signal that there\u2019s no overlap, or something else? Any actual implementation of the function will do one of these things; writing the tests first helps us figure out which is best before we\u2019re emotionally invested in whatever we happened to write before we realized there was an issue. And what about this case? assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == ??? Do two segments that touch at their endpoints overlap or not? Mathematicians usually say \u201cyes\u201d, but engineers usually say \u201cno\u201d. The best answer is \u201cwhatever is most useful in the rest of our program\u201d, but again, any actual implementation of range_overlap is going to do something , and whatever it is ought to be consistent with what it does when there\u2019s no overlap at all. Since we\u2019re planning to use the range this function returns as the X axis in a time series chart, we decide that: every overlap has to have non-zero width, and we will return the special value None when there\u2019s no overlap. None is built into Python, and means \u201cnothing here\u201d. (Other languages often call the equivalent value null or nil ). With that decision made, we can finish writing our last two tests: assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-13-42de7ddfb428> in <module>() ----> 1 assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None 2 assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None NameError: name 'range_overlap' is not defined Again, we get an error because we haven\u2019t written our function, but we\u2019re now ready to do so: def range_overlap(ranges): '''Return common overlap among a set of [low, high] ranges.''' lowest = 0.0 highest = 1.0 for (low, high) in ranges: lowest = max(lowest, low) highest = min(highest, high) return (lowest, highest) (Take a moment to think about why we use max to raise lowest and min to lower highest ). We\u2019d now like to re-run our tests, but they\u2019re scattered across three different cells. To make running them easier, let\u2019s put them all in a function: def test_range_overlap(): assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0) assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0) assert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0) We can now test range_overlap with a single function call: test_range_overlap() --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-16-80290759369d> in <module>() ----> 1 test_range_overlap() <ipython-input-15-d61f343ad67a> in test_range_overlap() 1 def test_range_overlap(): ----> 2 assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None 3 assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None 4 assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0) 5 assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0) AssertionError: The first test that was supposed to produce None fails, so we know something is wrong with our function. We don\u2019t know whether the other tests passed or failed because Python halted the program as soon as it spotted the first error. Still, some information is better than none, and if we trace the behavior of the function with that input, we realize that we\u2019re initializing lowest and highest to 0.0 and 1.0 respectively, regardless of the input values. This violates another important rule of programming: always initialize from data. Fix range_overlap . Re-run test_range_overlap after each change you make. Key points Program defensively, i.e., assume that errors are going to arise, and write code to detect them when they do. Put assertions in programs to check their state as they run, and to help readers understand how those programs are supposed to work. Use preconditions to check that the inputs to a function are safe to use. Use postconditions to check that the output from a function is safe to use. Write tests before writing code in order to help determine exactly what that code is supposed to do.","title":"Defensive Programming"},{"location":"modules/defensive_programming/#defensive-programming","text":"Our previous lessons have introduced the basic tools of programming: variables and lists, file operations, data visualisation, loops, conditionals, and functions. What they haven\u2019t done is show us how to tell whether a program is getting the right answer, and how to tell if it\u2019s still getting the right answer as we make changes to it. To achieve that, we need to: Write programs that check their own operation. Write and run tests for widely-used functions. Make sure we know what \u201ccorrect\u201d actually means. The good news is, doing these things will speed up our programming, not slow it down. As in real carpentry \u2014 the kind done with lumber \u2014 the time saved by measuring carefully before cutting a piece of wood is much greater than the time that measuring takes.","title":"Defensive Programming"},{"location":"modules/defensive_programming/#assertions","text":"The first step toward getting the right answers from our programs is to assume that mistakes will happen and to guard against them. This is called defensive programming, and the most common way to do it is to add assertions to our code so that it checks itself as it runs. An assertion is simply a statement that something must be true at a certain point in a program. When Python sees one, it evaluates the assertion\u2019s condition. If it\u2019s true, Python does nothing, but if it\u2019s false, Python halts the program immediately and prints the error message if one is provided. For example, this piece of code halts as soon as the loop encounters a value that isn\u2019t positive: numbers = [1.5, 2.3, 0.7, -0.001, 4.4] total = 0.0 for n in numbers: assert n > 0.0, 'Data should only contain positive values' total += n print('total is:', total) --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-1-091518d2f2e2> in <module>() 3 total = 0.0 4 for n in numbers: ----> 5 assert n > 0.0, 'Data should only contain positive values' 6 total += n 7 print('total is:', total) AssertionError: Data should only contain positive values Programs like the Firefox browser are full of assertions: 10-20% of the code they contain are there to check that the other 80\u201390% are working correctly. Broadly speaking, assertions fall into three categories: A precondition is something that must be true at the start of a function in order for it to work correctly. A postcondition is something that the function guarantees is true when it finishes. An invariant is something that is always true at a particular point inside a piece of code. For example, suppose we are representing rectangles using a tuple of four coordinates (x0, y0, x1, y1) , representing the lower left and upper right corners of the rectangle. In order to do some calculations, we need to normalize the rectangle so that the lower left corner is at the origin and the longest side is 1.0 units long. This function does that, but checks that its input is correctly formatted and that its result makes sense: def normalize_rectangle(rect): '''Normalizes a rectangle so that it is at the origin and 1.0 units long on its longest axis. Input should be of the format (x0, y0, x1, y1). (x0, y0) and (x1, y1) define the lower left and upper right corners of the rectangle, respectively.''' assert len(rect) == 4, 'Rectangles must contain 4 coordinates' x0, y0, x1, y1 = rect assert x0 < x1, 'Invalid X coordinates' assert y0 < y1, 'Invalid Y coordinates' dx = x1 - x0 dy = y1 - y0 if dx > dy: scaled = float(dx) / dy upper_x, upper_y = 1.0, scaled else: scaled = float(dx) / dy upper_x, upper_y = scaled, 1.0 assert 0 < upper_x <= 1.0, 'Calculated upper X coordinate invalid' assert 0 < upper_y <= 1.0, 'Calculated upper Y coordinate invalid' return (0, 0, upper_x, upper_y) The preconditions on lines 3, 5, and 6 catch invalid inputs: print(normalize_rectangle( (0.0, 1.0, 2.0) )) # missing the fourth coordinate --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-3-1b9cd8e18a1f> in <module>() ----> 1 print(normalize_rectangle( (0.0, 1.0, 2.0) )) # missing the fourth coordinate <ipython-input-2-c94cf5b065b9> in normalize_rectangle(rect) 4 (x0, y0) and (x1, y1) define the lower left and upper right corners 5 of the rectangle, respectively.''' ----> 6 assert len(rect) == 4, 'Rectangles must contain 4 coordinates' 7 x0, y0, x1, y1 = rect 8 assert x0 < x1, 'Invalid X coordinates' AssertionError: Rectangles must contain 4 coordinates print(normalize_rectangle( (4.0, 2.0, 1.0, 5.0) )) # X axis inverted --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-4-325036405532> in <module>() ----> 1 print(normalize_rectangle( (4.0, 2.0, 1.0, 5.0) )) # X axis inverted <ipython-input-2-c94cf5b065b9> in normalize_rectangle(rect) 6 assert len(rect) == 4, 'Rectangles must contain 4 coordinates' 7 x0, y0, x1, y1 = rect ----> 8 assert x0 < x1, 'Invalid X coordinates' 9 assert y0 < y1, 'Invalid Y coordinates' 10 AssertionError: Invalid X coordinates The post-conditions on lines 17 and 18 help us catch bugs by telling us when our calculations cannot have been correct. For example, if we normalize a rectangle that is taller than it is wide everything seems OK: print(normalize_rectangle( (0.0, 0.0, 1.0, 5.0) )) output (0, 0, 0.2, 1.0) but if we normalize one that\u2019s wider than it is tall, the assertion is triggered: print(normalize_rectangle( (0.0, 0.0, 5.0, 1.0) )) --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-6-8d4a48f1d068> in <module>() ----> 1 print(normalize_rectangle( (0.0, 0.0, 5.0, 1.0) )) <ipython-input-2-c94cf5b065b9> in normalize_rectangle(rect) 19 20 assert 0 < upper_x <= 1.0, 'Calculated upper X coordinate invalid' ---> 21 assert 0 < upper_y <= 1.0, 'Calculated upper Y coordinate invalid' 22 23 return (0, 0, upper_x, upper_y) AssertionError: Calculated upper Y coordinate invalid Re-reading our function, we realize that line 11 should divide dy by dx rather than dx by dy . If we had left out the assertion at the end of the function, we would have created and returned something that had the right shape as a valid answer, but wasn\u2019t. Detecting and debugging that would almost certainly have taken more time in the long run than writing the assertion. But assertions aren\u2019t just about catching errors: they also help people understand programs. Each assertion gives the person reading the program a chance to check (consciously or otherwise) that their understanding matches what the code is doing. Most good programmers follow two rules when adding assertions to their code. The first is, fail early, fail often. The greater the distance between when and where an error occurs and when it\u2019s noticed, the harder the error will be to debug, so good code catches mistakes as early as possible. The second rule is, turn bugs into assertions or tests. Whenever you fix a bug, write an assertion that catches the mistake should you make it again. If you made a mistake in a piece of code, the odds are good that you have made other mistakes nearby, or will make the same mistake (or a related one) the next time you change it. Writing assertions to check that you haven\u2019t regressed (i.e., haven\u2019t re-introduced an old problem) can save a lot of time in the long run, and helps to warn people who are reading the code (including your future self) that this bit is tricky.","title":"Assertions"},{"location":"modules/defensive_programming/#test-driven-development","text":"An assertion checks that something is true at a particular point in the program. The next step is to check the overall behavior of a piece of code, i.e., to make sure that it produces the right output when it\u2019s given a particular input. For example, suppose we need to find where two or more time series overlap. The range of each time series is represented as a pair of numbers, which are the time the interval started and ended. The output is the largest range that they all include: Most novice programmers would solve this problem like this: Write a function range_overlap . Call it interactively on two or three different inputs. If it produces the wrong answer, fix the function and re-run that test. This clearly works \u2014 after all, thousands of scientists are doing it right now \u2014 but there\u2019s a better way: Write a short function for each test. Write a range_overlap function that should pass those tests. If range_overlap produces any wrong answers, fix it and re-run the test functions. Writing the tests before writing the function they exercise is called test-driven development (TDD). Its advocates believe it produces better code faster because: If people write tests after writing the thing to be tested, they are subject to confirmation bias, i.e., they subconsciously write tests to show that their code is correct, rather than to find errors. Writing tests helps programmers figure out what the function is actually supposed to do. Here are three test functions for range_overlap : assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0) assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0) assert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-9-dc16b942c085> in <module>() ----> 1 assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0) 2 assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0) 3 assert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0) NameError: name 'range_overlap' is not defined The error is actually reassuring: we haven\u2019t written range_overlap yet, so if the tests passed, it would be a sign that someone else had and that we were accidentally using their function. And as a bonus of writing these tests, we\u2019ve implicitly defined what our input and output look like: we expect a list of pairs as input, and produce a single pair as output. Something important is missing, though. We don\u2019t have any tests for the case where the ranges don\u2019t overlap at all: assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == ??? What should range_overlap do in this case: fail with an error message, produce a special value like (0.0, 0.0) to signal that there\u2019s no overlap, or something else? Any actual implementation of the function will do one of these things; writing the tests first helps us figure out which is best before we\u2019re emotionally invested in whatever we happened to write before we realized there was an issue. And what about this case? assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == ??? Do two segments that touch at their endpoints overlap or not? Mathematicians usually say \u201cyes\u201d, but engineers usually say \u201cno\u201d. The best answer is \u201cwhatever is most useful in the rest of our program\u201d, but again, any actual implementation of range_overlap is going to do something , and whatever it is ought to be consistent with what it does when there\u2019s no overlap at all. Since we\u2019re planning to use the range this function returns as the X axis in a time series chart, we decide that: every overlap has to have non-zero width, and we will return the special value None when there\u2019s no overlap. None is built into Python, and means \u201cnothing here\u201d. (Other languages often call the equivalent value null or nil ). With that decision made, we can finish writing our last two tests: assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-13-42de7ddfb428> in <module>() ----> 1 assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None 2 assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None NameError: name 'range_overlap' is not defined Again, we get an error because we haven\u2019t written our function, but we\u2019re now ready to do so: def range_overlap(ranges): '''Return common overlap among a set of [low, high] ranges.''' lowest = 0.0 highest = 1.0 for (low, high) in ranges: lowest = max(lowest, low) highest = min(highest, high) return (lowest, highest) (Take a moment to think about why we use max to raise lowest and min to lower highest ). We\u2019d now like to re-run our tests, but they\u2019re scattered across three different cells. To make running them easier, let\u2019s put them all in a function: def test_range_overlap(): assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0) assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0) assert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0) We can now test range_overlap with a single function call: test_range_overlap() --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-16-80290759369d> in <module>() ----> 1 test_range_overlap() <ipython-input-15-d61f343ad67a> in test_range_overlap() 1 def test_range_overlap(): ----> 2 assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None 3 assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None 4 assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0) 5 assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0) AssertionError: The first test that was supposed to produce None fails, so we know something is wrong with our function. We don\u2019t know whether the other tests passed or failed because Python halted the program as soon as it spotted the first error. Still, some information is better than none, and if we trace the behavior of the function with that input, we realize that we\u2019re initializing lowest and highest to 0.0 and 1.0 respectively, regardless of the input values. This violates another important rule of programming: always initialize from data. Fix range_overlap . Re-run test_range_overlap after each change you make.","title":"Test-Driven Development"},{"location":"modules/defensive_programming/#key-points","text":"Program defensively, i.e., assume that errors are going to arise, and write code to detect them when they do. Put assertions in programs to check their state as they run, and to help readers understand how those programs are supposed to work. Use preconditions to check that the inputs to a function are safe to use. Use postconditions to check that the output from a function is safe to use. Write tests before writing code in order to help determine exactly what that code is supposed to do.","title":"Key points"},{"location":"modules/functions/","text":".output_label { text-align: right; margin: -1em; padding: 0; font-size: 0.5em; color: grey } Reusable and modular code with functions Functions Functions wrap up reusable pieces of code - they help you apply the Do Not Repeat Yourself (DRY) principle. Suppose that separating large data files into individual yearly files is a task that we frequently have to perform. We could write a for loop like the one above every time we needed to do it but that would be time consuming and error prone. A more elegant solution would be to create a reusable tool that performs this task with minimum input from the user. To do this, we are going to turn the code we\u2019ve already written into a function. Functions are reusable, self-contained pieces of code that are called with a single command. They can be designed to accept arguments as input and return values, but they don\u2019t need to do either. Variables declared inside functions only exist while the function is running and if a variable within the function (a local variable) has the same name as a variable somewhere else in the code, the local variable hides but doesn\u2019t overwrite the other. Every method used in Python (for example, print ) is a function, and the libraries we import (say, pandas ) are a collection of functions. We will only use functions that are housed within the same code that uses them, but it\u2019s also easy to write functions that can be used by different programs. Functions are declared following this general structure: def this_is_the_function_name(input_argument1, input_argument2): # The body of the function is indented # This function prints the two arguments to screen print('The function arguments are:', input_argument1, input_argument2, '(this is done inside the function!)') # And returns their product return input_argument1 * input_argument2 The function declaration starts with the word def , followed by the function name and any arguments in parenthesis, and ends in a colon. The body of the function is indented just like loops are. If the function returns something when it is called, it includes a return statement at the end. Let's rewrite this function with shorter (but still informative) names so we don't need to type as much: def product(a, b): print('The function arguments are:', a, b, '(this is done inside the function!)') return a * b This is how we call the function: product_of_inputs = product(2, 5) output The function arguments are: 2 5 (this is done inside the function!) print('Their product is:', product_of_inputs, '(this is done outside the function!)') output Their product is: 10 (this is done outside the function!) Challenge - Functions Change the values of the input arguments in the function and check its output. Try calling the function by giving it the wrong number of arguments (not 2) or not assigning the function call to a variable (no product_of_inputs = ). Bonus challenges Declare a variable inside the function and test to see where it exists (Hint: can you print it from outside the function?). Explore what happens when a variable both inside and outside the function have the same name. What happens to the global variable when you change the value of the local variable? Say we had some code for taking our survey.csv data and splitting it out into one file for each year: # First let's make sure we've read the survey data into a pandas DataFrame. import pandas as pd all_data = pd.read_csv(\"surveys.csv\") this_year = 2002 # Select data for just that year surveys_year = all_data[all_data.year == this_year] # Write the new DataFrame to a csv file filename = 'surveys' + str(this_year) + '.csv' surveys_year.to_csv(filename) There are many different \"chunks\" of this code that we can turn into functions, and we can even create functions that call other functions inside them. Let\u2019s first write a function that separates data for just one year and saves that data to a file: def year_to_csv(year, all_data): \"\"\" Writes a csv file for data from a given year. year --- year for which data is extracted all_data --- DataFrame with multi-year data \"\"\" # Select data for the year surveys_year = all_data[all_data.year == year] # Write the new DataFrame to a csv file filename = 'function_surveys' + str(year) + '.csv' surveys_year.to_csv(filename) The text between the two sets of triple double quotes is called a docstring and contains the documentation for the function. It does nothing when the function is running and is therefore not necessary, but it is good practice to include docstrings as a reminder of what the code does. Docstrings in functions also become part of their \u2018official\u2019 documentation: ?year_to_csv Signature: year_to_csv(year, all_data) Docstring: Writes a csv file for data from a given year. year --- year for which data is extracted all_data --- DataFrame with multi-year data File: ~/devel/python-workshop-base/workshops/docs/modules/notebooks/<ipython-input-16-978149c5937c> Type: function # Read the survey data into a pandas DataFrame. # (if you are jumping in to just this lesson and don't yet have the surveys.csv file yet, # see the \"Data analysis in Python with Pandas\" `working_with_data` module) import pandas as pd surveys_df = pd.read_csv(\"surveys.csv\") year_to_csv(2002, surveys_df) Aside - listing files and the os module Google Collaboratory and Juypter Notebooks have a built-in file browser, however, you can list the files and directories in the current directory (\"folder\") with Python code like: import os print(os.listdir()) You'll see a Python list, a bit like: ['surveys.csv','function_surveys2002.csv'] (you may have additional files listed here, generated in previous lessons) The os module contains, among other things, a bunch of useful functions for working with the filesystem and file paths. Two other useful examples (hint - these might help in a upcoming challenge): # This returns True if the file or directory specified exists os.path.exists('surveys.csv') # This creates empty (nested) directories based on a path (eg in a 'path' each directory is separated by slashes) os.makedirs('data/csvs/') If a directory already exists, os.makedirs fails and produces an error message (in Python terminology we might say it 'raises an exception' ). We can avoid this by using os.path.exists and os.makedirs together like: if not os.path.exists('data/csvs/'): os.makedirs('data/csvs/') What we really want to do, though, is create files for multiple years without having to request them one by one. Let\u2019s write another function that uses a for loop over a sequence of years and repeatedly calls the function we just wrote, year_to_csv : def create_csvs_by_year(start_year, end_year, all_data): \"\"\" Writes separate CSV files for each year of data. start_year --- the first year of data we want end_year --- the last year of data we want all_data --- DataFrame with multi-year data \"\"\" # \"end_year\" is the last year of data we want to pull, so we loop to end_year+1 for year in range(start_year, end_year+1): year_to_csv(year, all_data) Because people will naturally expect that the end year for the files is the last year with data, the for loop inside the function ends at end_year + 1 . By writing the entire loop into a function, we\u2019ve made a reusable tool for whenever we need to break a large data file into yearly files. Because we can specify the first and last year for which we want files, we can even use this function to create files for a subset of the years available. This is how we call this function: # Create CSV files, one for each year in the given range create_csvs_by_year(1977, 2002, surveys_df) Challenge - More Functions How could you use the function create_csvs_by_year to create a CSV file for only one year? (Hint: think about the syntax for range) Modify year_to_csv so that it has two new arguments, output_path (the path of the directory where the files will be written) and filename_prefix (a prefix to be added to the start of the file name). Call your new function year_to_csv_at_path . Eg, def year_to_csv_at_path(year, all_data, output_path, filename_prefix): . Call your new function to create a new file with a different name in a different directory. Hint: You could manually create the target directory before calling the function using the Collaboratory / Jupyter file browser, or for bonus points you could do it in Python inside the function using the os module. Create a new version of the create_csvs_by_year function called create_csvs_by_year_at_path that also takes the additional arguments output_path and filename_prefix . Internally create_csvs_by_year_at_path should pass these values to year_to_csv_at_path . Call your new function to create a new set of files with a different name in a different directory. Make these new functions return a list of the files they have written. There are many ways you can do this (and you should try them all!): you could make the function print the filenames to screen, or you could use a return statement to make the function produce a list of filenames, or you can use some combination of the two. You could also try using the os library to list the contents of directories. The functions we wrote demand that we give them a value for every argument. Ideally, we would like these functions to be as flexible and independent as possible. Let\u2019s modify the function create_csvs_by_year so that the start_year and end_year default to the full range of the data if they are not supplied by the user. Arguments can be given default values with an equal sign in the function declaration - we call these 'keyword' arguments. Any argument in the function without a default value (here, all_data ) is a required argument - we call these 'positional' arguments. Positional arguements MUST come before any keyword arguments. Keyword arguments are optional - if you don't include them when calling the function, the default value is used. def keyword_arg_test(all_data, start_year = 1977, end_year = 2002): \"\"\" A simple function to demonstrate the use of keyword arguments with defaults ! start_year --- the first year of data we want --- default: 1977 end_year --- the last year of data we want --- default: 2002 all_data --- DataFrame with multi-year data - not actually used \"\"\" return start_year, end_year start,end = keyword_arg_test(surveys_df, 1988, 1993) print('Both optional arguments:\\t', start, end) start,end = keyword_arg_test(surveys_df) print('Default values:\\t\\t\\t', start, end) output Both optional arguments: 1988 1993 Default values: 1977 2002 The \\t in the print statements are tabs, used to make the text align and be easier to read. What if our dataset doesn\u2019t start in 1977 and end in 2002? We can modify the function so that it looks for the ealiest and latest years in the dataset if those dates are not provided. Let's redefine csvs_by_year : def csvs_by_year(all_data, start_year = None, end_year = None): \"\"\" Writes separate CSV files for each year of data. The start year and end year can be optionally provided, otherwise the earliest and latest year in the dataset are used as the range. start_year --- the first year of data we want --- default: None - check all_data end_year --- the last year of data we want --- default: None - check all_data all_data --- DataFrame with multi-year data \"\"\" if start_year is None: start_year = min(all_data.year) if end_year is None: end_year = max(all_data.year) return start_year, end_year start,end = csvs_by_year(surveys_df, 1988, 1993) print('Both optional arguments:\\t', start, end) start,end = csvs_by_year(surveys_df) print('Default values:\\t\\t\\t', start, end) output Both optional arguments: 1988 1993 Default values: 1977 2002 The default values of the start_year and end_year arguments in this new version of the csvs_by_year function are now None . This is a built-in constant in Python that indicates the absence of a value - essentially, that the variable exists in the namespace of the function (the directory of variable names) but that it doesn\u2019t correspond to any existing object. Challenge - Experimenting with keyword arguments What type of object corresponds to a variable declared as None ? (Hint: create a variable set to None and use the function type() ) Compare the behavior of the function csvs_by_year when the keyword arguments have None as a default vs. calling the function by supplying (non-default) values to the keyword arguments What happens if you only include a value for start_year in the function call? Can you write the function call with only a value for end_year ? (Hint: think about how the function must be assigning values to each of the arguments - this is related to the need to put the arguments without default values before those with default values in the function definition!) Conditionals - if statements The body of the test function now has two conditionals ( if statements) that check the values of start_year and end_year . if statements execute a segment of code when some condition is met. They commonly look something like this: a = 5 if a < 0: # Meets first condition? # if a IS less than zero print('a is a negative number') elif a > 0: # Did not meet first condition. meets second condition? # if a ISN'T less than zero and IS more than zero print('a is a positive number') else: # Met neither condition # if a ISN'T less than zero and ISN'T more than zero print('a must be zero!') output a is a positive number Change the value of a to see how this function works. The statement elif means \u201celse if\u201d, and all of the conditional statements must end in a colon. The if statements in the function csvs_by_year check whether there is an object associated with the variable names start_year and end_year . If those variables are None , the if statements return the boolean True and execute whatever is in their body. On the other hand, if the variable names are associated with some value (they got a number in the function call), the if statements return False and do not execute. The opposite conditional statements, which would return True if the variables were associated with objects (if they had received value in the function call), would be if start_year and if end_year . As we\u2019ve written it so far, the function csvs_by_year associates values in the function call with arguments in the function definition just based in their order. If the function gets only two values in the function call, the first one will be associated with all_data and the second with start_year , regardless of what we intended them to be. We can get around this problem by calling the function using keyword arguments, where each of the arguments in the function definition is associated with a keyword and the function call passes values to the function using these keywords: start,end = csvs_by_year(surveys_df) print('Default values:\\t\\t\\t', start, end) start,end = csvs_by_year(surveys_df, 1988, 1993) print('No keywords:\\t\\t\\t', start, end) start,end = csvs_by_year(surveys_df, start_year = 1988, end_year = 1993) print('Both keywords, in order:\\t', start, end) start,end = csvs_by_year(surveys_df, end_year = 1993, start_year = 1988) print('Both keywords, flipped:\\t\\t', start, end) start,end = csvs_by_year(surveys_df, start_year = 1988) print('One keyword, default end:\\t', start, end) start,end = csvs_by_year(surveys_df, end_year = 1993) print('One keyword, default start:\\t', start, end) output Default values: 1977 2002 No keywords: 1988 1993 Both keywords, in order: 1988 1993 Both keywords, flipped: 1988 1993 One keyword, default end: 1988 2002 One keyword, default start: 1977 1993 Multiple choice challenge What output would you expect from the if statement (try to figure out the answer without running the code): pod_bay_doors_open = False dave_want_doors_open = False hal_insanity_level = 2001 if not pod_bay_doors_open: print(\"Dave: Open the pod bay doors please HAL.\") dave_wants_doors_open = True elif pod_bay_doors_open and hal_insanity_level >= 95: print(\"HAL: I'm closing the pod bay doors, Dave.\") if dave_wants_doors_open and not pod_bay_doors_open and hal_insanity_level >= 95: print(\"HAL: I\u2019m sorry, Dave. I\u2019m afraid I can\u2019t do that.\") elif dave_wants_doors_open and not pod_bay_doors_open: print(\"HAL: I'm opening the pod bay doors, welcome back Dave.\") else: print(\"... silence of space ...\") a) \"HAL: I'm closing the pod bay doors, Dave.\", \"... silence of space ...\" b) \"Dave: Open the pod bay doors please HAL.\", \"HAL: I\u2019m sorry, Dave. I\u2019m afraid I can\u2019t do that.\" c) \"... silence of space ...\" d) \"Dave: Open the pod bay doors please HAL.\", HAL: \"I'm opening the pod bay doors, welcome back Dave.\" Bonus Challenge - Modifying functions Rewrite the year_to_csv and csvs_by_year functions to have keyword arguments with default values. Modify the functions so that they don\u2019t create yearly files if there is no data for a given year and display an alert to the user (Hint: use conditional statements to do this. For an extra challenge, use try statements !). The code below checks to see whether a directory exists and creates one if it doesn\u2019t. Add some code to your function that writes out the CSV files, to check for a directory to write to. import os if 'dir_name_here' in os.listdir(): print('Processed directory exists') else: os.mkdir('dir_name_here') print('Processed directory created') 4. The code that you have written so far to loop through the years is good, however it is not necessarily reproducible with different datasets. For instance, what happens to the code if we have additional years of data in our CSV files? Using the tools that you learned in the previous activities, make a list of all years represented in the data. Then create a loop to process your data, that begins at the earliest year and ends at the latest year using that list. HINT: you can create a loop with a list as follows: for years in year_list:","title":"Reusable and modular code with functions"},{"location":"modules/functions/#reusable-and-modular-code-with-functions","text":"","title":"Reusable and modular code with functions"},{"location":"modules/functions/#functions","text":"Functions wrap up reusable pieces of code - they help you apply the Do Not Repeat Yourself (DRY) principle. Suppose that separating large data files into individual yearly files is a task that we frequently have to perform. We could write a for loop like the one above every time we needed to do it but that would be time consuming and error prone. A more elegant solution would be to create a reusable tool that performs this task with minimum input from the user. To do this, we are going to turn the code we\u2019ve already written into a function. Functions are reusable, self-contained pieces of code that are called with a single command. They can be designed to accept arguments as input and return values, but they don\u2019t need to do either. Variables declared inside functions only exist while the function is running and if a variable within the function (a local variable) has the same name as a variable somewhere else in the code, the local variable hides but doesn\u2019t overwrite the other. Every method used in Python (for example, print ) is a function, and the libraries we import (say, pandas ) are a collection of functions. We will only use functions that are housed within the same code that uses them, but it\u2019s also easy to write functions that can be used by different programs. Functions are declared following this general structure: def this_is_the_function_name(input_argument1, input_argument2): # The body of the function is indented # This function prints the two arguments to screen print('The function arguments are:', input_argument1, input_argument2, '(this is done inside the function!)') # And returns their product return input_argument1 * input_argument2 The function declaration starts with the word def , followed by the function name and any arguments in parenthesis, and ends in a colon. The body of the function is indented just like loops are. If the function returns something when it is called, it includes a return statement at the end. Let's rewrite this function with shorter (but still informative) names so we don't need to type as much: def product(a, b): print('The function arguments are:', a, b, '(this is done inside the function!)') return a * b This is how we call the function: product_of_inputs = product(2, 5) output The function arguments are: 2 5 (this is done inside the function!) print('Their product is:', product_of_inputs, '(this is done outside the function!)') output Their product is: 10 (this is done outside the function!)","title":"Functions"},{"location":"modules/functions/#challenge-functions","text":"Change the values of the input arguments in the function and check its output. Try calling the function by giving it the wrong number of arguments (not 2) or not assigning the function call to a variable (no product_of_inputs = ).","title":"Challenge - Functions"},{"location":"modules/functions/#bonus-challenges","text":"Declare a variable inside the function and test to see where it exists (Hint: can you print it from outside the function?). Explore what happens when a variable both inside and outside the function have the same name. What happens to the global variable when you change the value of the local variable? Say we had some code for taking our survey.csv data and splitting it out into one file for each year: # First let's make sure we've read the survey data into a pandas DataFrame. import pandas as pd all_data = pd.read_csv(\"surveys.csv\") this_year = 2002 # Select data for just that year surveys_year = all_data[all_data.year == this_year] # Write the new DataFrame to a csv file filename = 'surveys' + str(this_year) + '.csv' surveys_year.to_csv(filename) There are many different \"chunks\" of this code that we can turn into functions, and we can even create functions that call other functions inside them. Let\u2019s first write a function that separates data for just one year and saves that data to a file: def year_to_csv(year, all_data): \"\"\" Writes a csv file for data from a given year. year --- year for which data is extracted all_data --- DataFrame with multi-year data \"\"\" # Select data for the year surveys_year = all_data[all_data.year == year] # Write the new DataFrame to a csv file filename = 'function_surveys' + str(year) + '.csv' surveys_year.to_csv(filename) The text between the two sets of triple double quotes is called a docstring and contains the documentation for the function. It does nothing when the function is running and is therefore not necessary, but it is good practice to include docstrings as a reminder of what the code does. Docstrings in functions also become part of their \u2018official\u2019 documentation: ?year_to_csv Signature: year_to_csv(year, all_data) Docstring: Writes a csv file for data from a given year. year --- year for which data is extracted all_data --- DataFrame with multi-year data File: ~/devel/python-workshop-base/workshops/docs/modules/notebooks/<ipython-input-16-978149c5937c> Type: function # Read the survey data into a pandas DataFrame. # (if you are jumping in to just this lesson and don't yet have the surveys.csv file yet, # see the \"Data analysis in Python with Pandas\" `working_with_data` module) import pandas as pd surveys_df = pd.read_csv(\"surveys.csv\") year_to_csv(2002, surveys_df)","title":"Bonus challenges"},{"location":"modules/functions/#aside-listing-files-and-the-os-module","text":"Google Collaboratory and Juypter Notebooks have a built-in file browser, however, you can list the files and directories in the current directory (\"folder\") with Python code like: import os print(os.listdir()) You'll see a Python list, a bit like: ['surveys.csv','function_surveys2002.csv'] (you may have additional files listed here, generated in previous lessons) The os module contains, among other things, a bunch of useful functions for working with the filesystem and file paths. Two other useful examples (hint - these might help in a upcoming challenge): # This returns True if the file or directory specified exists os.path.exists('surveys.csv') # This creates empty (nested) directories based on a path (eg in a 'path' each directory is separated by slashes) os.makedirs('data/csvs/') If a directory already exists, os.makedirs fails and produces an error message (in Python terminology we might say it 'raises an exception' ). We can avoid this by using os.path.exists and os.makedirs together like: if not os.path.exists('data/csvs/'): os.makedirs('data/csvs/') What we really want to do, though, is create files for multiple years without having to request them one by one. Let\u2019s write another function that uses a for loop over a sequence of years and repeatedly calls the function we just wrote, year_to_csv : def create_csvs_by_year(start_year, end_year, all_data): \"\"\" Writes separate CSV files for each year of data. start_year --- the first year of data we want end_year --- the last year of data we want all_data --- DataFrame with multi-year data \"\"\" # \"end_year\" is the last year of data we want to pull, so we loop to end_year+1 for year in range(start_year, end_year+1): year_to_csv(year, all_data) Because people will naturally expect that the end year for the files is the last year with data, the for loop inside the function ends at end_year + 1 . By writing the entire loop into a function, we\u2019ve made a reusable tool for whenever we need to break a large data file into yearly files. Because we can specify the first and last year for which we want files, we can even use this function to create files for a subset of the years available. This is how we call this function: # Create CSV files, one for each year in the given range create_csvs_by_year(1977, 2002, surveys_df)","title":"Aside - listing files and the os module"},{"location":"modules/functions/#challenge-more-functions","text":"How could you use the function create_csvs_by_year to create a CSV file for only one year? (Hint: think about the syntax for range) Modify year_to_csv so that it has two new arguments, output_path (the path of the directory where the files will be written) and filename_prefix (a prefix to be added to the start of the file name). Call your new function year_to_csv_at_path . Eg, def year_to_csv_at_path(year, all_data, output_path, filename_prefix): . Call your new function to create a new file with a different name in a different directory. Hint: You could manually create the target directory before calling the function using the Collaboratory / Jupyter file browser, or for bonus points you could do it in Python inside the function using the os module. Create a new version of the create_csvs_by_year function called create_csvs_by_year_at_path that also takes the additional arguments output_path and filename_prefix . Internally create_csvs_by_year_at_path should pass these values to year_to_csv_at_path . Call your new function to create a new set of files with a different name in a different directory. Make these new functions return a list of the files they have written. There are many ways you can do this (and you should try them all!): you could make the function print the filenames to screen, or you could use a return statement to make the function produce a list of filenames, or you can use some combination of the two. You could also try using the os library to list the contents of directories. The functions we wrote demand that we give them a value for every argument. Ideally, we would like these functions to be as flexible and independent as possible. Let\u2019s modify the function create_csvs_by_year so that the start_year and end_year default to the full range of the data if they are not supplied by the user. Arguments can be given default values with an equal sign in the function declaration - we call these 'keyword' arguments. Any argument in the function without a default value (here, all_data ) is a required argument - we call these 'positional' arguments. Positional arguements MUST come before any keyword arguments. Keyword arguments are optional - if you don't include them when calling the function, the default value is used. def keyword_arg_test(all_data, start_year = 1977, end_year = 2002): \"\"\" A simple function to demonstrate the use of keyword arguments with defaults ! start_year --- the first year of data we want --- default: 1977 end_year --- the last year of data we want --- default: 2002 all_data --- DataFrame with multi-year data - not actually used \"\"\" return start_year, end_year start,end = keyword_arg_test(surveys_df, 1988, 1993) print('Both optional arguments:\\t', start, end) start,end = keyword_arg_test(surveys_df) print('Default values:\\t\\t\\t', start, end) output Both optional arguments: 1988 1993 Default values: 1977 2002 The \\t in the print statements are tabs, used to make the text align and be easier to read. What if our dataset doesn\u2019t start in 1977 and end in 2002? We can modify the function so that it looks for the ealiest and latest years in the dataset if those dates are not provided. Let's redefine csvs_by_year : def csvs_by_year(all_data, start_year = None, end_year = None): \"\"\" Writes separate CSV files for each year of data. The start year and end year can be optionally provided, otherwise the earliest and latest year in the dataset are used as the range. start_year --- the first year of data we want --- default: None - check all_data end_year --- the last year of data we want --- default: None - check all_data all_data --- DataFrame with multi-year data \"\"\" if start_year is None: start_year = min(all_data.year) if end_year is None: end_year = max(all_data.year) return start_year, end_year start,end = csvs_by_year(surveys_df, 1988, 1993) print('Both optional arguments:\\t', start, end) start,end = csvs_by_year(surveys_df) print('Default values:\\t\\t\\t', start, end) output Both optional arguments: 1988 1993 Default values: 1977 2002 The default values of the start_year and end_year arguments in this new version of the csvs_by_year function are now None . This is a built-in constant in Python that indicates the absence of a value - essentially, that the variable exists in the namespace of the function (the directory of variable names) but that it doesn\u2019t correspond to any existing object.","title":"Challenge - More Functions"},{"location":"modules/functions/#challenge-experimenting-with-keyword-arguments","text":"What type of object corresponds to a variable declared as None ? (Hint: create a variable set to None and use the function type() ) Compare the behavior of the function csvs_by_year when the keyword arguments have None as a default vs. calling the function by supplying (non-default) values to the keyword arguments What happens if you only include a value for start_year in the function call? Can you write the function call with only a value for end_year ? (Hint: think about how the function must be assigning values to each of the arguments - this is related to the need to put the arguments without default values before those with default values in the function definition!)","title":"Challenge - Experimenting with keyword arguments"},{"location":"modules/functions/#conditionals-if-statements","text":"The body of the test function now has two conditionals ( if statements) that check the values of start_year and end_year . if statements execute a segment of code when some condition is met. They commonly look something like this: a = 5 if a < 0: # Meets first condition? # if a IS less than zero print('a is a negative number') elif a > 0: # Did not meet first condition. meets second condition? # if a ISN'T less than zero and IS more than zero print('a is a positive number') else: # Met neither condition # if a ISN'T less than zero and ISN'T more than zero print('a must be zero!') output a is a positive number Change the value of a to see how this function works. The statement elif means \u201celse if\u201d, and all of the conditional statements must end in a colon. The if statements in the function csvs_by_year check whether there is an object associated with the variable names start_year and end_year . If those variables are None , the if statements return the boolean True and execute whatever is in their body. On the other hand, if the variable names are associated with some value (they got a number in the function call), the if statements return False and do not execute. The opposite conditional statements, which would return True if the variables were associated with objects (if they had received value in the function call), would be if start_year and if end_year . As we\u2019ve written it so far, the function csvs_by_year associates values in the function call with arguments in the function definition just based in their order. If the function gets only two values in the function call, the first one will be associated with all_data and the second with start_year , regardless of what we intended them to be. We can get around this problem by calling the function using keyword arguments, where each of the arguments in the function definition is associated with a keyword and the function call passes values to the function using these keywords: start,end = csvs_by_year(surveys_df) print('Default values:\\t\\t\\t', start, end) start,end = csvs_by_year(surveys_df, 1988, 1993) print('No keywords:\\t\\t\\t', start, end) start,end = csvs_by_year(surveys_df, start_year = 1988, end_year = 1993) print('Both keywords, in order:\\t', start, end) start,end = csvs_by_year(surveys_df, end_year = 1993, start_year = 1988) print('Both keywords, flipped:\\t\\t', start, end) start,end = csvs_by_year(surveys_df, start_year = 1988) print('One keyword, default end:\\t', start, end) start,end = csvs_by_year(surveys_df, end_year = 1993) print('One keyword, default start:\\t', start, end) output Default values: 1977 2002 No keywords: 1988 1993 Both keywords, in order: 1988 1993 Both keywords, flipped: 1988 1993 One keyword, default end: 1988 2002 One keyword, default start: 1977 1993","title":"Conditionals - if statements"},{"location":"modules/functions/#multiple-choice-challenge","text":"What output would you expect from the if statement (try to figure out the answer without running the code): pod_bay_doors_open = False dave_want_doors_open = False hal_insanity_level = 2001 if not pod_bay_doors_open: print(\"Dave: Open the pod bay doors please HAL.\") dave_wants_doors_open = True elif pod_bay_doors_open and hal_insanity_level >= 95: print(\"HAL: I'm closing the pod bay doors, Dave.\") if dave_wants_doors_open and not pod_bay_doors_open and hal_insanity_level >= 95: print(\"HAL: I\u2019m sorry, Dave. I\u2019m afraid I can\u2019t do that.\") elif dave_wants_doors_open and not pod_bay_doors_open: print(\"HAL: I'm opening the pod bay doors, welcome back Dave.\") else: print(\"... silence of space ...\") a) \"HAL: I'm closing the pod bay doors, Dave.\", \"... silence of space ...\" b) \"Dave: Open the pod bay doors please HAL.\", \"HAL: I\u2019m sorry, Dave. I\u2019m afraid I can\u2019t do that.\" c) \"... silence of space ...\" d) \"Dave: Open the pod bay doors please HAL.\", HAL: \"I'm opening the pod bay doors, welcome back Dave.\"","title":"Multiple choice challenge"},{"location":"modules/functions/#bonus-challenge-modifying-functions","text":"Rewrite the year_to_csv and csvs_by_year functions to have keyword arguments with default values. Modify the functions so that they don\u2019t create yearly files if there is no data for a given year and display an alert to the user (Hint: use conditional statements to do this. For an extra challenge, use try statements !). The code below checks to see whether a directory exists and creates one if it doesn\u2019t. Add some code to your function that writes out the CSV files, to check for a directory to write to. import os if 'dir_name_here' in os.listdir(): print('Processed directory exists') else: os.mkdir('dir_name_here') print('Processed directory created') 4. The code that you have written so far to loop through the years is good, however it is not necessarily reproducible with different datasets. For instance, what happens to the code if we have additional years of data in our CSV files? Using the tools that you learned in the previous activities, make a list of all years represented in the data. Then create a loop to process your data, that begins at the earliest year and ends at the latest year using that list. HINT: you can create a loop with a list as follows: for years in year_list:","title":"Bonus Challenge - Modifying functions"},{"location":"modules/indexing/","text":".output_label { text-align: right; margin: -1em; padding: 0; font-size: 0.5em; color: grey } Indexing, Slicing and Subsetting In this lesson, we will explore ways to access different parts of the data in a Pandas DataFrame using: Indexing, Slicing, and Subsetting Ensure the Pandas package is installed !pip install pandas matplotlib output Requirement already satisfied: pandas in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (0.25.0) Requirement already satisfied: matplotlib in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (3.1.1) Requirement already satisfied: numpy>=1.13.3 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (1.17.0) Requirement already satisfied: pytz>=2017.2 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2019.1) Requirement already satisfied: python-dateutil>=2.6.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2.8.0) Requirement already satisfied: cycler>=0.10 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (2.4.1.1) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (1.1.0) Requirement already satisfied: six>=1.5 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0) Requirement already satisfied: setuptools in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0) Loading our data We will continue to use the surveys dataset that we worked with in the last lesson. Let's reopen and read in the data again: # Make sure pandas is loaded import pandas as pd # Read in the survey CSV surveys_df = pd.read_csv(\"surveys.csv\") Indexing and Slicing in Python We often want to work with subsets of a DataFrame object. There are different ways to accomplish this including: using labels (column headings), numeric ranges, or specific x,y index locations. Selecting data using Labels (Column Headings) We use square brackets [] to select a subset of an Python object. For example, we can select all data from a column named species_id from the surveys_df DataFrame by name. There are two ways to do this: # Method 1: select a 'subset' of the data using the column name surveys_df['species_id'].head() output 0 NL 1 NL 2 DM 3 DM 4 DM Name: species_id, dtype: object # Method 2: use the column name as an 'attribute'; gives the same output surveys_df.species_id.head() output 0 NL 1 NL 2 DM 3 DM 4 DM Name: species_id, dtype: object We can also create a new object that contains only the data within the species_id column as follows: # Creates an object, surveys_species, that only contains the `species_id` column surveys_species = surveys_df['species_id'] We can pass a list of column names too, as an index to select columns in that order. This is useful when we need to reorganize our data. NOTE: If a column name is not contained in the DataFrame, an exception (error) will be raised. # Select the species and plot columns from the DataFrame surveys_df[['species_id', 'site_id']].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } species_id site_id 0 NL 2 1 NL 3 2 DM 2 3 DM 7 4 DM 3 What happens if you ask for a column that doesn't exist? surveys_df['speciess'] Outputs: --------------------------------------------------------------------------- KeyError Traceback (most recent call last) /Applications/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance) 2392 try: -> 2393 return self._engine.get_loc(key) 2394 except KeyError: pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5239)() pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20405)() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20359)() KeyError: 'speciess' During handling of the above exception, another exception occurred: KeyError Traceback (most recent call last) <ipython-input-7-7d65fa0158b8> in <module>() 1 2 # What happens if you ask for a column that doesn't exist? ----> 3 surveys_df['speciess'] 4 /Applications/anaconda/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key) 2060 return self._getitem_multilevel(key) 2061 else: -> 2062 return self._getitem_column(key) 2063 2064 def _getitem_column(self, key): /Applications/anaconda/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key) 2067 # get column 2068 if self.columns.is_unique: -> 2069 return self._get_item_cache(key) 2070 2071 # duplicate columns & possible reduce dimensionality /Applications/anaconda/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item) 1532 res = cache.get(item) 1533 if res is None: -> 1534 values = self._data.get(item) 1535 res = self._box_item_values(item, values) 1536 cache[item] = res /Applications/anaconda/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath) 3588 3589 if not isnull(item): -> 3590 loc = self.items.get_loc(item) 3591 else: 3592 indexer = np.arange(len(self.items))[isnull(self.items)] /Applications/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance) 2393 return self._engine.get_loc(key) 2394 except KeyError: -> 2395 return self._engine.get_loc(self._maybe_cast_indexer(key)) 2396 2397 indexer = self.get_indexer([key], method=method, tolerance=tolerance) pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5239)() pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20405)() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20359)() KeyError: 'speciess' Python tells us what type of error it is in the traceback, at the bottom it says KeyError: 'speciess' which means that speciess is not a column name (or Key in the related python data type dictionary). # What happens when you flip the order? surveys_df[['site_id', 'species_id']].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } site_id species_id 0 2 NL 1 3 NL 2 2 DM 3 7 DM 4 3 DM Extracting Range based Subsets: Slicing REMINDER : Python Uses 0-based Indexing Let's remind ourselves that Python uses 0-based indexing. This means that the first element in an object is located at position 0. This is different from other tools like R and Matlab that index elements within objects starting at 1. # Create a list of numbers: a = [1, 2, 3, 4, 5] Challenge - Extracting data What value does the code a[0] return? How about this: a[5] In the example above, calling a[5] returns an error. Why is that? What about a[len(a)] ? Solutions - Extracting data Slicing Subsets of Rows in Python Slicing using the [] operator selects a set of rows and/or columns from a DataFrame. To slice out a set of rows, you use the following syntax: data[start:stop] . When slicing in pandas the start bound is included in the output. The stop bound is one step BEYOND the row you want to select. So if you want to select rows 0, 1 and 2 your code would look like this with our surveys data : # Select rows 0, 1, 2 (row 3 is not selected) surveys_df[0:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN The stop bound in Python is different from what you might be used to in languages like Matlab and R. Now lets select the first 5 rows (rows 0, 1, 2, 3, 4) . surveys_df[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN # Select the last element in the list # (the slice starts at the last element, and ends at the end of the list) surveys_df[-1:] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 35548 35549 12 31 2002 5 NaN NaN NaN NaN We can also reassign values within subsets of our DataFrame. Let's create a brand new clean dataframe from the original data CSV file. surveys_df = pd.read_csv(\"surveys.csv\") Slicing Subsets of Rows and Columns in Python We can select specific ranges of our data in both the row and column directions using either label or integer-based indexing. loc is primarily label based indexing. Integers may be used but they are interpreted as a label . iloc is primarily integer based indexing To select a subset of rows and columns from our DataFrame, we can use the iloc method. For example, we can select month, day and year (columns 2, 3 and 4 if we start counting at 1), like this: iloc[row slicing, column slicing] surveys_df.iloc[0:3, 1:4] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } month day year 0 7 16 1977 1 7 16 1977 2 7 16 1977 Notice that we asked for a slice from 0:3. This yielded 3 rows of data. When you ask for 0:3, you are actually telling Python to start at index 0 and select rows 0, 1, 2 up to but not including 3 . Let's explore some other ways to index and select subsets of data: # Select all columns for rows of index values 0 and 10 surveys_df.loc[[0, 10], :] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN # What does this do? surveys_df.loc[0, ['species_id', 'site_id', 'weight']] output species_id NL site_id 2 weight NaN Name: 0, dtype: object # What happens when you type the code below? surveys_df.loc[[0, 10, 35549], :] output /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages/pandas/core/indexing.py:1404: FutureWarning: Passing list-likes to .loc or [] with any missing label will raise KeyError in the future, you can use .reindex() as an alternative. See the documentation here: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike return self._getitem_tuple(key) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1.0 7.0 16.0 1977.0 2.0 NL M 32.0 NaN 10 11.0 7.0 16.0 1977.0 5.0 DS F 53.0 NaN 35549 NaN NaN NaN NaN NaN NaN NaN NaN NaN NOTE : Labels must be found in the DataFrame or you will get a KeyError . Indexing by labels loc differs from indexing by integers iloc . With loc , the both start bound and the stop bound are inclusive . When using loc , integers can be used, but the integers refer to the index label and not the position. For example, using loc and select 1:4 will get a different result than using iloc to select rows 1:4. We can also select a specific data value using a row and column location within the DataFrame and iloc indexing: # Syntax for iloc indexing to finding a specific data element dat.iloc[row, column] In following iloc example: surveys_df.iloc[2, 6] output 'F' Remember that Python indexing begins at 0. So, the index location [2, 6] selects the element that is 3 rows down and 7 columns over in the DataFrame. Challenge - Range What happens when you execute: surveys_df[0:1] surveys_df[:4] surveys_df[:-1] What happens when you call: surveys_df.iloc[0:4, 1:4] Subsetting Data using Criteria We can also select a subset of our data using criteria. For example, we can select all rows that have a year value of 2002 : surveys_df[surveys_df.year == 2002].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 33320 33321 1 12 2002 1 DM M 38.0 44.0 33321 33322 1 12 2002 1 DO M 37.0 58.0 33322 33323 1 12 2002 1 PB M 28.0 45.0 33323 33324 1 12 2002 1 AB NaN NaN NaN 33324 33325 1 12 2002 1 DO M 35.0 29.0 Or we can select all rows that do not contain the year 2002 : surveys_df[surveys_df.year != 2002] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN 5 6 7 16 1977 1 PF M 14.0 NaN 6 7 7 16 1977 2 PE F NaN NaN 7 8 7 16 1977 1 DM M 37.0 NaN 8 9 7 16 1977 1 DM F 34.0 NaN 9 10 7 16 1977 6 PF F 20.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN 11 12 7 16 1977 7 DM M 38.0 NaN 12 13 7 16 1977 3 DM M 35.0 NaN 13 14 7 16 1977 8 DM NaN NaN NaN 14 15 7 16 1977 6 DM F 36.0 NaN 15 16 7 16 1977 4 DM F 36.0 NaN 16 17 7 16 1977 3 DS F 48.0 NaN 17 18 7 16 1977 2 PP M 22.0 NaN 18 19 7 16 1977 4 PF NaN NaN NaN 19 20 7 17 1977 11 DS F 48.0 NaN 20 21 7 17 1977 14 DM F 34.0 NaN 21 22 7 17 1977 15 NL F 31.0 NaN 22 23 7 17 1977 13 DM M 36.0 NaN 23 24 7 17 1977 13 SH M 21.0 NaN 24 25 7 17 1977 9 DM M 35.0 NaN 25 26 7 17 1977 15 DM M 31.0 NaN 26 27 7 17 1977 15 DM M 36.0 NaN 27 28 7 17 1977 11 DM M 38.0 NaN 28 29 7 17 1977 11 PP M NaN NaN 29 30 7 17 1977 10 DS F 52.0 NaN ... ... ... ... ... ... ... ... ... ... 33290 33291 12 15 2001 23 PE M 20.0 18.0 33291 33292 12 15 2001 23 RM F 16.0 8.0 33292 33293 12 15 2001 20 PE F 20.0 22.0 33293 33294 12 15 2001 20 SH M 25.0 43.0 33294 33295 12 15 2001 20 PB F 27.0 33.0 33295 33296 12 15 2001 20 PB M 25.0 35.0 33296 33297 12 15 2001 20 RM M 16.0 11.0 33297 33298 12 15 2001 20 RM F 16.0 8.0 33298 33299 12 15 2001 20 PB F 25.0 28.0 33299 33300 12 15 2001 20 PB F 26.0 30.0 33300 33301 12 15 2001 20 PB F 27.0 31.0 33301 33302 12 15 2001 24 PE M 20.0 24.0 33302 33303 12 15 2001 24 PE M 20.0 23.0 33303 33304 12 15 2001 24 RM M 16.0 10.0 33304 33305 12 15 2001 7 PB M 29.0 44.0 33305 33306 12 15 2001 7 OT M 19.0 21.0 33306 33307 12 15 2001 7 OT M 20.0 19.0 33307 33308 12 15 2001 7 PP M 24.0 16.0 33308 33309 12 16 2001 3 NaN NaN NaN NaN 33309 33310 12 16 2001 4 NaN NaN NaN NaN 33310 33311 12 16 2001 5 NaN NaN NaN NaN 33311 33312 12 16 2001 6 NaN NaN NaN NaN 33312 33313 12 16 2001 8 NaN NaN NaN NaN 33313 33314 12 16 2001 9 NaN NaN NaN NaN 33314 33315 12 16 2001 10 NaN NaN NaN NaN 33315 33316 12 16 2001 11 NaN NaN NaN NaN 33316 33317 12 16 2001 13 NaN NaN NaN NaN 33317 33318 12 16 2001 14 NaN NaN NaN NaN 33318 33319 12 16 2001 15 NaN NaN NaN NaN 33319 33320 12 16 2001 16 NaN NaN NaN NaN 33320 rows \u00d7 9 columns We can define sets of criteria too: surveys_df[(surveys_df.year >= 1980) & (surveys_df.year <= 1985)] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 2270 2271 1 15 1980 8 DO M 35.0 53.0 2271 2272 1 15 1980 11 PF F 16.0 10.0 2272 2273 1 15 1980 18 DM F 34.0 33.0 2273 2274 1 15 1980 11 DM M 38.0 37.0 2274 2275 1 15 1980 8 DO F 33.0 29.0 2275 2276 1 15 1980 11 DS M 47.0 132.0 2276 2277 1 15 1980 8 PF M 15.0 8.0 2277 2278 1 15 1980 9 OT M 21.0 23.0 2278 2279 1 15 1980 11 DM F 36.0 36.0 2279 2280 1 15 1980 21 OT F 20.0 21.0 2280 2281 1 15 1980 11 OL M 20.0 29.0 2281 2282 1 15 1980 17 DM F 36.0 49.0 2282 2283 1 15 1980 11 OL M 21.0 23.0 2283 2284 1 15 1980 9 OL M 20.0 32.0 2284 2285 1 15 1980 10 OL F 20.0 24.0 2285 2286 1 15 1980 11 DM M 38.0 47.0 2286 2287 1 15 1980 21 OT M 19.0 22.0 2287 2288 1 15 1980 19 RM F 17.0 12.0 2288 2289 1 15 1980 20 DS F 52.0 150.0 2289 2290 1 15 1980 11 DM M 37.0 49.0 2290 2291 1 15 1980 9 OL F 21.0 34.0 2291 2292 1 15 1980 12 DM F 35.0 40.0 2292 2293 1 15 1980 18 DS F 51.0 132.0 2293 2294 1 15 1980 22 DM F 34.0 25.0 2294 2295 1 15 1980 9 OL M 21.0 36.0 2295 2296 1 15 1980 8 DO F 34.0 50.0 2296 2297 1 15 1980 11 DM M 37.0 45.0 2297 2298 1 15 1980 17 DM M 35.0 47.0 2298 2299 1 15 1980 9 DM M 38.0 46.0 2299 2300 1 15 1980 18 DM F 32.0 29.0 ... ... ... ... ... ... ... ... ... ... 11197 11198 12 8 1985 4 DS M 45.0 129.0 11198 11199 12 8 1985 8 DM F 38.0 42.0 11199 11200 12 8 1985 7 AB NaN NaN NaN 11200 11201 12 8 1985 5 OL M 21.0 29.0 11201 11202 12 8 1985 9 DM F 35.0 39.0 11202 11203 12 8 1985 7 PE F 17.0 19.0 11203 11204 12 8 1985 3 PP F 22.0 16.0 11204 11205 12 8 1985 5 DO M 37.0 56.0 11205 11206 12 8 1985 11 DM F 38.0 38.0 11206 11207 12 8 1985 2 PE M 18.0 19.0 11207 11208 12 8 1985 8 DS F 50.0 120.0 11208 11209 12 8 1985 2 DO F 37.0 52.0 11209 11210 12 8 1985 2 DM F 35.0 40.0 11210 11211 12 8 1985 13 DM M 37.0 45.0 11211 11212 12 8 1985 4 DS NaN NaN 121.0 11212 11213 12 8 1985 13 AH NaN NaN NaN 11213 11214 12 8 1985 1 DM F 37.0 44.0 11214 11215 12 8 1985 2 NL F 32.0 160.0 11215 11216 12 8 1985 3 RM M 17.0 9.0 11216 11217 12 8 1985 4 OL M 24.0 34.0 11217 11218 12 8 1985 9 DM F 36.0 39.0 11218 11219 12 8 1985 8 DM F 38.0 41.0 11219 11220 12 8 1985 5 DO F 37.0 56.0 11220 11221 12 8 1985 13 AH NaN NaN NaN 11221 11222 12 8 1985 7 AB NaN NaN NaN 11222 11223 12 8 1985 4 DM M 36.0 40.0 11223 11224 12 8 1985 11 DM M 37.0 49.0 11224 11225 12 8 1985 7 PE M 20.0 18.0 11225 11226 12 8 1985 1 DM M 38.0 47.0 11226 11227 12 8 1985 15 NaN NaN NaN NaN 8957 rows \u00d7 9 columns Python Syntax Cheat Sheet Use can use the syntax below when querying data by criteria from a DataFrame. Experiment with selecting various subsets of the \"surveys\" data. Equals: == Not equals: != Greater than, less than: > or < Greater than or equal to >= Less than or equal to <= Challenge - Queries Select a subset of rows in the surveys_df DataFrame that contain data from the year 1999 and that contain weight values less than or equal to 8. How many rows did you end up with? What did your neighbor get? (Extra) Use the isin function to find all plots that contain PB and PL species in the \"surveys\" DataFrame. How many records contain these values? You can use the isin command in Python to query a DataFrame based upon a list of values as follows: surveys_df[surveys_df['species_id'].isin([listGoesHere])] Extra Challenges (Extra) Create a query that finds all rows with a weight value greater than ( > ) or equal to 0. (Extra) The ~ symbol in Python can be used to return the OPPOSITE of the selection that you specify in Python. It is equivalent to is not in . Write a query that selects all rows with sex NOT equal to 'M' or 'F' in the \"surveys\" data. Using masks to identify a specific condition A mask can be useful to locate where a particular subset of values exist or don't exist - for example, NaN, or \"Not a Number\" values. To understand masks, we also need to understand BOOLEAN objects in Python. Boolean values include True or False . For example, # Set x to 5 x = 5 # What does the code below return? x > 5 output False # How about this? x == 5 output True Extra Challenges - Putting it all together Create a new DataFrame that only contains observations with sex values that are not female or male. Assign each sex value in the new DataFrame to a new value of 'x'. Determine the number of null values in the subset. Create a new DataFrame that contains only observations that are of sex male or female and where weight values are greater than 0. Create a stacked bar plot of average weight by plot with male vs female values stacked for each plot. Count the number of missing values per column. Hint: The method .count() gives you the number of non-NA observations per column. # Solution extra challenge 2 # selection of the data with isin stack_selection = surveys_df[(surveys_df['sex'].isin(['M', 'F'])) & surveys_df[\"weight\"] > 0.][[\"sex\", \"weight\", \"site_id\"]] # calculate the mean weight for each site id and sex combination: stack_selection = stack_selection.groupby([\"site_id\", \"sex\"]).mean().unstack() # Plot inside jupyter notebook %matplotlib inline # and we can make a stacked bar plot from this: stack_selection.plot(kind='bar', stacked=True) output","title":"Indexing"},{"location":"modules/indexing/#indexing-slicing-and-subsetting","text":"In this lesson, we will explore ways to access different parts of the data in a Pandas DataFrame using: Indexing, Slicing, and Subsetting","title":"Indexing, Slicing and Subsetting"},{"location":"modules/indexing/#ensure-the-pandas-package-is-installed","text":"!pip install pandas matplotlib output Requirement already satisfied: pandas in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (0.25.0) Requirement already satisfied: matplotlib in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (3.1.1) Requirement already satisfied: numpy>=1.13.3 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (1.17.0) Requirement already satisfied: pytz>=2017.2 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2019.1) Requirement already satisfied: python-dateutil>=2.6.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2.8.0) Requirement already satisfied: cycler>=0.10 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (2.4.1.1) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (1.1.0) Requirement already satisfied: six>=1.5 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0) Requirement already satisfied: setuptools in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0)","title":"Ensure the Pandas package is installed"},{"location":"modules/indexing/#loading-our-data","text":"We will continue to use the surveys dataset that we worked with in the last lesson. Let's reopen and read in the data again: # Make sure pandas is loaded import pandas as pd # Read in the survey CSV surveys_df = pd.read_csv(\"surveys.csv\")","title":"Loading our data"},{"location":"modules/indexing/#indexing-and-slicing-in-python","text":"We often want to work with subsets of a DataFrame object. There are different ways to accomplish this including: using labels (column headings), numeric ranges, or specific x,y index locations.","title":"Indexing and Slicing in Python"},{"location":"modules/indexing/#selecting-data-using-labels-column-headings","text":"We use square brackets [] to select a subset of an Python object. For example, we can select all data from a column named species_id from the surveys_df DataFrame by name. There are two ways to do this: # Method 1: select a 'subset' of the data using the column name surveys_df['species_id'].head() output 0 NL 1 NL 2 DM 3 DM 4 DM Name: species_id, dtype: object # Method 2: use the column name as an 'attribute'; gives the same output surveys_df.species_id.head() output 0 NL 1 NL 2 DM 3 DM 4 DM Name: species_id, dtype: object We can also create a new object that contains only the data within the species_id column as follows: # Creates an object, surveys_species, that only contains the `species_id` column surveys_species = surveys_df['species_id'] We can pass a list of column names too, as an index to select columns in that order. This is useful when we need to reorganize our data. NOTE: If a column name is not contained in the DataFrame, an exception (error) will be raised. # Select the species and plot columns from the DataFrame surveys_df[['species_id', 'site_id']].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } species_id site_id 0 NL 2 1 NL 3 2 DM 2 3 DM 7 4 DM 3 What happens if you ask for a column that doesn't exist? surveys_df['speciess'] Outputs: --------------------------------------------------------------------------- KeyError Traceback (most recent call last) /Applications/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance) 2392 try: -> 2393 return self._engine.get_loc(key) 2394 except KeyError: pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5239)() pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20405)() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20359)() KeyError: 'speciess' During handling of the above exception, another exception occurred: KeyError Traceback (most recent call last) <ipython-input-7-7d65fa0158b8> in <module>() 1 2 # What happens if you ask for a column that doesn't exist? ----> 3 surveys_df['speciess'] 4 /Applications/anaconda/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key) 2060 return self._getitem_multilevel(key) 2061 else: -> 2062 return self._getitem_column(key) 2063 2064 def _getitem_column(self, key): /Applications/anaconda/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key) 2067 # get column 2068 if self.columns.is_unique: -> 2069 return self._get_item_cache(key) 2070 2071 # duplicate columns & possible reduce dimensionality /Applications/anaconda/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item) 1532 res = cache.get(item) 1533 if res is None: -> 1534 values = self._data.get(item) 1535 res = self._box_item_values(item, values) 1536 cache[item] = res /Applications/anaconda/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath) 3588 3589 if not isnull(item): -> 3590 loc = self.items.get_loc(item) 3591 else: 3592 indexer = np.arange(len(self.items))[isnull(self.items)] /Applications/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance) 2393 return self._engine.get_loc(key) 2394 except KeyError: -> 2395 return self._engine.get_loc(self._maybe_cast_indexer(key)) 2396 2397 indexer = self.get_indexer([key], method=method, tolerance=tolerance) pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5239)() pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20405)() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20359)() KeyError: 'speciess' Python tells us what type of error it is in the traceback, at the bottom it says KeyError: 'speciess' which means that speciess is not a column name (or Key in the related python data type dictionary). # What happens when you flip the order? surveys_df[['site_id', 'species_id']].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } site_id species_id 0 2 NL 1 3 NL 2 2 DM 3 7 DM 4 3 DM","title":"Selecting data using Labels (Column Headings)"},{"location":"modules/indexing/#extracting-range-based-subsets-slicing","text":"REMINDER : Python Uses 0-based Indexing Let's remind ourselves that Python uses 0-based indexing. This means that the first element in an object is located at position 0. This is different from other tools like R and Matlab that index elements within objects starting at 1. # Create a list of numbers: a = [1, 2, 3, 4, 5]","title":"Extracting Range based Subsets: Slicing"},{"location":"modules/indexing/#challenge-extracting-data","text":"What value does the code a[0] return? How about this: a[5] In the example above, calling a[5] returns an error. Why is that? What about a[len(a)] ?","title":"Challenge - Extracting data"},{"location":"modules/indexing/#solutions-extracting-data","text":"","title":"Solutions - Extracting data"},{"location":"modules/indexing/#slicing-subsets-of-rows-in-python","text":"Slicing using the [] operator selects a set of rows and/or columns from a DataFrame. To slice out a set of rows, you use the following syntax: data[start:stop] . When slicing in pandas the start bound is included in the output. The stop bound is one step BEYOND the row you want to select. So if you want to select rows 0, 1 and 2 your code would look like this with our surveys data : # Select rows 0, 1, 2 (row 3 is not selected) surveys_df[0:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN The stop bound in Python is different from what you might be used to in languages like Matlab and R. Now lets select the first 5 rows (rows 0, 1, 2, 3, 4) . surveys_df[:5] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN # Select the last element in the list # (the slice starts at the last element, and ends at the end of the list) surveys_df[-1:] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 35548 35549 12 31 2002 5 NaN NaN NaN NaN We can also reassign values within subsets of our DataFrame. Let's create a brand new clean dataframe from the original data CSV file. surveys_df = pd.read_csv(\"surveys.csv\")","title":"Slicing Subsets of Rows in Python"},{"location":"modules/indexing/#slicing-subsets-of-rows-and-columns-in-python","text":"We can select specific ranges of our data in both the row and column directions using either label or integer-based indexing. loc is primarily label based indexing. Integers may be used but they are interpreted as a label . iloc is primarily integer based indexing To select a subset of rows and columns from our DataFrame, we can use the iloc method. For example, we can select month, day and year (columns 2, 3 and 4 if we start counting at 1), like this: iloc[row slicing, column slicing] surveys_df.iloc[0:3, 1:4] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } month day year 0 7 16 1977 1 7 16 1977 2 7 16 1977 Notice that we asked for a slice from 0:3. This yielded 3 rows of data. When you ask for 0:3, you are actually telling Python to start at index 0 and select rows 0, 1, 2 up to but not including 3 . Let's explore some other ways to index and select subsets of data: # Select all columns for rows of index values 0 and 10 surveys_df.loc[[0, 10], :] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN # What does this do? surveys_df.loc[0, ['species_id', 'site_id', 'weight']] output species_id NL site_id 2 weight NaN Name: 0, dtype: object # What happens when you type the code below? surveys_df.loc[[0, 10, 35549], :] output /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages/pandas/core/indexing.py:1404: FutureWarning: Passing list-likes to .loc or [] with any missing label will raise KeyError in the future, you can use .reindex() as an alternative. See the documentation here: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike return self._getitem_tuple(key) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1.0 7.0 16.0 1977.0 2.0 NL M 32.0 NaN 10 11.0 7.0 16.0 1977.0 5.0 DS F 53.0 NaN 35549 NaN NaN NaN NaN NaN NaN NaN NaN NaN NOTE : Labels must be found in the DataFrame or you will get a KeyError . Indexing by labels loc differs from indexing by integers iloc . With loc , the both start bound and the stop bound are inclusive . When using loc , integers can be used, but the integers refer to the index label and not the position. For example, using loc and select 1:4 will get a different result than using iloc to select rows 1:4. We can also select a specific data value using a row and column location within the DataFrame and iloc indexing: # Syntax for iloc indexing to finding a specific data element dat.iloc[row, column] In following iloc example: surveys_df.iloc[2, 6] output 'F' Remember that Python indexing begins at 0. So, the index location [2, 6] selects the element that is 3 rows down and 7 columns over in the DataFrame.","title":"Slicing Subsets of Rows and Columns in Python"},{"location":"modules/indexing/#challenge-range","text":"What happens when you execute: surveys_df[0:1] surveys_df[:4] surveys_df[:-1] What happens when you call: surveys_df.iloc[0:4, 1:4]","title":"Challenge - Range"},{"location":"modules/indexing/#subsetting-data-using-criteria","text":"We can also select a subset of our data using criteria. For example, we can select all rows that have a year value of 2002 : surveys_df[surveys_df.year == 2002].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 33320 33321 1 12 2002 1 DM M 38.0 44.0 33321 33322 1 12 2002 1 DO M 37.0 58.0 33322 33323 1 12 2002 1 PB M 28.0 45.0 33323 33324 1 12 2002 1 AB NaN NaN NaN 33324 33325 1 12 2002 1 DO M 35.0 29.0 Or we can select all rows that do not contain the year 2002 : surveys_df[surveys_df.year != 2002] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN 5 6 7 16 1977 1 PF M 14.0 NaN 6 7 7 16 1977 2 PE F NaN NaN 7 8 7 16 1977 1 DM M 37.0 NaN 8 9 7 16 1977 1 DM F 34.0 NaN 9 10 7 16 1977 6 PF F 20.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN 11 12 7 16 1977 7 DM M 38.0 NaN 12 13 7 16 1977 3 DM M 35.0 NaN 13 14 7 16 1977 8 DM NaN NaN NaN 14 15 7 16 1977 6 DM F 36.0 NaN 15 16 7 16 1977 4 DM F 36.0 NaN 16 17 7 16 1977 3 DS F 48.0 NaN 17 18 7 16 1977 2 PP M 22.0 NaN 18 19 7 16 1977 4 PF NaN NaN NaN 19 20 7 17 1977 11 DS F 48.0 NaN 20 21 7 17 1977 14 DM F 34.0 NaN 21 22 7 17 1977 15 NL F 31.0 NaN 22 23 7 17 1977 13 DM M 36.0 NaN 23 24 7 17 1977 13 SH M 21.0 NaN 24 25 7 17 1977 9 DM M 35.0 NaN 25 26 7 17 1977 15 DM M 31.0 NaN 26 27 7 17 1977 15 DM M 36.0 NaN 27 28 7 17 1977 11 DM M 38.0 NaN 28 29 7 17 1977 11 PP M NaN NaN 29 30 7 17 1977 10 DS F 52.0 NaN ... ... ... ... ... ... ... ... ... ... 33290 33291 12 15 2001 23 PE M 20.0 18.0 33291 33292 12 15 2001 23 RM F 16.0 8.0 33292 33293 12 15 2001 20 PE F 20.0 22.0 33293 33294 12 15 2001 20 SH M 25.0 43.0 33294 33295 12 15 2001 20 PB F 27.0 33.0 33295 33296 12 15 2001 20 PB M 25.0 35.0 33296 33297 12 15 2001 20 RM M 16.0 11.0 33297 33298 12 15 2001 20 RM F 16.0 8.0 33298 33299 12 15 2001 20 PB F 25.0 28.0 33299 33300 12 15 2001 20 PB F 26.0 30.0 33300 33301 12 15 2001 20 PB F 27.0 31.0 33301 33302 12 15 2001 24 PE M 20.0 24.0 33302 33303 12 15 2001 24 PE M 20.0 23.0 33303 33304 12 15 2001 24 RM M 16.0 10.0 33304 33305 12 15 2001 7 PB M 29.0 44.0 33305 33306 12 15 2001 7 OT M 19.0 21.0 33306 33307 12 15 2001 7 OT M 20.0 19.0 33307 33308 12 15 2001 7 PP M 24.0 16.0 33308 33309 12 16 2001 3 NaN NaN NaN NaN 33309 33310 12 16 2001 4 NaN NaN NaN NaN 33310 33311 12 16 2001 5 NaN NaN NaN NaN 33311 33312 12 16 2001 6 NaN NaN NaN NaN 33312 33313 12 16 2001 8 NaN NaN NaN NaN 33313 33314 12 16 2001 9 NaN NaN NaN NaN 33314 33315 12 16 2001 10 NaN NaN NaN NaN 33315 33316 12 16 2001 11 NaN NaN NaN NaN 33316 33317 12 16 2001 13 NaN NaN NaN NaN 33317 33318 12 16 2001 14 NaN NaN NaN NaN 33318 33319 12 16 2001 15 NaN NaN NaN NaN 33319 33320 12 16 2001 16 NaN NaN NaN NaN 33320 rows \u00d7 9 columns We can define sets of criteria too: surveys_df[(surveys_df.year >= 1980) & (surveys_df.year <= 1985)] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 2270 2271 1 15 1980 8 DO M 35.0 53.0 2271 2272 1 15 1980 11 PF F 16.0 10.0 2272 2273 1 15 1980 18 DM F 34.0 33.0 2273 2274 1 15 1980 11 DM M 38.0 37.0 2274 2275 1 15 1980 8 DO F 33.0 29.0 2275 2276 1 15 1980 11 DS M 47.0 132.0 2276 2277 1 15 1980 8 PF M 15.0 8.0 2277 2278 1 15 1980 9 OT M 21.0 23.0 2278 2279 1 15 1980 11 DM F 36.0 36.0 2279 2280 1 15 1980 21 OT F 20.0 21.0 2280 2281 1 15 1980 11 OL M 20.0 29.0 2281 2282 1 15 1980 17 DM F 36.0 49.0 2282 2283 1 15 1980 11 OL M 21.0 23.0 2283 2284 1 15 1980 9 OL M 20.0 32.0 2284 2285 1 15 1980 10 OL F 20.0 24.0 2285 2286 1 15 1980 11 DM M 38.0 47.0 2286 2287 1 15 1980 21 OT M 19.0 22.0 2287 2288 1 15 1980 19 RM F 17.0 12.0 2288 2289 1 15 1980 20 DS F 52.0 150.0 2289 2290 1 15 1980 11 DM M 37.0 49.0 2290 2291 1 15 1980 9 OL F 21.0 34.0 2291 2292 1 15 1980 12 DM F 35.0 40.0 2292 2293 1 15 1980 18 DS F 51.0 132.0 2293 2294 1 15 1980 22 DM F 34.0 25.0 2294 2295 1 15 1980 9 OL M 21.0 36.0 2295 2296 1 15 1980 8 DO F 34.0 50.0 2296 2297 1 15 1980 11 DM M 37.0 45.0 2297 2298 1 15 1980 17 DM M 35.0 47.0 2298 2299 1 15 1980 9 DM M 38.0 46.0 2299 2300 1 15 1980 18 DM F 32.0 29.0 ... ... ... ... ... ... ... ... ... ... 11197 11198 12 8 1985 4 DS M 45.0 129.0 11198 11199 12 8 1985 8 DM F 38.0 42.0 11199 11200 12 8 1985 7 AB NaN NaN NaN 11200 11201 12 8 1985 5 OL M 21.0 29.0 11201 11202 12 8 1985 9 DM F 35.0 39.0 11202 11203 12 8 1985 7 PE F 17.0 19.0 11203 11204 12 8 1985 3 PP F 22.0 16.0 11204 11205 12 8 1985 5 DO M 37.0 56.0 11205 11206 12 8 1985 11 DM F 38.0 38.0 11206 11207 12 8 1985 2 PE M 18.0 19.0 11207 11208 12 8 1985 8 DS F 50.0 120.0 11208 11209 12 8 1985 2 DO F 37.0 52.0 11209 11210 12 8 1985 2 DM F 35.0 40.0 11210 11211 12 8 1985 13 DM M 37.0 45.0 11211 11212 12 8 1985 4 DS NaN NaN 121.0 11212 11213 12 8 1985 13 AH NaN NaN NaN 11213 11214 12 8 1985 1 DM F 37.0 44.0 11214 11215 12 8 1985 2 NL F 32.0 160.0 11215 11216 12 8 1985 3 RM M 17.0 9.0 11216 11217 12 8 1985 4 OL M 24.0 34.0 11217 11218 12 8 1985 9 DM F 36.0 39.0 11218 11219 12 8 1985 8 DM F 38.0 41.0 11219 11220 12 8 1985 5 DO F 37.0 56.0 11220 11221 12 8 1985 13 AH NaN NaN NaN 11221 11222 12 8 1985 7 AB NaN NaN NaN 11222 11223 12 8 1985 4 DM M 36.0 40.0 11223 11224 12 8 1985 11 DM M 37.0 49.0 11224 11225 12 8 1985 7 PE M 20.0 18.0 11225 11226 12 8 1985 1 DM M 38.0 47.0 11226 11227 12 8 1985 15 NaN NaN NaN NaN 8957 rows \u00d7 9 columns","title":"Subsetting Data using Criteria"},{"location":"modules/indexing/#python-syntax-cheat-sheet","text":"Use can use the syntax below when querying data by criteria from a DataFrame. Experiment with selecting various subsets of the \"surveys\" data. Equals: == Not equals: != Greater than, less than: > or < Greater than or equal to >= Less than or equal to <=","title":"Python Syntax Cheat Sheet"},{"location":"modules/indexing/#challenge-queries","text":"Select a subset of rows in the surveys_df DataFrame that contain data from the year 1999 and that contain weight values less than or equal to 8. How many rows did you end up with? What did your neighbor get? (Extra) Use the isin function to find all plots that contain PB and PL species in the \"surveys\" DataFrame. How many records contain these values? You can use the isin command in Python to query a DataFrame based upon a list of values as follows: surveys_df[surveys_df['species_id'].isin([listGoesHere])]","title":"Challenge - Queries"},{"location":"modules/indexing/#extra-challenges","text":"(Extra) Create a query that finds all rows with a weight value greater than ( > ) or equal to 0. (Extra) The ~ symbol in Python can be used to return the OPPOSITE of the selection that you specify in Python. It is equivalent to is not in . Write a query that selects all rows with sex NOT equal to 'M' or 'F' in the \"surveys\" data.","title":"Extra Challenges"},{"location":"modules/indexing/#using-masks-to-identify-a-specific-condition","text":"A mask can be useful to locate where a particular subset of values exist or don't exist - for example, NaN, or \"Not a Number\" values. To understand masks, we also need to understand BOOLEAN objects in Python. Boolean values include True or False . For example, # Set x to 5 x = 5 # What does the code below return? x > 5 output False # How about this? x == 5 output True","title":"Using masks to identify a specific condition"},{"location":"modules/indexing/#extra-challenges-putting-it-all-together","text":"Create a new DataFrame that only contains observations with sex values that are not female or male. Assign each sex value in the new DataFrame to a new value of 'x'. Determine the number of null values in the subset. Create a new DataFrame that contains only observations that are of sex male or female and where weight values are greater than 0. Create a stacked bar plot of average weight by plot with male vs female values stacked for each plot. Count the number of missing values per column. Hint: The method .count() gives you the number of non-NA observations per column. # Solution extra challenge 2 # selection of the data with isin stack_selection = surveys_df[(surveys_df['sex'].isin(['M', 'F'])) & surveys_df[\"weight\"] > 0.][[\"sex\", \"weight\", \"site_id\"]] # calculate the mean weight for each site id and sex combination: stack_selection = stack_selection.groupby([\"site_id\", \"sex\"]).mean().unstack() # Plot inside jupyter notebook %matplotlib inline # and we can make a stacked bar plot from this: stack_selection.plot(kind='bar', stacked=True) output","title":"Extra Challenges - Putting it all together"},{"location":"modules/intro/","text":".output_label { text-align: right; margin: -1em; padding: 0; font-size: 0.5em; color: grey } Python: the basics Python is a general purpose programming language that supports rapid development of scripts and applications. Python's main advantages: Open Source software, supported by Python Software Foundation Available on all major platforms (ie. Windows, Linux and MacOS) It is a general-purpose programming language, designed for readability Supports multiple programming paradigms ('functional', 'object oriented') Very large community with a rich ecosystem of third-party packages Interpreter Python is an interpreted language * which can be used in two ways: \"Interactive\" Mode: It functions like an \"advanced calculator\", executing one command at a time: user:host:~$ python Python 3.5.1 (default, Oct 23 2015, 18:05:06) [GCC 4.8.3] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 2 + 2 4 >>> print(\"Hello World\") Hello World \"Scripting\" Mode: Executing a series of \"commands\" saved in text file, usually with a .py extension after the name of your file: user:host:~$ python my_script.py Hello World Using interactive Python in Jupyter-style notebooks A convenient and powerful way to use interactive-mode Python is via a Jupyter Notebook, or similar browser-based interface. This particularly lends itself to data analysis since the notebook records a history of commands and shows output and graphs immediately in the browser. There are several ways you can run a Jupyter(-style) notebook - locally installed on your computer or hosted as a service on the web. Today we will use a Jupyter notebook service provided by Google: https://colab.research.google.com (Colaboratory). Jupyter-style notebooks: a quick tour Go to https://colab.research.google.com and login with your Google account. Select NEW NOTEBOOK \u2192 NEW PYTHON 3 NOTEBOOK - a new notebook will be created. Type some Python code in the top cell, eg: print(\"Hello Jupyter !\") Shift-Enter to run the contents of the cell You can add new cells. Insert \u2192 Insert Code Cell NOTE: When the text on the left hand of the cell is: In [*] (with an asterisk rather than a number), the cell is still running. It's usually best to wait until one cell has finished running before running the next. Let's begin writing some code in our notebook. print(\"Hello Jupyter !\") output Hello Jupyter ! In Jupyter/Collaboratory, just typing the name of a variable in the cell prints its representation: message = \"Hello again !\" message output 'Hello again !' # A 'hash' symbol denotes a comment # This is a comment. Anything after the 'hash' symbol on the line is ignored by the Python interpreter print(\"No comment\") # comment output No comment Variables and data types Integers, floats, strings a = 5 a output 5 type(a) output int Adding a decimal point creates a float b = 5.0 b output 5.0 type(b) output float int and float are collectively called 'numeric' types (There are also other numeric types like hex for hexidemical and complex for complex numbers) Challenge - Types What is the type of the variable letters defined below ? letters = \"ABACBS\" A) int B) str C) float D) text Write some code the outputs the type - paste your answer into the Etherpad. Strings some_words = \"Python3 strings are Unicode (UTF-8) \u2764\u2764\u2764 \ud83d\ude38 \u86c7\" some_words output 'Python3 strings are Unicode (UTF-8) \u2764\u2764\u2764 \ud83d\ude38 \u86c7' type(some_words) output str The variable some_words is of type str , short for \"string\". Strings hold sequences of characters, which can be letters, numbers, punctuation or more exotic forms of text (even emoji!). Operators We can perform mathematical calculations in Python using the basic operators: + - * / % ** 2 + 2 # Addition output 4 6 * 7 # Multiplication output 42 2 ** 16 # Power output 65536 13 % 5 # Modulo output 3 # int + int = int a = 5 a + 1 output 6 # float + int = float b = 5.0 b + 1 output 6.0 a + b output 10.0 some_words = \"I'm a string\" a = 6 a + some_words Outputs: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-1-781eba7cf148> in <module>() 1 some_words = \"I'm a string\" 2 a = 6 ----> 3 a + some_words TypeError: unsupported operand type(s) for +: 'int' and 'str' str(a) + \" \" + some_words output '5 Python3 strings are Unicode (UTF-8) \u2764\u2764\u2764 \ud83d\ude38 \u86c7' # Shorthand: operators with assignment a += 1 a # Equivalent to: # a = a + 1 output 6 Boolean operations We can also use comparison and logic operators: <, >, ==, !=, <=, >= and statements of identity such as and, or, not . The data type returned by this is called a boolean . 3 > 4 output False True and True output True True or False output True Lists and sequence types Lists numbers = [2, 4, 6, 8, 10] numbers output [2, 4, 6, 8, 10] # `len` get the length of a list len(numbers) output 5 # Lists can contain multiple data types, including other lists mixed_list = [\"asdf\", 2, 3.142, numbers, ['a','b','c']] mixed_list output ['asdf', 2, 3.142, [2, 4, 6, 8, 10], ['a', 'b', 'c']] You can retrieve items from a list by their index . In Python, the first item has an index of 0 (zero). numbers[0] output 2 numbers[3] output 8 You can also assign a new value to any position in the list. numbers[3] = numbers[3] * 100 numbers output [2, 4, 6, 800, 10] You can append items to the end of the list. numbers.append(12) numbers output [2, 4, 6, 800, 10, 12] You can add multiple items to the end of a list with extend . numbers.extend([14, 16, 18]) numbers output [2, 4, 6, 800, 10, 12, 14, 16, 18] Loops A for loop can be used to access the elements in a list or other Python data structure one at a time. We will learn about loops in other lesson. for num in numbers: print(num) output 2 4 6 800 10 12 14 16 18 Indentation is very important in Python. Note that the second line in the example above is indented, indicating the code that is the body of the loop. To find out what methods are available for an object, we can use the built-in help command: help(numbers) output Help on list object: class list(object) | list() -> new empty list | list(iterable) -> new list initialized from iterable's items | | Methods defined here: | | __add__(self, value, /) | Return self+value. | | __contains__(self, key, /) | Return key in self. | | __delitem__(self, key, /) | Delete self[key]. | | __eq__(self, value, /) | Return self==value. | | __ge__(self, value, /) | Return self>=value. | | __getattribute__(self, name, /) | Return getattr(self, name). | | __getitem__(...) | x.__getitem__(y) < ==> x[y] | | __gt__(self, value, /) | Return self>value. | | __iadd__(self, value, /) | Implement self+=value. | | __imul__(self, value, /) | Implement self*=value. | | __init__(self, /, *args, **kwargs) | Initialize self. See help(type(self)) for accurate signature. | | __iter__(self, /) | Implement iter(self). | | __le__(self, value, /) | Return self < =value. | | __len__(self, /) | Return len(self). | | __lt__(self, value, /) | Return self None -- append object to end | | clear(...) | L.clear() -> None -- remove all items from L | | copy(...) | L.copy() -> list -- a shallow copy of L | | count(...) | L.count(value) -> integer -- return number of occurrences of value | | extend(...) | L.extend(iterable) -> None -- extend list by appending elements from the iterable | | index(...) | L.index(value, [start, [stop]]) -> integer -- return first index of value. | Raises ValueError if the value is not present. | | insert(...) | L.insert(index, object) -- insert object before index | | pop(...) | L.pop([index]) -> item -- remove and return item at index (default last). | Raises IndexError if list is empty or index is out of range. | | remove(...) | L.remove(value) -> None -- remove first occurrence of value. | Raises ValueError if the value is not present. | | reverse(...) | L.reverse() -- reverse *IN PLACE* | | sort(...) | L.sort(key=None, reverse=False) -> None -- stable sort *IN PLACE* | | ---------------------------------------------------------------------- | Data and other attributes defined here: | | __hash__ = None Tuples A tuple is similar to a list in that it's an ordered sequence of elements. However, tuples can not be changed once created (they are \"immutable\"). Tuples are created by placing comma-separated values inside parentheses () . tuples_are_immutable = (\"bar\", 100, 200, \"foo\") tuples_are_immutable output ('bar', 100, 200, 'foo') tuples_are_immutable[1] output 100 tuples_are_immutable[1] = 666 Outputs: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-39-c91965b0815a> in <module>() ----> 1 tuples_are_immutable[1] = 666 TypeError: 'tuple' object does not support item assignment Dictionaries Dictionaries are a container that store key-value pairs. They are unordered. Other programming languages might call this a 'hash', 'hashtable' or 'hashmap'. pairs = {'Apple': 1, 'Orange': 2, 'Pear': 4} pairs output {'Apple': 1, 'Orange': 2, 'Pear': 4} pairs['Orange'] output 2 pairs['Orange'] = 16 pairs output {'Apple': 1, 'Orange': 16, 'Pear': 4} The items method returns a sequence of the key-value pairs as tuples. values returns a sequence of just the values. keys returns a sequence of just the keys. In Python 3, the .items() , .values() and .keys() methods return a 'dictionary view' object that behaves like a list or tuple in for loops but doesn't support indexing. 'Dictionary views' stay in sync even when the dictionary changes. You can turn them into a normal list or tuple with the list() or tuple() functions. pairs.items() # list(pairs.items()) output dict_items([('Apple', 1), ('Orange', 16), ('Pear', 4)]) pairs.values() # list(pairs.values()) output dict_values([1, 16, 4]) pairs.keys() # list(pairs.keys()) output dict_keys(['Apple', 'Orange', 'Pear']) len(pairs) output 3 dict_of_dicts = {'first': {1:2, 2: 4, 4: 8, 8: 16}, 'second': {'a': 2.2, 'b': 4.4}} dict_of_dicts output {'first': {1: 2, 2: 4, 4: 8, 8: 16}, 'second': {'a': 2.2, 'b': 4.4}} Challenge - Dictionaries Given the dictionary: jam_ratings = {'Plum': 6, 'Apricot': 2, 'Strawberry': 8} How would you change the value associated with the key Apricot to 9 . A) jam_ratings = {'apricot': 9} B) jam_ratings[9] = 'Apricot' C) jam_ratings['Apricot'] = 9 D) jam_ratings[2] = 'Apricot'","title":"Introduction - the basics of Python"},{"location":"modules/intro/#python-the-basics","text":"Python is a general purpose programming language that supports rapid development of scripts and applications. Python's main advantages: Open Source software, supported by Python Software Foundation Available on all major platforms (ie. Windows, Linux and MacOS) It is a general-purpose programming language, designed for readability Supports multiple programming paradigms ('functional', 'object oriented') Very large community with a rich ecosystem of third-party packages","title":"Python: the basics"},{"location":"modules/intro/#interpreter","text":"Python is an interpreted language * which can be used in two ways: \"Interactive\" Mode: It functions like an \"advanced calculator\", executing one command at a time: user:host:~$ python Python 3.5.1 (default, Oct 23 2015, 18:05:06) [GCC 4.8.3] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 2 + 2 4 >>> print(\"Hello World\") Hello World \"Scripting\" Mode: Executing a series of \"commands\" saved in text file, usually with a .py extension after the name of your file: user:host:~$ python my_script.py Hello World","title":"Interpreter"},{"location":"modules/intro/#using-interactive-python-in-jupyter-style-notebooks","text":"A convenient and powerful way to use interactive-mode Python is via a Jupyter Notebook, or similar browser-based interface. This particularly lends itself to data analysis since the notebook records a history of commands and shows output and graphs immediately in the browser. There are several ways you can run a Jupyter(-style) notebook - locally installed on your computer or hosted as a service on the web. Today we will use a Jupyter notebook service provided by Google: https://colab.research.google.com (Colaboratory).","title":"Using interactive Python in Jupyter-style notebooks"},{"location":"modules/intro/#jupyter-style-notebooks-a-quick-tour","text":"Go to https://colab.research.google.com and login with your Google account. Select NEW NOTEBOOK \u2192 NEW PYTHON 3 NOTEBOOK - a new notebook will be created. Type some Python code in the top cell, eg: print(\"Hello Jupyter !\") Shift-Enter to run the contents of the cell You can add new cells. Insert \u2192 Insert Code Cell NOTE: When the text on the left hand of the cell is: In [*] (with an asterisk rather than a number), the cell is still running. It's usually best to wait until one cell has finished running before running the next. Let's begin writing some code in our notebook. print(\"Hello Jupyter !\") output Hello Jupyter ! In Jupyter/Collaboratory, just typing the name of a variable in the cell prints its representation: message = \"Hello again !\" message output 'Hello again !' # A 'hash' symbol denotes a comment # This is a comment. Anything after the 'hash' symbol on the line is ignored by the Python interpreter print(\"No comment\") # comment output No comment","title":"Jupyter-style notebooks: a quick tour"},{"location":"modules/intro/#variables-and-data-types","text":"","title":"Variables and data types"},{"location":"modules/intro/#integers-floats-strings","text":"a = 5 a output 5 type(a) output int Adding a decimal point creates a float b = 5.0 b output 5.0 type(b) output float int and float are collectively called 'numeric' types (There are also other numeric types like hex for hexidemical and complex for complex numbers)","title":"Integers, floats, strings"},{"location":"modules/intro/#challenge-types","text":"What is the type of the variable letters defined below ? letters = \"ABACBS\" A) int B) str C) float D) text Write some code the outputs the type - paste your answer into the Etherpad.","title":"Challenge - Types"},{"location":"modules/intro/#strings","text":"some_words = \"Python3 strings are Unicode (UTF-8) \u2764\u2764\u2764 \ud83d\ude38 \u86c7\" some_words output 'Python3 strings are Unicode (UTF-8) \u2764\u2764\u2764 \ud83d\ude38 \u86c7' type(some_words) output str The variable some_words is of type str , short for \"string\". Strings hold sequences of characters, which can be letters, numbers, punctuation or more exotic forms of text (even emoji!).","title":"Strings"},{"location":"modules/intro/#operators","text":"We can perform mathematical calculations in Python using the basic operators: + - * / % ** 2 + 2 # Addition output 4 6 * 7 # Multiplication output 42 2 ** 16 # Power output 65536 13 % 5 # Modulo output 3 # int + int = int a = 5 a + 1 output 6 # float + int = float b = 5.0 b + 1 output 6.0 a + b output 10.0 some_words = \"I'm a string\" a = 6 a + some_words Outputs: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-1-781eba7cf148> in <module>() 1 some_words = \"I'm a string\" 2 a = 6 ----> 3 a + some_words TypeError: unsupported operand type(s) for +: 'int' and 'str' str(a) + \" \" + some_words output '5 Python3 strings are Unicode (UTF-8) \u2764\u2764\u2764 \ud83d\ude38 \u86c7' # Shorthand: operators with assignment a += 1 a # Equivalent to: # a = a + 1 output 6","title":"Operators"},{"location":"modules/intro/#boolean-operations","text":"We can also use comparison and logic operators: <, >, ==, !=, <=, >= and statements of identity such as and, or, not . The data type returned by this is called a boolean . 3 > 4 output False True and True output True True or False output True","title":"Boolean operations"},{"location":"modules/intro/#lists-and-sequence-types","text":"","title":"Lists and sequence types"},{"location":"modules/intro/#lists","text":"numbers = [2, 4, 6, 8, 10] numbers output [2, 4, 6, 8, 10] # `len` get the length of a list len(numbers) output 5 # Lists can contain multiple data types, including other lists mixed_list = [\"asdf\", 2, 3.142, numbers, ['a','b','c']] mixed_list output ['asdf', 2, 3.142, [2, 4, 6, 8, 10], ['a', 'b', 'c']] You can retrieve items from a list by their index . In Python, the first item has an index of 0 (zero). numbers[0] output 2 numbers[3] output 8 You can also assign a new value to any position in the list. numbers[3] = numbers[3] * 100 numbers output [2, 4, 6, 800, 10] You can append items to the end of the list. numbers.append(12) numbers output [2, 4, 6, 800, 10, 12] You can add multiple items to the end of a list with extend . numbers.extend([14, 16, 18]) numbers output [2, 4, 6, 800, 10, 12, 14, 16, 18]","title":"Lists"},{"location":"modules/intro/#loops","text":"A for loop can be used to access the elements in a list or other Python data structure one at a time. We will learn about loops in other lesson. for num in numbers: print(num) output 2 4 6 800 10 12 14 16 18 Indentation is very important in Python. Note that the second line in the example above is indented, indicating the code that is the body of the loop. To find out what methods are available for an object, we can use the built-in help command: help(numbers) output Help on list object: class list(object) | list() -> new empty list | list(iterable) -> new list initialized from iterable's items | | Methods defined here: | | __add__(self, value, /) | Return self+value. | | __contains__(self, key, /) | Return key in self. | | __delitem__(self, key, /) | Delete self[key]. | | __eq__(self, value, /) | Return self==value. | | __ge__(self, value, /) | Return self>=value. | | __getattribute__(self, name, /) | Return getattr(self, name). | | __getitem__(...) | x.__getitem__(y) < ==> x[y] | | __gt__(self, value, /) | Return self>value. | | __iadd__(self, value, /) | Implement self+=value. | | __imul__(self, value, /) | Implement self*=value. | | __init__(self, /, *args, **kwargs) | Initialize self. See help(type(self)) for accurate signature. | | __iter__(self, /) | Implement iter(self). | | __le__(self, value, /) | Return self < =value. | | __len__(self, /) | Return len(self). | | __lt__(self, value, /) | Return self None -- append object to end | | clear(...) | L.clear() -> None -- remove all items from L | | copy(...) | L.copy() -> list -- a shallow copy of L | | count(...) | L.count(value) -> integer -- return number of occurrences of value | | extend(...) | L.extend(iterable) -> None -- extend list by appending elements from the iterable | | index(...) | L.index(value, [start, [stop]]) -> integer -- return first index of value. | Raises ValueError if the value is not present. | | insert(...) | L.insert(index, object) -- insert object before index | | pop(...) | L.pop([index]) -> item -- remove and return item at index (default last). | Raises IndexError if list is empty or index is out of range. | | remove(...) | L.remove(value) -> None -- remove first occurrence of value. | Raises ValueError if the value is not present. | | reverse(...) | L.reverse() -- reverse *IN PLACE* | | sort(...) | L.sort(key=None, reverse=False) -> None -- stable sort *IN PLACE* | | ---------------------------------------------------------------------- | Data and other attributes defined here: | | __hash__ = None","title":"Loops"},{"location":"modules/intro/#tuples","text":"A tuple is similar to a list in that it's an ordered sequence of elements. However, tuples can not be changed once created (they are \"immutable\"). Tuples are created by placing comma-separated values inside parentheses () . tuples_are_immutable = (\"bar\", 100, 200, \"foo\") tuples_are_immutable output ('bar', 100, 200, 'foo') tuples_are_immutable[1] output 100 tuples_are_immutable[1] = 666 Outputs: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-39-c91965b0815a> in <module>() ----> 1 tuples_are_immutable[1] = 666 TypeError: 'tuple' object does not support item assignment","title":"Tuples"},{"location":"modules/intro/#dictionaries","text":"Dictionaries are a container that store key-value pairs. They are unordered. Other programming languages might call this a 'hash', 'hashtable' or 'hashmap'. pairs = {'Apple': 1, 'Orange': 2, 'Pear': 4} pairs output {'Apple': 1, 'Orange': 2, 'Pear': 4} pairs['Orange'] output 2 pairs['Orange'] = 16 pairs output {'Apple': 1, 'Orange': 16, 'Pear': 4} The items method returns a sequence of the key-value pairs as tuples. values returns a sequence of just the values. keys returns a sequence of just the keys. In Python 3, the .items() , .values() and .keys() methods return a 'dictionary view' object that behaves like a list or tuple in for loops but doesn't support indexing. 'Dictionary views' stay in sync even when the dictionary changes. You can turn them into a normal list or tuple with the list() or tuple() functions. pairs.items() # list(pairs.items()) output dict_items([('Apple', 1), ('Orange', 16), ('Pear', 4)]) pairs.values() # list(pairs.values()) output dict_values([1, 16, 4]) pairs.keys() # list(pairs.keys()) output dict_keys(['Apple', 'Orange', 'Pear']) len(pairs) output 3 dict_of_dicts = {'first': {1:2, 2: 4, 4: 8, 8: 16}, 'second': {'a': 2.2, 'b': 4.4}} dict_of_dicts output {'first': {1: 2, 2: 4, 4: 8, 8: 16}, 'second': {'a': 2.2, 'b': 4.4}}","title":"Dictionaries"},{"location":"modules/intro/#challenge-dictionaries","text":"Given the dictionary: jam_ratings = {'Plum': 6, 'Apricot': 2, 'Strawberry': 8} How would you change the value associated with the key Apricot to 9 . A) jam_ratings = {'apricot': 9} B) jam_ratings[9] = 'Apricot' C) jam_ratings['Apricot'] = 9 D) jam_ratings[2] = 'Apricot'","title":"Challenge - Dictionaries"},{"location":"modules/loops/","text":".output_label { text-align: right; margin: -1em; padding: 0; font-size: 0.5em; color: grey } Automation with Loops An example task that we might want to repeat is printing each character in a word on a line of its own. word = 'lead' We can access a character in a string using its index. For example, we can get the first character of the word 'lead' , by using word[0] . One way to print each character is to use four print statements: print(word[0]) print(word[1]) print(word[2]) print(word[3]) output l e a d While this works, it's a bad approach for two reasons: It doesn't scale: if we want to print the characters in a string that's hundreds of letters long, we'd be better off just typing them in. It's fragile: if we give it a longer string, it only prints part of the data, and if we give it a shorter one, it produces an error because we're asking for characters that don't exist. Running: word = 'tin' print(word[0]) print(word[1]) print(word[2]) print(word[3]) Gives the error: --------------------------------------------------------------------------- IndexError Traceback (most recent call last) <ipython-input-4-e59d5eac5430> in <module>() 3 print(word[1]) 4 print(word[2]) ----> 5 print(word[3]) IndexError: string index out of range Here's a better approach: word = 'lead' for char in word: print(char) output l e a d This is shorter --- certainly shorter than something that prints every character in a hundred-letter string --- and more robust as well: word = 'oxygen' for char in word: print(char) output o x y g e n The improved version uses a for loop to repeat an operation --- in this case, printing --- once for each thing in a sequence. The general form of a loop is: for variable in collection: # do things with variable Using the oxygen example above, the loop might look like this: where each character ( char ) in the variable word is looped through and printed one character after another. The numbers in the diagram denote which loop cycle the character was printed in (1 being the first loop, and 6 being the final loop). We can call the loop variable anything we like, but there must be a colon at the end of the line starting the loop, and we must indent anything we want to run inside the loop. Unlike many other languages, there is no command to signify the end of the loop body (e.g. end for ); what is indented after the for statement belongs to the loop. What's in a name? In the example above, the loop variable was given the name char as a mnemonic; it is short for 'character'. We can choose any name we want for variables. We might just as easily have chosen the name banana for the loop variable, as long as we use the same name when we invoke the variable inside the loop: word = 'oxygen' for banana in word: print(banana) output o x y g e n It is a good idea to choose variable names that are meaningful, otherwise it would be more difficult to understand what the loop is doing. Here's another loop that repeatedly updates a variable: length = 0 for vowel in 'aeiou': length = length + 1 print('There are', length, 'vowels') output There are 5 vowels It's worth tracing the execution of this little program step by step. Since there are five characters in 'aeiou' , the statement on line 3 will be executed five times. The first time around, length is zero (the value assigned to it on line 1) and vowel is 'a' . The statement adds 1 to the old value of length , producing 1, and updates length to refer to that new value. The next time around, vowel is 'e' and length is 1, so length is updated to be 2. After three more updates, length is 5; since there is nothing left in 'aeiou' for Python to process, the loop finishes and the print statement on line 4 tells us our final answer. Note that a loop variable vowel is just a variable that's being used to record progress in a loop. Challenge - scope of the loop variable In the loop over \"aeiou\" above, does the loop variable vowel exist after the loop has finished ? length = 0 for vowel in 'aeiou': length = length + 1 print('After the loop, `vowel` exists and has the value: ' + vowel) # The loop variable `vowel` exists after the loop is completed, not only inside the loop output After the loop, `vowel` exists and has the value: u Note also that finding the length of a string is such a common operation that Python actually has a built-in function to do it called len : print(len('aeiou')) output 5 len is much faster than any function we could write ourselves, and much easier to read than a two-line loop; it will also give us the length of many other things that we haven't met yet, so we should always use it when we can. From 1 to N Python has a built-in function called range that creates a sequence of numbers. range can accept 1, 2, or 3 parameters. If one parameter is given, range creates an array of that length, starting at zero and incrementing by 1. For example, range(3) produces the numbers 0, 1, 2 . If two parameters are given, range starts at the first and ends just before the second, incrementing by one. For example, range(2, 5) produces 2, 3, 4 . If range is given 3 parameters, it starts at the first one, ends just before the second one, and increments by the third one. For exmaple range(3, 10, 2) produces 3, 5, 7, 9 . Challenge - loop over a range Using range , write a loop that uses range to print the first 3 natural numbers: 1 2 3 Computing Powers With Loops Exponentiation is built into Python: print(5 ** 3) output 125 Challenge - multiplication in a loop Write a loop that calculates the same result as 5 ** 3 using multiplication (and without exponentiation). Bonus challenge: reverse a string Knowing that two strings can be concatenated using the + operator, write a loop that takes a string and produces a new string with the characters in reverse order, so 'Newton' becomes 'notweN' . Enumerate The built-in function enumerate takes a sequence (e.g. a list) and generates a new sequence of the same length. Each element of the new sequence is a pair composed of the index (0, 1, 2,...) and the value from the original sequence: for i, x in enumerate(xs): # Do something with i and x The code above loops through xs , assigning the index to i and the value to x . Bonus challenge: enumeration for computing the value of a polynomial Suppose you have encoded a polynomial as a list of coefficients in the following way: the first element is the constant term, the second element is the coefficient of the linear term, the third is the coefficient of the quadratic term, etc. x = 5 cc = [2, 4, 3] y = cc[0] * x**0 + cc[1] * x**1 + cc[2] * x**2 y = 97 Write a loop using enumerate(cc) which computes the value y of any polynomial, given x and cc .","title":"Loops"},{"location":"modules/loops/#automation-with-loops","text":"An example task that we might want to repeat is printing each character in a word on a line of its own. word = 'lead' We can access a character in a string using its index. For example, we can get the first character of the word 'lead' , by using word[0] . One way to print each character is to use four print statements: print(word[0]) print(word[1]) print(word[2]) print(word[3]) output l e a d While this works, it's a bad approach for two reasons: It doesn't scale: if we want to print the characters in a string that's hundreds of letters long, we'd be better off just typing them in. It's fragile: if we give it a longer string, it only prints part of the data, and if we give it a shorter one, it produces an error because we're asking for characters that don't exist. Running: word = 'tin' print(word[0]) print(word[1]) print(word[2]) print(word[3]) Gives the error: --------------------------------------------------------------------------- IndexError Traceback (most recent call last) <ipython-input-4-e59d5eac5430> in <module>() 3 print(word[1]) 4 print(word[2]) ----> 5 print(word[3]) IndexError: string index out of range Here's a better approach: word = 'lead' for char in word: print(char) output l e a d This is shorter --- certainly shorter than something that prints every character in a hundred-letter string --- and more robust as well: word = 'oxygen' for char in word: print(char) output o x y g e n The improved version uses a for loop to repeat an operation --- in this case, printing --- once for each thing in a sequence. The general form of a loop is: for variable in collection: # do things with variable Using the oxygen example above, the loop might look like this: where each character ( char ) in the variable word is looped through and printed one character after another. The numbers in the diagram denote which loop cycle the character was printed in (1 being the first loop, and 6 being the final loop). We can call the loop variable anything we like, but there must be a colon at the end of the line starting the loop, and we must indent anything we want to run inside the loop. Unlike many other languages, there is no command to signify the end of the loop body (e.g. end for ); what is indented after the for statement belongs to the loop.","title":"Automation with Loops"},{"location":"modules/loops/#whats-in-a-name","text":"In the example above, the loop variable was given the name char as a mnemonic; it is short for 'character'. We can choose any name we want for variables. We might just as easily have chosen the name banana for the loop variable, as long as we use the same name when we invoke the variable inside the loop: word = 'oxygen' for banana in word: print(banana) output o x y g e n It is a good idea to choose variable names that are meaningful, otherwise it would be more difficult to understand what the loop is doing. Here's another loop that repeatedly updates a variable: length = 0 for vowel in 'aeiou': length = length + 1 print('There are', length, 'vowels') output There are 5 vowels It's worth tracing the execution of this little program step by step. Since there are five characters in 'aeiou' , the statement on line 3 will be executed five times. The first time around, length is zero (the value assigned to it on line 1) and vowel is 'a' . The statement adds 1 to the old value of length , producing 1, and updates length to refer to that new value. The next time around, vowel is 'e' and length is 1, so length is updated to be 2. After three more updates, length is 5; since there is nothing left in 'aeiou' for Python to process, the loop finishes and the print statement on line 4 tells us our final answer. Note that a loop variable vowel is just a variable that's being used to record progress in a loop.","title":"What's in a name?"},{"location":"modules/loops/#challenge-scope-of-the-loop-variable","text":"In the loop over \"aeiou\" above, does the loop variable vowel exist after the loop has finished ? length = 0 for vowel in 'aeiou': length = length + 1 print('After the loop, `vowel` exists and has the value: ' + vowel) # The loop variable `vowel` exists after the loop is completed, not only inside the loop output After the loop, `vowel` exists and has the value: u Note also that finding the length of a string is such a common operation that Python actually has a built-in function to do it called len : print(len('aeiou')) output 5 len is much faster than any function we could write ourselves, and much easier to read than a two-line loop; it will also give us the length of many other things that we haven't met yet, so we should always use it when we can.","title":"Challenge - scope of the loop variable"},{"location":"modules/loops/#from-1-to-n","text":"Python has a built-in function called range that creates a sequence of numbers. range can accept 1, 2, or 3 parameters. If one parameter is given, range creates an array of that length, starting at zero and incrementing by 1. For example, range(3) produces the numbers 0, 1, 2 . If two parameters are given, range starts at the first and ends just before the second, incrementing by one. For example, range(2, 5) produces 2, 3, 4 . If range is given 3 parameters, it starts at the first one, ends just before the second one, and increments by the third one. For exmaple range(3, 10, 2) produces 3, 5, 7, 9 .","title":"From 1 to N"},{"location":"modules/loops/#challenge-loop-over-a-range","text":"Using range , write a loop that uses range to print the first 3 natural numbers: 1 2 3","title":"Challenge - loop over a range"},{"location":"modules/loops/#computing-powers-with-loops","text":"Exponentiation is built into Python: print(5 ** 3) output 125","title":"Computing Powers With Loops"},{"location":"modules/loops/#challenge-multiplication-in-a-loop","text":"Write a loop that calculates the same result as 5 ** 3 using multiplication (and without exponentiation).","title":"Challenge - multiplication in a loop"},{"location":"modules/loops/#bonus-challenge-reverse-a-string","text":"Knowing that two strings can be concatenated using the + operator, write a loop that takes a string and produces a new string with the characters in reverse order, so 'Newton' becomes 'notweN' .","title":"Bonus challenge: reverse a string"},{"location":"modules/loops/#enumerate","text":"The built-in function enumerate takes a sequence (e.g. a list) and generates a new sequence of the same length. Each element of the new sequence is a pair composed of the index (0, 1, 2,...) and the value from the original sequence: for i, x in enumerate(xs): # Do something with i and x The code above loops through xs , assigning the index to i and the value to x .","title":"Enumerate"},{"location":"modules/loops/#bonus-challenge-enumeration-for-computing-the-value-of-a-polynomial","text":"Suppose you have encoded a polynomial as a list of coefficients in the following way: the first element is the constant term, the second element is the coefficient of the linear term, the third is the coefficient of the quadratic term, etc. x = 5 cc = [2, 4, 3] y = cc[0] * x**0 + cc[1] * x**1 + cc[2] * x**2 y = 97 Write a loop using enumerate(cc) which computes the value y of any polynomial, given x and cc .","title":"Bonus challenge: enumeration for computing the value of a polynomial"},{"location":"modules/merging_data/","text":".output_label { text-align: right; margin: -1em; padding: 0; font-size: 0.5em; color: grey } Combining DataFrames with Pandas In many \u201creal world\u201d situations, the data that we want to use come in multiple files. We often need to combine these files into a single DataFrame to analyze the data. The pandas package provides various methods for combining DataFrames including merge and concat . To work through the examples below, we first need to load the species and surveys files into pandas DataFrames. Before we start, we will make sure that libraries are currectly installed. !pip install pandas matplotlib Requirement already satisfied: pandas in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (0.23.0) Requirement already satisfied: matplotlib in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (2.2.2) Requirement already satisfied: python-dateutil>=2.5.0 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from pandas) (2.7.3) Requirement already satisfied: pytz>=2011k in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from pandas) (2018.4) Requirement already satisfied: numpy>=1.9.0 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from pandas) (1.14.3) Requirement already satisfied: cycler>=0.10 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: six>=1.10 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from matplotlib) (1.11.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from matplotlib) (2.2.0) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from matplotlib) (1.0.1) Requirement already satisfied: setuptools in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.2.0) You are using pip version 10.0.1, however version 18.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. import pandas as pd surveys_df = pd.read_csv(\"surveys.csv\", keep_default_na=False, na_values=[\"\"]) surveys_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN 5 6 7 16 1977 1 PF M 14.0 NaN 6 7 7 16 1977 2 PE F NaN NaN 7 8 7 16 1977 1 DM M 37.0 NaN 8 9 7 16 1977 1 DM F 34.0 NaN 9 10 7 16 1977 6 PF F 20.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN 11 12 7 16 1977 7 DM M 38.0 NaN 12 13 7 16 1977 3 DM M 35.0 NaN 13 14 7 16 1977 8 DM NaN NaN NaN 14 15 7 16 1977 6 DM F 36.0 NaN 15 16 7 16 1977 4 DM F 36.0 NaN 16 17 7 16 1977 3 DS F 48.0 NaN 17 18 7 16 1977 2 PP M 22.0 NaN 18 19 7 16 1977 4 PF NaN NaN NaN 19 20 7 17 1977 11 DS F 48.0 NaN 20 21 7 17 1977 14 DM F 34.0 NaN 21 22 7 17 1977 15 NL F 31.0 NaN 22 23 7 17 1977 13 DM M 36.0 NaN 23 24 7 17 1977 13 SH M 21.0 NaN 24 25 7 17 1977 9 DM M 35.0 NaN 25 26 7 17 1977 15 DM M 31.0 NaN 26 27 7 17 1977 15 DM M 36.0 NaN 27 28 7 17 1977 11 DM M 38.0 NaN 28 29 7 17 1977 11 PP M NaN NaN 29 30 7 17 1977 10 DS F 52.0 NaN ... ... ... ... ... ... ... ... ... ... 35519 35520 12 31 2002 9 SF NaN 24.0 36.0 35520 35521 12 31 2002 9 DM M 37.0 48.0 35521 35522 12 31 2002 9 DM F 35.0 45.0 35522 35523 12 31 2002 9 DM F 36.0 44.0 35523 35524 12 31 2002 9 PB F 25.0 27.0 35524 35525 12 31 2002 9 OL M 21.0 26.0 35525 35526 12 31 2002 8 OT F 20.0 24.0 35526 35527 12 31 2002 13 DO F 33.0 43.0 35527 35528 12 31 2002 13 US NaN NaN NaN 35528 35529 12 31 2002 13 PB F 25.0 25.0 35529 35530 12 31 2002 13 OT F 20.0 NaN 35530 35531 12 31 2002 13 PB F 27.0 NaN 35531 35532 12 31 2002 14 DM F 34.0 43.0 35532 35533 12 31 2002 14 DM F 36.0 48.0 35533 35534 12 31 2002 14 DM M 37.0 56.0 35534 35535 12 31 2002 14 DM M 37.0 53.0 35535 35536 12 31 2002 14 DM F 35.0 42.0 35536 35537 12 31 2002 14 DM F 36.0 46.0 35537 35538 12 31 2002 15 PB F 26.0 31.0 35538 35539 12 31 2002 15 SF M 26.0 68.0 35539 35540 12 31 2002 15 PB F 26.0 23.0 35540 35541 12 31 2002 15 PB F 24.0 31.0 35541 35542 12 31 2002 15 PB F 26.0 29.0 35542 35543 12 31 2002 15 PB F 27.0 34.0 35543 35544 12 31 2002 15 US NaN NaN NaN 35544 35545 12 31 2002 15 AH NaN NaN NaN 35545 35546 12 31 2002 15 AH NaN NaN NaN 35546 35547 12 31 2002 10 RM F 15.0 14.0 35547 35548 12 31 2002 7 DO M 36.0 51.0 35548 35549 12 31 2002 5 NaN NaN NaN NaN 35549 rows \u00d7 9 columns Concatenating DataFrames We can use the concat function in pandas to append either columns or rows from one DataFrame to another. Let\u2019s grab two subsets of our data to see how this works. # Read in first 10 lines of surveys table survey_sub = surveys_df.head(10) # Grab the last 10 rows survey_sub_last10 = surveys_df.tail(10) # Reset the index values to the second dataframe appends properly survey_sub_last10=survey_sub_last10.reset_index(drop=True) # drop=True option avoids adding new index column with old index values When we concatenate DataFrames, we need to specify the axis. axis=0 tells pandas to stack the second DataFrame under the first one. It will automatically detect whether the column names are the same and will stack accordingly. axis=1 will stack the columns in the second DataFrame to the RIGHT of the first DataFrame. To stack the data vertically, we need to make sure we have the same columns and associated column format in both datasets. When we stack horizonally, we want to make sure what we are doing makes sense (ie the data are related in some way). # Stack the DataFrames on top of each other vertical_stack = pd.concat([survey_sub, survey_sub_last10], axis=0) # Place the DataFrames side by side horizontal_stack = pd.concat([survey_sub, survey_sub_last10], axis=1) Row Index Values and Concat Have a look at the vertical_stack dataframe? Notice anything unusual? The row indexes for the two data frames survey_sub and survey_sub_last10 have been repeated. We can reindex the new dataframe using the reset_index() method. Writing Out Data to CSV We can use the to_csv command to do export a DataFrame in CSV format. Note that the code below will by default save the data into the current working directory. We can save it to a different folder by adding the foldername and a slash to the file vertical_stack.to_csv('foldername/out.csv') . We use the \u2018index=False\u2019 so that pandas doesn\u2019t include the index number for each line. # Write DataFrame to CSV vertical_stack.to_csv('output/out.csv', index=False) Check out your working directory to make sure the CSV wrote out properly, and that you can open it! If you want, try to bring it back into Python to make sure it imports properly. # For kicks read our output back into Python and make sure all looks good new_output = pd.read_csv('output/out.csv', keep_default_na=False, na_values=[\"\"]) Challenge - Combine Data In the data folder, there are two survey data files: survey2001.csv and survey2002.csv. Read the data into Python and combine the files to make one new data frame. Create a plot of average plot weight by year grouped by sex. Export your results as a CSV and make sure it reads back into Python properly. Joining DataFrames When we concatenated our DataFrames we simply added them to each other - stacking them either vertically or side by side. Another way to combine DataFrames is to use columns in each dataset that contain common values (a common unique id). Combining DataFrames using a common field is called \u201cjoining\u201d. The columns containing the common values are called \u201cjoin key(s)\u201d. Joining DataFrames in this way is often useful when one DataFrame is a \u201clookup table\u201d containing additional data that we want to include in the other. NOTE: This process of joining tables is similar to what we do with tables in an SQL database. For example, the species.csv file that we\u2019ve been working with is a lookup table. This table contains the genus, species and taxa code for 55 species. The species code is unique for each line. These species are identified in our survey data as well using the unique species code. Rather than adding 3 more columns for the genus, species and taxa to each of the 35,549 line Survey data table, we can maintain the shorter table with the species information. When we want to access that information, we can create a query that joins the additional columns of information to the Survey data. Storing data in this way has many benefits including: It ensures consistency in the spelling of species attributes (genus, species and taxa) given each species is only entered once. Imagine the possibilities for spelling errors when entering the genus and species thousands of times! It also makes it easy for us to make changes to the species information once without having to find each instance of it in the larger survey data. It optimizes the size of our data. Joining Two DataFrames To better understand joins, let\u2019s grab the first 10 lines of our data as a subset to work with. We\u2019ll use the .head method to do this. We\u2019ll also read in a subset of the species table. # Read in first 10 lines of surveys table survey_sub = surveys_df.head(10) ### Download speciesSubset.csv file from web import urllib.request url = 'https://bit.ly/2DfqN6C' urllib.request.urlretrieve(url, 'speciesSubset.csv') # Import a small subset of the species data designed for this part of the lesson. # It is stored in the data folder. species_sub = pd.read_csv('speciesSubset.csv', keep_default_na=False, na_values=[\"\"]) In this example, species_sub is the lookup table containing genus, species, and taxa names that we want to join with the data in survey_sub to produce a new DataFrame that contains all of the columns from both species_df and survey_df species_sub.columns output Index(['species_id', 'genus', 'species', 'taxa'], dtype='object') survey_sub.columns output Index(['record_id', 'month', 'day', 'year', 'site_id', 'species_id', 'sex', 'hindfoot_length', 'weight'], dtype='object') In our example, the join key is the column containing the two-letter species identifier, which is called species_id . Now that we know the fields with the common species ID attributes in each DataFrame, we are almost ready to join our data. However, since there are different types of joins , we also need to decide which type of join makes sense for our analysis. Inner Joins The most common type of join is called an inner join . An inner join combines two DataFrames based on a join key and returns a new DataFrame that contains only those rows that have matching values in both of the original DataFrames. Inner joins yield a DataFrame that contains only rows where the value being joins exists in BOTH tables. An example of an inner join, adapted from this page is below: The pandas function for performing joins is called merge and an Inner join is the default option: merged_inner = pd.merge(left=survey_sub,right=species_sub, left_on='species_id', right_on='species_id') # In this case `species_id` is the only column name in both dataframes, so if we skippd `left_on` # And `right_on` arguments we would still get the same result # What's the size of the output data? merged_inner.shape merged_inner .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight genus species taxa 0 1 7 16 1977 2 NL M 32.0 NaN Neotoma albigula Rodent 1 2 7 16 1977 3 NL M 33.0 NaN Neotoma albigula Rodent 2 3 7 16 1977 2 DM F 37.0 NaN Dipodomys merriami Rodent 3 4 7 16 1977 7 DM M 36.0 NaN Dipodomys merriami Rodent 4 5 7 16 1977 3 DM M 35.0 NaN Dipodomys merriami Rodent 5 8 7 16 1977 1 DM M 37.0 NaN Dipodomys merriami Rodent 6 9 7 16 1977 1 DM F 34.0 NaN Dipodomys merriami Rodent 7 7 7 16 1977 2 PE F NaN NaN Peromyscus eremicus Rodent The result of an inner join of survey_sub and species_sub is a new DataFrame that contains the combined set of columns from survey_sub and species_sub . It only contains rows that have two-letter species codes that are the same in both the survey_sub and species_sub DataFrames. In other words, if a row in survey_sub has a value of species_id that does not appear in the species_id column of species , it will not be included in the DataFrame returned by an inner join. Similarly, if a row in species_sub has a value of species_id that does not appear in the species_id column of survey_sub , that row will not be included in the DataFrame returned by an inner join . The two DataFrames that we want to join are passed to the merge function using the left and right argument. The left_on='species' argument tells merge to use the species_id column as the join key from survey_sub (the left DataFrame). Similarly , the right_on='species_id' argument tells merge to use the species_id column as the join key from species_sub (the right DataFrame). For inner joins, the order of the left and right arguments does not matter. The result merged_inner DataFrame contains all of the columns from survey_sub (record id, month, day, etc.) as well as all the columns from species_sub (species_id, genus, species, and taxa). Notice that merged_inner has fewer rows than survey_sub . This is an indication that there were rows in surveys_df with value(s) for species_id that do not exist as value(s) for species_id in species_df . Left Joins What if we want to add information from species_sub to survey_sub without losing any of the information from survey_sub ? In this case, we use a different type of join called a \u201cleft outer join\u201d, or a \u201cleft join\u201d. Like an inner join, a left join uses join keys to combine two DataFrames. Unlike an inner join, a left join will return all of the rows from the left DataFrame, even those rows whose join key(s) do not have values in the right DataFrame. Rows in the left DataFrame that are missing values for the join key(s) in the right DataFrame will simply have null (i.e., NaN or None) values for those columns in the resulting joined DataFrame. Note: a left join will still discard rows from the right DataFrame that do not have values for the join key(s) in the left DataFrame. A left join is performed in pandas by calling the same merge function used for inner join, but using the how='left' argument: merged_left = pd.merge(left=survey_sub,right=species_sub, how='left', left_on='species_id', right_on='species_id') merged_left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight genus species taxa 0 1 7 16 1977 2 NL M 32.0 NaN Neotoma albigula Rodent 1 2 7 16 1977 3 NL M 33.0 NaN Neotoma albigula Rodent 2 3 7 16 1977 2 DM F 37.0 NaN Dipodomys merriami Rodent 3 4 7 16 1977 7 DM M 36.0 NaN Dipodomys merriami Rodent 4 5 7 16 1977 3 DM M 35.0 NaN Dipodomys merriami Rodent 5 6 7 16 1977 1 PF M 14.0 NaN NaN NaN NaN 6 7 7 16 1977 2 PE F NaN NaN Peromyscus eremicus Rodent 7 8 7 16 1977 1 DM M 37.0 NaN Dipodomys merriami Rodent 8 9 7 16 1977 1 DM F 34.0 NaN Dipodomys merriami Rodent 9 10 7 16 1977 6 PF F 20.0 NaN NaN NaN NaN The result DataFrame from a left join ( merged_left ) looks very much like the result DataFrame from an inner join ( merged_inner ) in terms of the columns it contains. However, unlike merged_inner , merged_left contains the same number of rows as the original survey_sub DataFrame. When we inspect merged_left , we find there are rows where the information that should have come from species_sub (i.e., species_id, genus, and taxa) is missing (they contain NaN values): merged_left[ pd.isnull(merged_left.genus) ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight genus species taxa 5 6 7 16 1977 1 PF M 14.0 NaN NaN NaN NaN 9 10 7 16 1977 6 PF F 20.0 NaN NaN NaN NaN These rows are the ones where the value of species_id from survey_sub (in this case, PF) does not occur in species_sub . Other join types The pandas merge function supports two other join types: Right (outer) join: Invoked by passing how='right' as an argument. Similar to a left join, except all rows from the right DataFrame are kept, while rows from the left DataFrame without matching join key(s) values are discarded. Full (outer) join: Invoked by passing how='outer' as an argument. This join type returns the all pairwise combinations of rows from both DataFrames; i.e., the result DataFrame will NaN where data is missing in one of the dataframes. This join type is very rarely used. Extra Challenge 1 Create a new DataFrame by joining the contents of the surveys.csv and speciesSubset.csv tables. Then calculate and plot the distribution of: taxa by plot taxa by sex by plot","title":"Combining DataFrames with Pandas"},{"location":"modules/merging_data/#combining-dataframes-with-pandas","text":"In many \u201creal world\u201d situations, the data that we want to use come in multiple files. We often need to combine these files into a single DataFrame to analyze the data. The pandas package provides various methods for combining DataFrames including merge and concat . To work through the examples below, we first need to load the species and surveys files into pandas DataFrames. Before we start, we will make sure that libraries are currectly installed. !pip install pandas matplotlib Requirement already satisfied: pandas in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (0.23.0) Requirement already satisfied: matplotlib in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (2.2.2) Requirement already satisfied: python-dateutil>=2.5.0 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from pandas) (2.7.3) Requirement already satisfied: pytz>=2011k in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from pandas) (2018.4) Requirement already satisfied: numpy>=1.9.0 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from pandas) (1.14.3) Requirement already satisfied: cycler>=0.10 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: six>=1.10 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from matplotlib) (1.11.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from matplotlib) (2.2.0) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from matplotlib) (1.0.1) Requirement already satisfied: setuptools in /Users/asha0035/.local/share/virtualenvs/python-workshop-base-LFzz33nP/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.2.0) You are using pip version 10.0.1, however version 18.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. import pandas as pd surveys_df = pd.read_csv(\"surveys.csv\", keep_default_na=False, na_values=[\"\"]) surveys_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN 5 6 7 16 1977 1 PF M 14.0 NaN 6 7 7 16 1977 2 PE F NaN NaN 7 8 7 16 1977 1 DM M 37.0 NaN 8 9 7 16 1977 1 DM F 34.0 NaN 9 10 7 16 1977 6 PF F 20.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN 11 12 7 16 1977 7 DM M 38.0 NaN 12 13 7 16 1977 3 DM M 35.0 NaN 13 14 7 16 1977 8 DM NaN NaN NaN 14 15 7 16 1977 6 DM F 36.0 NaN 15 16 7 16 1977 4 DM F 36.0 NaN 16 17 7 16 1977 3 DS F 48.0 NaN 17 18 7 16 1977 2 PP M 22.0 NaN 18 19 7 16 1977 4 PF NaN NaN NaN 19 20 7 17 1977 11 DS F 48.0 NaN 20 21 7 17 1977 14 DM F 34.0 NaN 21 22 7 17 1977 15 NL F 31.0 NaN 22 23 7 17 1977 13 DM M 36.0 NaN 23 24 7 17 1977 13 SH M 21.0 NaN 24 25 7 17 1977 9 DM M 35.0 NaN 25 26 7 17 1977 15 DM M 31.0 NaN 26 27 7 17 1977 15 DM M 36.0 NaN 27 28 7 17 1977 11 DM M 38.0 NaN 28 29 7 17 1977 11 PP M NaN NaN 29 30 7 17 1977 10 DS F 52.0 NaN ... ... ... ... ... ... ... ... ... ... 35519 35520 12 31 2002 9 SF NaN 24.0 36.0 35520 35521 12 31 2002 9 DM M 37.0 48.0 35521 35522 12 31 2002 9 DM F 35.0 45.0 35522 35523 12 31 2002 9 DM F 36.0 44.0 35523 35524 12 31 2002 9 PB F 25.0 27.0 35524 35525 12 31 2002 9 OL M 21.0 26.0 35525 35526 12 31 2002 8 OT F 20.0 24.0 35526 35527 12 31 2002 13 DO F 33.0 43.0 35527 35528 12 31 2002 13 US NaN NaN NaN 35528 35529 12 31 2002 13 PB F 25.0 25.0 35529 35530 12 31 2002 13 OT F 20.0 NaN 35530 35531 12 31 2002 13 PB F 27.0 NaN 35531 35532 12 31 2002 14 DM F 34.0 43.0 35532 35533 12 31 2002 14 DM F 36.0 48.0 35533 35534 12 31 2002 14 DM M 37.0 56.0 35534 35535 12 31 2002 14 DM M 37.0 53.0 35535 35536 12 31 2002 14 DM F 35.0 42.0 35536 35537 12 31 2002 14 DM F 36.0 46.0 35537 35538 12 31 2002 15 PB F 26.0 31.0 35538 35539 12 31 2002 15 SF M 26.0 68.0 35539 35540 12 31 2002 15 PB F 26.0 23.0 35540 35541 12 31 2002 15 PB F 24.0 31.0 35541 35542 12 31 2002 15 PB F 26.0 29.0 35542 35543 12 31 2002 15 PB F 27.0 34.0 35543 35544 12 31 2002 15 US NaN NaN NaN 35544 35545 12 31 2002 15 AH NaN NaN NaN 35545 35546 12 31 2002 15 AH NaN NaN NaN 35546 35547 12 31 2002 10 RM F 15.0 14.0 35547 35548 12 31 2002 7 DO M 36.0 51.0 35548 35549 12 31 2002 5 NaN NaN NaN NaN 35549 rows \u00d7 9 columns","title":"Combining DataFrames with Pandas"},{"location":"modules/merging_data/#concatenating-dataframes","text":"We can use the concat function in pandas to append either columns or rows from one DataFrame to another. Let\u2019s grab two subsets of our data to see how this works. # Read in first 10 lines of surveys table survey_sub = surveys_df.head(10) # Grab the last 10 rows survey_sub_last10 = surveys_df.tail(10) # Reset the index values to the second dataframe appends properly survey_sub_last10=survey_sub_last10.reset_index(drop=True) # drop=True option avoids adding new index column with old index values When we concatenate DataFrames, we need to specify the axis. axis=0 tells pandas to stack the second DataFrame under the first one. It will automatically detect whether the column names are the same and will stack accordingly. axis=1 will stack the columns in the second DataFrame to the RIGHT of the first DataFrame. To stack the data vertically, we need to make sure we have the same columns and associated column format in both datasets. When we stack horizonally, we want to make sure what we are doing makes sense (ie the data are related in some way). # Stack the DataFrames on top of each other vertical_stack = pd.concat([survey_sub, survey_sub_last10], axis=0) # Place the DataFrames side by side horizontal_stack = pd.concat([survey_sub, survey_sub_last10], axis=1)","title":"Concatenating DataFrames"},{"location":"modules/merging_data/#row-index-values-and-concat","text":"Have a look at the vertical_stack dataframe? Notice anything unusual? The row indexes for the two data frames survey_sub and survey_sub_last10 have been repeated. We can reindex the new dataframe using the reset_index() method.","title":"Row Index Values and Concat"},{"location":"modules/merging_data/#writing-out-data-to-csv","text":"We can use the to_csv command to do export a DataFrame in CSV format. Note that the code below will by default save the data into the current working directory. We can save it to a different folder by adding the foldername and a slash to the file vertical_stack.to_csv('foldername/out.csv') . We use the \u2018index=False\u2019 so that pandas doesn\u2019t include the index number for each line. # Write DataFrame to CSV vertical_stack.to_csv('output/out.csv', index=False) Check out your working directory to make sure the CSV wrote out properly, and that you can open it! If you want, try to bring it back into Python to make sure it imports properly. # For kicks read our output back into Python and make sure all looks good new_output = pd.read_csv('output/out.csv', keep_default_na=False, na_values=[\"\"])","title":"Writing Out Data to CSV"},{"location":"modules/merging_data/#challenge-combine-data","text":"In the data folder, there are two survey data files: survey2001.csv and survey2002.csv. Read the data into Python and combine the files to make one new data frame. Create a plot of average plot weight by year grouped by sex. Export your results as a CSV and make sure it reads back into Python properly.","title":"Challenge - Combine Data"},{"location":"modules/merging_data/#joining-dataframes","text":"When we concatenated our DataFrames we simply added them to each other - stacking them either vertically or side by side. Another way to combine DataFrames is to use columns in each dataset that contain common values (a common unique id). Combining DataFrames using a common field is called \u201cjoining\u201d. The columns containing the common values are called \u201cjoin key(s)\u201d. Joining DataFrames in this way is often useful when one DataFrame is a \u201clookup table\u201d containing additional data that we want to include in the other. NOTE: This process of joining tables is similar to what we do with tables in an SQL database. For example, the species.csv file that we\u2019ve been working with is a lookup table. This table contains the genus, species and taxa code for 55 species. The species code is unique for each line. These species are identified in our survey data as well using the unique species code. Rather than adding 3 more columns for the genus, species and taxa to each of the 35,549 line Survey data table, we can maintain the shorter table with the species information. When we want to access that information, we can create a query that joins the additional columns of information to the Survey data. Storing data in this way has many benefits including: It ensures consistency in the spelling of species attributes (genus, species and taxa) given each species is only entered once. Imagine the possibilities for spelling errors when entering the genus and species thousands of times! It also makes it easy for us to make changes to the species information once without having to find each instance of it in the larger survey data. It optimizes the size of our data.","title":"Joining DataFrames"},{"location":"modules/merging_data/#joining-two-dataframes","text":"To better understand joins, let\u2019s grab the first 10 lines of our data as a subset to work with. We\u2019ll use the .head method to do this. We\u2019ll also read in a subset of the species table. # Read in first 10 lines of surveys table survey_sub = surveys_df.head(10) ### Download speciesSubset.csv file from web import urllib.request url = 'https://bit.ly/2DfqN6C' urllib.request.urlretrieve(url, 'speciesSubset.csv') # Import a small subset of the species data designed for this part of the lesson. # It is stored in the data folder. species_sub = pd.read_csv('speciesSubset.csv', keep_default_na=False, na_values=[\"\"]) In this example, species_sub is the lookup table containing genus, species, and taxa names that we want to join with the data in survey_sub to produce a new DataFrame that contains all of the columns from both species_df and survey_df species_sub.columns output Index(['species_id', 'genus', 'species', 'taxa'], dtype='object') survey_sub.columns output Index(['record_id', 'month', 'day', 'year', 'site_id', 'species_id', 'sex', 'hindfoot_length', 'weight'], dtype='object') In our example, the join key is the column containing the two-letter species identifier, which is called species_id . Now that we know the fields with the common species ID attributes in each DataFrame, we are almost ready to join our data. However, since there are different types of joins , we also need to decide which type of join makes sense for our analysis.","title":"Joining Two DataFrames"},{"location":"modules/merging_data/#inner-joins","text":"The most common type of join is called an inner join . An inner join combines two DataFrames based on a join key and returns a new DataFrame that contains only those rows that have matching values in both of the original DataFrames. Inner joins yield a DataFrame that contains only rows where the value being joins exists in BOTH tables. An example of an inner join, adapted from this page is below: The pandas function for performing joins is called merge and an Inner join is the default option: merged_inner = pd.merge(left=survey_sub,right=species_sub, left_on='species_id', right_on='species_id') # In this case `species_id` is the only column name in both dataframes, so if we skippd `left_on` # And `right_on` arguments we would still get the same result # What's the size of the output data? merged_inner.shape merged_inner .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight genus species taxa 0 1 7 16 1977 2 NL M 32.0 NaN Neotoma albigula Rodent 1 2 7 16 1977 3 NL M 33.0 NaN Neotoma albigula Rodent 2 3 7 16 1977 2 DM F 37.0 NaN Dipodomys merriami Rodent 3 4 7 16 1977 7 DM M 36.0 NaN Dipodomys merriami Rodent 4 5 7 16 1977 3 DM M 35.0 NaN Dipodomys merriami Rodent 5 8 7 16 1977 1 DM M 37.0 NaN Dipodomys merriami Rodent 6 9 7 16 1977 1 DM F 34.0 NaN Dipodomys merriami Rodent 7 7 7 16 1977 2 PE F NaN NaN Peromyscus eremicus Rodent The result of an inner join of survey_sub and species_sub is a new DataFrame that contains the combined set of columns from survey_sub and species_sub . It only contains rows that have two-letter species codes that are the same in both the survey_sub and species_sub DataFrames. In other words, if a row in survey_sub has a value of species_id that does not appear in the species_id column of species , it will not be included in the DataFrame returned by an inner join. Similarly, if a row in species_sub has a value of species_id that does not appear in the species_id column of survey_sub , that row will not be included in the DataFrame returned by an inner join . The two DataFrames that we want to join are passed to the merge function using the left and right argument. The left_on='species' argument tells merge to use the species_id column as the join key from survey_sub (the left DataFrame). Similarly , the right_on='species_id' argument tells merge to use the species_id column as the join key from species_sub (the right DataFrame). For inner joins, the order of the left and right arguments does not matter. The result merged_inner DataFrame contains all of the columns from survey_sub (record id, month, day, etc.) as well as all the columns from species_sub (species_id, genus, species, and taxa). Notice that merged_inner has fewer rows than survey_sub . This is an indication that there were rows in surveys_df with value(s) for species_id that do not exist as value(s) for species_id in species_df .","title":"Inner Joins"},{"location":"modules/merging_data/#left-joins","text":"What if we want to add information from species_sub to survey_sub without losing any of the information from survey_sub ? In this case, we use a different type of join called a \u201cleft outer join\u201d, or a \u201cleft join\u201d. Like an inner join, a left join uses join keys to combine two DataFrames. Unlike an inner join, a left join will return all of the rows from the left DataFrame, even those rows whose join key(s) do not have values in the right DataFrame. Rows in the left DataFrame that are missing values for the join key(s) in the right DataFrame will simply have null (i.e., NaN or None) values for those columns in the resulting joined DataFrame. Note: a left join will still discard rows from the right DataFrame that do not have values for the join key(s) in the left DataFrame. A left join is performed in pandas by calling the same merge function used for inner join, but using the how='left' argument: merged_left = pd.merge(left=survey_sub,right=species_sub, how='left', left_on='species_id', right_on='species_id') merged_left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight genus species taxa 0 1 7 16 1977 2 NL M 32.0 NaN Neotoma albigula Rodent 1 2 7 16 1977 3 NL M 33.0 NaN Neotoma albigula Rodent 2 3 7 16 1977 2 DM F 37.0 NaN Dipodomys merriami Rodent 3 4 7 16 1977 7 DM M 36.0 NaN Dipodomys merriami Rodent 4 5 7 16 1977 3 DM M 35.0 NaN Dipodomys merriami Rodent 5 6 7 16 1977 1 PF M 14.0 NaN NaN NaN NaN 6 7 7 16 1977 2 PE F NaN NaN Peromyscus eremicus Rodent 7 8 7 16 1977 1 DM M 37.0 NaN Dipodomys merriami Rodent 8 9 7 16 1977 1 DM F 34.0 NaN Dipodomys merriami Rodent 9 10 7 16 1977 6 PF F 20.0 NaN NaN NaN NaN The result DataFrame from a left join ( merged_left ) looks very much like the result DataFrame from an inner join ( merged_inner ) in terms of the columns it contains. However, unlike merged_inner , merged_left contains the same number of rows as the original survey_sub DataFrame. When we inspect merged_left , we find there are rows where the information that should have come from species_sub (i.e., species_id, genus, and taxa) is missing (they contain NaN values): merged_left[ pd.isnull(merged_left.genus) ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight genus species taxa 5 6 7 16 1977 1 PF M 14.0 NaN NaN NaN NaN 9 10 7 16 1977 6 PF F 20.0 NaN NaN NaN NaN These rows are the ones where the value of species_id from survey_sub (in this case, PF) does not occur in species_sub .","title":"Left Joins"},{"location":"modules/merging_data/#other-join-types","text":"The pandas merge function supports two other join types: Right (outer) join: Invoked by passing how='right' as an argument. Similar to a left join, except all rows from the right DataFrame are kept, while rows from the left DataFrame without matching join key(s) values are discarded. Full (outer) join: Invoked by passing how='outer' as an argument. This join type returns the all pairwise combinations of rows from both DataFrames; i.e., the result DataFrame will NaN where data is missing in one of the dataframes. This join type is very rarely used.","title":"Other join types"},{"location":"modules/merging_data/#extra-challenge-1","text":"Create a new DataFrame by joining the contents of the surveys.csv and speciesSubset.csv tables. Then calculate and plot the distribution of: taxa by plot taxa by sex by plot","title":"Extra Challenge 1"},{"location":"modules/missing_values/","text":".output_label { text-align: right; margin: -1em; padding: 0; font-size: 0.5em; color: grey } Handling Missing Data Most of the times real-world data is rarely clean and homogeneous. In many cases, dataset of interest will have some amount of data missing. To make matters even more complicated, different data sources may indicate missing data in different ways. In this module, we will discuss some general considerations for missing data, discuss how Pandas chooses to represent it, and demonstrate some built-in Pandas tools for handling missing data in Python. We refer the missing data as null, NaN, or NA values in general. Before we start, lets make sure the Pandas and matplotlib packages are installed . !pip install pandas matplotlib output Requirement already satisfied: pandas in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (0.25.0) Requirement already satisfied: matplotlib in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (3.1.1) Requirement already satisfied: numpy>=1.13.3 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (1.17.0) Requirement already satisfied: python-dateutil>=2.6.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2.8.0) Requirement already satisfied: pytz>=2017.2 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2019.1) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (1.1.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (2.4.1.1) Requirement already satisfied: cycler>=0.10 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: six>=1.5 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0) Requirement already satisfied: setuptools in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0) # Now import pandas into your notebook as pd import pandas as pd Now again import surveys.csv dataset into our notebook as we did in previous lesson. surveys_df = pd.read_csv(\"surveys.csv\") Using masks to identify a specific condition A mask can be useful to locate where a particular subset of values exist or don't exist - for example, NaN, or \"Not a Number\" values. To understand masks, we also need to understand BOOLEAN objects in Python. Boolean values include True or False . For example, # set value of x to be 5 x = 5 x > 5 output False x == 5 output True Finding Missing Values Let's identify all locations in the survey data that have null (missing or NaN) data values. We can use the isnull method to do this. The isnull method will compare each cell with a null value. If an element has a null value, it will be assigned a value of True in the output object. pd.isnull(surveys_df).head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 False False False False False False False False True 1 False False False False False False False False True 2 False False False False False False False False True 3 False False False False False False False False True 4 False False False False False False False False True How to select rows with missing data To select the rows where there are null values, we can use the mask as an index to subset our data as follows: # To select only the rows with NaN values, we can use the 'any()' method surveys_df[pd.isnull(surveys_df).any(axis=1)] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN 5 6 7 16 1977 1 PF M 14.0 NaN 6 7 7 16 1977 2 PE F NaN NaN 7 8 7 16 1977 1 DM M 37.0 NaN 8 9 7 16 1977 1 DM F 34.0 NaN 9 10 7 16 1977 6 PF F 20.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN 11 12 7 16 1977 7 DM M 38.0 NaN 12 13 7 16 1977 3 DM M 35.0 NaN 13 14 7 16 1977 8 DM NaN NaN NaN 14 15 7 16 1977 6 DM F 36.0 NaN 15 16 7 16 1977 4 DM F 36.0 NaN 16 17 7 16 1977 3 DS F 48.0 NaN 17 18 7 16 1977 2 PP M 22.0 NaN 18 19 7 16 1977 4 PF NaN NaN NaN 19 20 7 17 1977 11 DS F 48.0 NaN 20 21 7 17 1977 14 DM F 34.0 NaN 21 22 7 17 1977 15 NL F 31.0 NaN 22 23 7 17 1977 13 DM M 36.0 NaN 23 24 7 17 1977 13 SH M 21.0 NaN 24 25 7 17 1977 9 DM M 35.0 NaN 25 26 7 17 1977 15 DM M 31.0 NaN 26 27 7 17 1977 15 DM M 36.0 NaN 27 28 7 17 1977 11 DM M 38.0 NaN 28 29 7 17 1977 11 PP M NaN NaN 29 30 7 17 1977 10 DS F 52.0 NaN ... ... ... ... ... ... ... ... ... ... 35187 35188 11 10 2002 10 NaN NaN NaN NaN 35256 35257 12 7 2002 22 PB M 26.0 NaN 35259 35260 12 7 2002 21 PB F 24.0 NaN 35277 35278 12 7 2002 20 AH NaN NaN NaN 35279 35280 12 7 2002 16 PB M 28.0 NaN 35322 35323 12 8 2002 11 AH NaN NaN NaN 35328 35329 12 8 2002 11 PP M NaN 16.0 35370 35371 12 8 2002 14 AH NaN NaN NaN 35378 35379 12 8 2002 15 PB F 26.0 NaN 35384 35385 12 8 2002 10 NaN NaN NaN NaN 35387 35388 12 29 2002 1 DO M 35.0 NaN 35403 35404 12 29 2002 2 NL F 30.0 NaN 35448 35449 12 29 2002 20 OT F 20.0 NaN 35452 35453 12 29 2002 20 PB M 28.0 NaN 35457 35458 12 29 2002 20 AH NaN NaN NaN 35477 35478 12 29 2002 24 AH NaN NaN NaN 35485 35486 12 29 2002 16 DO M 37.0 NaN 35495 35496 12 31 2002 4 PB NaN NaN NaN 35510 35511 12 31 2002 11 DX NaN NaN NaN 35511 35512 12 31 2002 11 US NaN NaN NaN 35512 35513 12 31 2002 11 US NaN NaN NaN 35514 35515 12 31 2002 11 SF F 27.0 NaN 35519 35520 12 31 2002 9 SF NaN 24.0 36.0 35527 35528 12 31 2002 13 US NaN NaN NaN 35529 35530 12 31 2002 13 OT F 20.0 NaN 35530 35531 12 31 2002 13 PB F 27.0 NaN 35543 35544 12 31 2002 15 US NaN NaN NaN 35544 35545 12 31 2002 15 AH NaN NaN NaN 35545 35546 12 31 2002 15 AH NaN NaN NaN 35548 35549 12 31 2002 5 NaN NaN NaN NaN 4873 rows \u00d7 9 columns Explaination Notice that we have 4873 observations/rows that contain one or more missing values. Thats roughly 14% of data contains missing values. We have used [] convension to select subset of data. More information about slicing and indexing can be found out here . (axis=1) is a numpy convention to specify columns. Note that the weight column of our DataFrame contains many null or NaN values. Next, we will explore ways of dealing with this. If we look at the weight column in the surveys data we notice that there are NaN ( N ot a N umber) values. NaN values are undefined values that cannot be represented mathematically. Pandas, for example, will read an empty cell in a CSV or Excel sheet as a NaN. NaNs have some desirable properties: if we were to average the weight column without replacing our NaNs, Python would know to skip over those cells. Dealing with missing values. Dealing with missing data values is always a challenge. It's sometimes hard to know why values are missing - was it because of a data entry error? Or data that someone was unable to collect? Should the value be 0? We need to know how missing values are represented in the dataset in order to make good decisions. If we're lucky, we have some metadata that will tell us more about how null values were handled. For instance, in some disciplines, like Remote Sensing, missing data values are often defined as -9999. Having a bunch of -9999 values in your data could really alter numeric calculations. Often in spreadsheets, cells are left empty where no data are available. Pandas will, by default, replace those missing values with NaN. However it is good practice to get in the habit of intentionally marking cells that have no data, with a no data value! That way there are no questions in the future when you (or someone else) explores your data. Where Are the NaN's? Let's explore the NaN values in our data a bit further. Using the tools we learned in lesson 02, we can figure out how many rows contain NaN values for weight. We can also create a new subset from our data that only contains rows with weight values > 0 (i.e., select meaningful weight values): ## How many missing values are there in weight column? len(surveys_df[pd.isnull(surveys_df.weight)]) output 3266 # How many rows have weight values? len(surveys_df[surveys_df.weight> 0]) output 32283 We can replace all NaN values with zeroes using the .fillna() method (after making a copy of the data so we don't lose our work): # Creat a new DataFrame using copy df1 = surveys_df.copy() # Fill all NaN values with 0 df1['weight'] = df1['weight'].fillna(0) However NaN and 0 yield different analysis results. The mean value when NaN values are replaced with 0 is different from when NaN values are simply thrown out or ignored. surveys_df['weight'].mean() output 42.672428212991356 df1['weight'].mean() output 38.751976145601844 Extra Information We can fill NaN values with any value that we chose. The code below fills all NaN values with a mean for all weight values. df1['weight'] = surveys_df['weight'].fillna(surveys_df['weight'].mean()) df1['weight'].mean() output 42.672428212991356 Writing Out Data to CSV We've learned about using manipulating data to get desired outputs. But we've also discussed keeping data that has been manipulated separate from our raw data. Something we might be interested in doing is working with only the columns that have full data. First, let's reload the data so we're not mixing up all of our previous manipulations. df_na = surveys_df.dropna() If you now type df_na , you should observe that the resulting DataFrame has 30676 rows and 9 columns, much smaller than the 35549 row original. We can now use the to_csv command to do export a DataFrame in CSV format. Note that the code below will by default save the data into the current working directory. We can save it to a different folder by adding the foldername and a slash before the filename: df1.to_csv('foldername/out.csv') . We use 'index=False' so that pandas doesn't include the index number for each line. # Write DataFrame to CSV df_na.to_csv('output/surveys_complete.csv', index=False) Recap What we've learned: What NaN values are, how they might be represented, and what this means for your work How to replace NaN values, if desired How to use to_csv to write manipulated data to a file. Extra We can run isnull on a particular column too. What does the code below do? # What does this do? empty_weights = surveys_df[pd.isnull(surveys_df['weight'])]['weight'] print(empty_weights) output 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN .. 35530 NaN 35543 NaN 35544 NaN 35545 NaN 35548 NaN Name: weight, Length: 3266, dtype: float64 Let's take a minute to look at the statement above. We are using the Boolean object pd.isnull(surveys_df['weight']) as an index to surveys_df . We are asking Python to select rows that have a NaN value of weight.","title":"Missing values"},{"location":"modules/missing_values/#handling-missing-data","text":"Most of the times real-world data is rarely clean and homogeneous. In many cases, dataset of interest will have some amount of data missing. To make matters even more complicated, different data sources may indicate missing data in different ways. In this module, we will discuss some general considerations for missing data, discuss how Pandas chooses to represent it, and demonstrate some built-in Pandas tools for handling missing data in Python. We refer the missing data as null, NaN, or NA values in general. Before we start, lets make sure the Pandas and matplotlib packages are installed . !pip install pandas matplotlib output Requirement already satisfied: pandas in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (0.25.0) Requirement already satisfied: matplotlib in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (3.1.1) Requirement already satisfied: numpy>=1.13.3 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (1.17.0) Requirement already satisfied: python-dateutil>=2.6.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2.8.0) Requirement already satisfied: pytz>=2017.2 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2019.1) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (1.1.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (2.4.1.1) Requirement already satisfied: cycler>=0.10 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: six>=1.5 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0) Requirement already satisfied: setuptools in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0) # Now import pandas into your notebook as pd import pandas as pd Now again import surveys.csv dataset into our notebook as we did in previous lesson. surveys_df = pd.read_csv(\"surveys.csv\")","title":"Handling Missing Data"},{"location":"modules/missing_values/#using-masks-to-identify-a-specific-condition","text":"A mask can be useful to locate where a particular subset of values exist or don't exist - for example, NaN, or \"Not a Number\" values. To understand masks, we also need to understand BOOLEAN objects in Python. Boolean values include True or False . For example, # set value of x to be 5 x = 5 x > 5 output False x == 5 output True","title":"Using masks to identify a specific condition"},{"location":"modules/missing_values/#finding-missing-values","text":"Let's identify all locations in the survey data that have null (missing or NaN) data values. We can use the isnull method to do this. The isnull method will compare each cell with a null value. If an element has a null value, it will be assigned a value of True in the output object. pd.isnull(surveys_df).head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 False False False False False False False False True 1 False False False False False False False False True 2 False False False False False False False False True 3 False False False False False False False False True 4 False False False False False False False False True","title":"Finding Missing Values"},{"location":"modules/missing_values/#how-to-select-rows-with-missing-data","text":"To select the rows where there are null values, we can use the mask as an index to subset our data as follows: # To select only the rows with NaN values, we can use the 'any()' method surveys_df[pd.isnull(surveys_df).any(axis=1)] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN 5 6 7 16 1977 1 PF M 14.0 NaN 6 7 7 16 1977 2 PE F NaN NaN 7 8 7 16 1977 1 DM M 37.0 NaN 8 9 7 16 1977 1 DM F 34.0 NaN 9 10 7 16 1977 6 PF F 20.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN 11 12 7 16 1977 7 DM M 38.0 NaN 12 13 7 16 1977 3 DM M 35.0 NaN 13 14 7 16 1977 8 DM NaN NaN NaN 14 15 7 16 1977 6 DM F 36.0 NaN 15 16 7 16 1977 4 DM F 36.0 NaN 16 17 7 16 1977 3 DS F 48.0 NaN 17 18 7 16 1977 2 PP M 22.0 NaN 18 19 7 16 1977 4 PF NaN NaN NaN 19 20 7 17 1977 11 DS F 48.0 NaN 20 21 7 17 1977 14 DM F 34.0 NaN 21 22 7 17 1977 15 NL F 31.0 NaN 22 23 7 17 1977 13 DM M 36.0 NaN 23 24 7 17 1977 13 SH M 21.0 NaN 24 25 7 17 1977 9 DM M 35.0 NaN 25 26 7 17 1977 15 DM M 31.0 NaN 26 27 7 17 1977 15 DM M 36.0 NaN 27 28 7 17 1977 11 DM M 38.0 NaN 28 29 7 17 1977 11 PP M NaN NaN 29 30 7 17 1977 10 DS F 52.0 NaN ... ... ... ... ... ... ... ... ... ... 35187 35188 11 10 2002 10 NaN NaN NaN NaN 35256 35257 12 7 2002 22 PB M 26.0 NaN 35259 35260 12 7 2002 21 PB F 24.0 NaN 35277 35278 12 7 2002 20 AH NaN NaN NaN 35279 35280 12 7 2002 16 PB M 28.0 NaN 35322 35323 12 8 2002 11 AH NaN NaN NaN 35328 35329 12 8 2002 11 PP M NaN 16.0 35370 35371 12 8 2002 14 AH NaN NaN NaN 35378 35379 12 8 2002 15 PB F 26.0 NaN 35384 35385 12 8 2002 10 NaN NaN NaN NaN 35387 35388 12 29 2002 1 DO M 35.0 NaN 35403 35404 12 29 2002 2 NL F 30.0 NaN 35448 35449 12 29 2002 20 OT F 20.0 NaN 35452 35453 12 29 2002 20 PB M 28.0 NaN 35457 35458 12 29 2002 20 AH NaN NaN NaN 35477 35478 12 29 2002 24 AH NaN NaN NaN 35485 35486 12 29 2002 16 DO M 37.0 NaN 35495 35496 12 31 2002 4 PB NaN NaN NaN 35510 35511 12 31 2002 11 DX NaN NaN NaN 35511 35512 12 31 2002 11 US NaN NaN NaN 35512 35513 12 31 2002 11 US NaN NaN NaN 35514 35515 12 31 2002 11 SF F 27.0 NaN 35519 35520 12 31 2002 9 SF NaN 24.0 36.0 35527 35528 12 31 2002 13 US NaN NaN NaN 35529 35530 12 31 2002 13 OT F 20.0 NaN 35530 35531 12 31 2002 13 PB F 27.0 NaN 35543 35544 12 31 2002 15 US NaN NaN NaN 35544 35545 12 31 2002 15 AH NaN NaN NaN 35545 35546 12 31 2002 15 AH NaN NaN NaN 35548 35549 12 31 2002 5 NaN NaN NaN NaN 4873 rows \u00d7 9 columns","title":"How to select rows with missing data"},{"location":"modules/missing_values/#explaination","text":"Notice that we have 4873 observations/rows that contain one or more missing values. Thats roughly 14% of data contains missing values. We have used [] convension to select subset of data. More information about slicing and indexing can be found out here . (axis=1) is a numpy convention to specify columns. Note that the weight column of our DataFrame contains many null or NaN values. Next, we will explore ways of dealing with this. If we look at the weight column in the surveys data we notice that there are NaN ( N ot a N umber) values. NaN values are undefined values that cannot be represented mathematically. Pandas, for example, will read an empty cell in a CSV or Excel sheet as a NaN. NaNs have some desirable properties: if we were to average the weight column without replacing our NaNs, Python would know to skip over those cells.","title":"Explaination"},{"location":"modules/missing_values/#dealing-with-missing-values","text":"Dealing with missing data values is always a challenge. It's sometimes hard to know why values are missing - was it because of a data entry error? Or data that someone was unable to collect? Should the value be 0? We need to know how missing values are represented in the dataset in order to make good decisions. If we're lucky, we have some metadata that will tell us more about how null values were handled. For instance, in some disciplines, like Remote Sensing, missing data values are often defined as -9999. Having a bunch of -9999 values in your data could really alter numeric calculations. Often in spreadsheets, cells are left empty where no data are available. Pandas will, by default, replace those missing values with NaN. However it is good practice to get in the habit of intentionally marking cells that have no data, with a no data value! That way there are no questions in the future when you (or someone else) explores your data.","title":"Dealing with missing values."},{"location":"modules/missing_values/#where-are-the-nans","text":"Let's explore the NaN values in our data a bit further. Using the tools we learned in lesson 02, we can figure out how many rows contain NaN values for weight. We can also create a new subset from our data that only contains rows with weight values > 0 (i.e., select meaningful weight values): ## How many missing values are there in weight column? len(surveys_df[pd.isnull(surveys_df.weight)]) output 3266 # How many rows have weight values? len(surveys_df[surveys_df.weight> 0]) output 32283 We can replace all NaN values with zeroes using the .fillna() method (after making a copy of the data so we don't lose our work): # Creat a new DataFrame using copy df1 = surveys_df.copy() # Fill all NaN values with 0 df1['weight'] = df1['weight'].fillna(0) However NaN and 0 yield different analysis results. The mean value when NaN values are replaced with 0 is different from when NaN values are simply thrown out or ignored. surveys_df['weight'].mean() output 42.672428212991356 df1['weight'].mean() output 38.751976145601844","title":"Where Are the NaN's?"},{"location":"modules/missing_values/#extra-information","text":"We can fill NaN values with any value that we chose. The code below fills all NaN values with a mean for all weight values. df1['weight'] = surveys_df['weight'].fillna(surveys_df['weight'].mean()) df1['weight'].mean() output 42.672428212991356","title":"Extra Information"},{"location":"modules/missing_values/#writing-out-data-to-csv","text":"We've learned about using manipulating data to get desired outputs. But we've also discussed keeping data that has been manipulated separate from our raw data. Something we might be interested in doing is working with only the columns that have full data. First, let's reload the data so we're not mixing up all of our previous manipulations. df_na = surveys_df.dropna() If you now type df_na , you should observe that the resulting DataFrame has 30676 rows and 9 columns, much smaller than the 35549 row original. We can now use the to_csv command to do export a DataFrame in CSV format. Note that the code below will by default save the data into the current working directory. We can save it to a different folder by adding the foldername and a slash before the filename: df1.to_csv('foldername/out.csv') . We use 'index=False' so that pandas doesn't include the index number for each line. # Write DataFrame to CSV df_na.to_csv('output/surveys_complete.csv', index=False)","title":"Writing Out Data to CSV"},{"location":"modules/missing_values/#recap","text":"What we've learned: What NaN values are, how they might be represented, and what this means for your work How to replace NaN values, if desired How to use to_csv to write manipulated data to a file.","title":"Recap"},{"location":"modules/missing_values/#extra","text":"We can run isnull on a particular column too. What does the code below do? # What does this do? empty_weights = surveys_df[pd.isnull(surveys_df['weight'])]['weight'] print(empty_weights) output 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN .. 35530 NaN 35543 NaN 35544 NaN 35545 NaN 35548 NaN Name: weight, Length: 3266, dtype: float64 Let's take a minute to look at the statement above. We are using the Boolean object pd.isnull(surveys_df['weight']) as an index to surveys_df . We are asking Python to select rows that have a NaN value of weight.","title":"Extra"},{"location":"modules/plotting_with_ggplot/","text":".output_label { text-align: right; margin: -1em; padding: 0; font-size: 0.5em; color: grey } Making Plots With plotnine (aka ggplot) Introduction Python has a number of powerful plotting libraries to choose from. One of the oldest and most popular is matplotlib - it forms the foundation for many other Python plotting libraries. For this exercise we are going to use plotnine which is a Python implementation of the The Grammar of Graphics , inspired by the interface of the ggplot2 package from R. plotnine (and it's R cousin ggplot2 ) is a very nice way to create publication quality plots. The Grammar of Graphics Statistical graphics is a mapping from data to aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars) Faceting can be used to generate the same plot for different subsets of the dataset These are basic building blocks according to the grammar of graphics: data The data + a set of aesthetic mappings that describing variables mapping geom Geometric objects, represent what you actually see on the plot: points, lines, polygons, etc. stats Statistical transformations, summarise data in many useful ways. scale The scales map values in the data space to values in an aesthetic space coord A coordinate system, describes how data coordinates are mapped to the plane of the graphic. facet A faceting specification describes how to break up the data into subsets for plotting individual set Let's explore these in detail. First, install the pandas and plotnine packages to ensure they are available. !pip install pandas plotnine output Requirement already satisfied: pandas in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (0.25.0) Requirement already satisfied: plotnine in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (0.5.1) Requirement already satisfied: python-dateutil>=2.6.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2.8.0) Requirement already satisfied: numpy>=1.13.3 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (1.17.0) Requirement already satisfied: pytz>=2017.2 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2019.1) Requirement already satisfied: patsy>=0.4.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (0.5.1) Requirement already satisfied: mizani>=0.5.2 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (0.5.4) Requirement already satisfied: matplotlib>=3.0.0 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (3.1.1) Requirement already satisfied: scipy>=1.0.0 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (1.3.0) Requirement already satisfied: statsmodels>=0.8.0 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (0.10.1) Requirement already satisfied: descartes>=1.1.0 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (1.1.0) Requirement already satisfied: six>=1.5 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0) Requirement already satisfied: palettable in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from mizani>=0.5.2->plotnine) (3.2.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib>=3.0.0->plotnine) (2.4.1.1) Requirement already satisfied: cycler>=0.10 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib>=3.0.0->plotnine) (0.10.0) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib>=3.0.0->plotnine) (1.1.0) Requirement already satisfied: setuptools in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->plotnine) (39.1.0) # We run this to suppress various deprecation warnings from plotnine - keeps our notebook cleaner import warnings warnings.filterwarnings('ignore') Plotting in ggplot style Let's set up our working environment with necessary libraries and also load our csv file into data frame called survs_df , import numpy as np import pandas as pd from plotnine import * %matplotlib inline survs_df = pd.read_csv('surveys.csv').dropna() To produce a plot with the ggplot class from plotnine , we must provide three things: A data frame containing our data. How the columns of the data frame can be translated into positions, colors, sizes, and shapes of graphical elements (\"aesthetics\"). The actual graphical elements to display (\"geometric objects\"). Introduction to plotting ggplot(survs_df, aes(x='weight', y='hindfoot_length')) + geom_point() output Let's see if we can also include information about species and year. ggplot(survs_df, aes(x='weight', y='hindfoot_length', size = 'year')) + geom_point() output Notice that we've dropped the x= and y= ? These are implied for the first and second argument of aes() . ggplot(survs_df, aes(x='weight', y='hindfoot_length', size = 'year', color = 'species_id')) + geom_point() output We can do simple counting plot, to see how many observation (data points) we have for each year for example ggplot(survs_df, aes(x='year')) + \\ geom_bar(stat = 'count') output Let's now also color by species to see how many observation we have per species in a given year ggplot(survs_df, aes(x='year', fill = 'species_id')) + \\ geom_bar(stat = 'count') output Challenges Produce a plot comparing the number of observations for each species at each site. The plot should have site_id on the x axis, ideally as categorical data. (HINT: You can convert a column in a DataFrame df to the 'category' type using: df['some_col_name'] = df['some_col_name'].astype('category') ) Create a boxplot of hindfoot_length across different species ( species_id column) (HINT: There's a list of geoms available for plotnine in the docs - instead of geom_bar , which one should you use ?) More geom types ggplot(survs_df, aes(x='year', y='weight')) + \\ geom_boxplot() output Why are we not seeing mulitple boxplots, one for each year? This is because year variable is continuous in our data frame, but for this purpose we want it to be categorical. survs_df['year_fact'] = survs_df['year'].astype(\"category\") ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() output You'll notice the x-axis labels are overlapped. To flip them 90-degrees we can apply a theme so they look less cluttered. We will revisit themes later. ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() + \\ theme(axis_text_x = element_text(angle=90, hjust=1)) output To save some typing, let's define this x-axis label rotating theme as a short variable name that we can reuse: flip_xlabels = theme(axis_text_x = element_text(angle=90, hjust=1)) ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_violin() + \\ flip_xlabels output To save an image for later: plt1 = ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() + \\ xlab(\"Years\") + \\ ylab(\"Weight log2(kg)\") + \\ ggtitle(\"Boxplots, summary of species weight in each year\") ggsave(filename=\"plot1.png\", plot=plt1, device='png', dpi=300, height=25, width=25) Challenges Can you log2 transform weight and plot a \"normalised\" boxplot ? Hint: use np.log2() function and name new column weight_log . Does a log2 transform make this data visualisation better ? Faceting ggplot has a special technique called faceting that allows to split one plot into multiple plots based on a factor included in the dataset. We will use it to make one plot for a time series for each species. ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() + \\ facet_wrap(['sex']) + \\ flip_xlabels + \\ theme(axis_text_x = element_text(size=6)) output ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() + \\ theme(axis_text_x = element_text(size=4)) + \\ facet_wrap(['species_id']) + \\ flip_xlabels output The two faceted plots above are probably easier to interpret using the weight_log column we created - give it a try ! The \"Layered Grammar of Graphics\" ggplot(data = <DATA>) + <GEOM_FUNCTION>( mapping = aes(<MAPPINGS>), stat = <STAT>, position = <POSITION> ) + <COORDINATE_FUNCTION> + <FACET_FUNCTION> Theming plotnine allows pre-defined 'themes' to be applied as aesthetics to the plot. A list available theme you may want to experiment with is here: https://plotnine.readthedocs.io/en/stable/api.html#themes ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() + \\ theme_bw() + \\ flip_xlabels output ggplot(survs_df, aes(x='year_fact', y='weight_log')) + \\ geom_boxplot() + \\ facet_wrap(['species_id']) + \\ theme_xkcd() + \\ theme(axis_text_x = element_text(size=4, angle=90, hjust=1)) output Extra bits 1 Let's try to bin years into decades, which could be crude but might gives simple images to look at. bins = [(survs_df['year'] < 1980), (survs_df['year'] < 1990), (survs_df['year'] < 2000), (survs_df['year'] >= 2000)] labels = ['70s', '80s', '90s', 'Z'] survs_df['year_bins'] = np.select(bins, labels) plt2 = ggplot(survs_df, aes(x='year_bins', y='weight_log')) + \\ geom_boxplot() plt2 output plt2 = ggplot(survs_df, aes(x='year_bins', y='weight_log')) + \\ geom_boxplot() + \\ flip_xlabels + \\ facet_wrap(['species_id']) plt2 output Extra bits 2 This is a different way to look at your data ggplot(survs_df, aes(\"year_fact\", \"weight\")) + \\ stat_summary(fun_y = np.mean, fun_ymin=np.min, fun_ymax=np.max) + \\ theme(axis_text_x = element_text(angle=90, hjust=1)) ggplot(survs_df, aes(\"year_fact\", \"weight\")) + \\ stat_summary(fun_y = np.median, fun_ymin=np.min, fun_ymax=np.max) + \\ theme(axis_text_x = element_text(angle=90, hjust=1)) ggplot(survs_df, aes(\"year_fact\", \"weight_log\")) + \\ stat_summary(fun_y = np.mean, fun_ymin=np.min, fun_ymax=np.max) + \\ theme(axis_text_x = element_text(angle=90, hjust=1)) output","title":"Plotting with ggplot for Python"},{"location":"modules/plotting_with_ggplot/#making-plots-with-plotnine-aka-ggplot","text":"","title":"Making Plots With plotnine (aka ggplot)"},{"location":"modules/plotting_with_ggplot/#introduction","text":"Python has a number of powerful plotting libraries to choose from. One of the oldest and most popular is matplotlib - it forms the foundation for many other Python plotting libraries. For this exercise we are going to use plotnine which is a Python implementation of the The Grammar of Graphics , inspired by the interface of the ggplot2 package from R. plotnine (and it's R cousin ggplot2 ) is a very nice way to create publication quality plots.","title":"Introduction"},{"location":"modules/plotting_with_ggplot/#the-grammar-of-graphics","text":"Statistical graphics is a mapping from data to aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars) Faceting can be used to generate the same plot for different subsets of the dataset These are basic building blocks according to the grammar of graphics: data The data + a set of aesthetic mappings that describing variables mapping geom Geometric objects, represent what you actually see on the plot: points, lines, polygons, etc. stats Statistical transformations, summarise data in many useful ways. scale The scales map values in the data space to values in an aesthetic space coord A coordinate system, describes how data coordinates are mapped to the plane of the graphic. facet A faceting specification describes how to break up the data into subsets for plotting individual set Let's explore these in detail. First, install the pandas and plotnine packages to ensure they are available. !pip install pandas plotnine output Requirement already satisfied: pandas in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (0.25.0) Requirement already satisfied: plotnine in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (0.5.1) Requirement already satisfied: python-dateutil>=2.6.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2.8.0) Requirement already satisfied: numpy>=1.13.3 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (1.17.0) Requirement already satisfied: pytz>=2017.2 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2019.1) Requirement already satisfied: patsy>=0.4.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (0.5.1) Requirement already satisfied: mizani>=0.5.2 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (0.5.4) Requirement already satisfied: matplotlib>=3.0.0 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (3.1.1) Requirement already satisfied: scipy>=1.0.0 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (1.3.0) Requirement already satisfied: statsmodels>=0.8.0 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (0.10.1) Requirement already satisfied: descartes>=1.1.0 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from plotnine) (1.1.0) Requirement already satisfied: six>=1.5 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0) Requirement already satisfied: palettable in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from mizani>=0.5.2->plotnine) (3.2.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib>=3.0.0->plotnine) (2.4.1.1) Requirement already satisfied: cycler>=0.10 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib>=3.0.0->plotnine) (0.10.0) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib>=3.0.0->plotnine) (1.1.0) Requirement already satisfied: setuptools in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->plotnine) (39.1.0) # We run this to suppress various deprecation warnings from plotnine - keeps our notebook cleaner import warnings warnings.filterwarnings('ignore')","title":"The Grammar of Graphics"},{"location":"modules/plotting_with_ggplot/#plotting-in-ggplot-style","text":"Let's set up our working environment with necessary libraries and also load our csv file into data frame called survs_df , import numpy as np import pandas as pd from plotnine import * %matplotlib inline survs_df = pd.read_csv('surveys.csv').dropna() To produce a plot with the ggplot class from plotnine , we must provide three things: A data frame containing our data. How the columns of the data frame can be translated into positions, colors, sizes, and shapes of graphical elements (\"aesthetics\"). The actual graphical elements to display (\"geometric objects\").","title":"Plotting in ggplot style"},{"location":"modules/plotting_with_ggplot/#introduction-to-plotting","text":"ggplot(survs_df, aes(x='weight', y='hindfoot_length')) + geom_point() output Let's see if we can also include information about species and year. ggplot(survs_df, aes(x='weight', y='hindfoot_length', size = 'year')) + geom_point() output Notice that we've dropped the x= and y= ? These are implied for the first and second argument of aes() . ggplot(survs_df, aes(x='weight', y='hindfoot_length', size = 'year', color = 'species_id')) + geom_point() output We can do simple counting plot, to see how many observation (data points) we have for each year for example ggplot(survs_df, aes(x='year')) + \\ geom_bar(stat = 'count') output Let's now also color by species to see how many observation we have per species in a given year ggplot(survs_df, aes(x='year', fill = 'species_id')) + \\ geom_bar(stat = 'count') output","title":"Introduction to plotting"},{"location":"modules/plotting_with_ggplot/#challenges","text":"Produce a plot comparing the number of observations for each species at each site. The plot should have site_id on the x axis, ideally as categorical data. (HINT: You can convert a column in a DataFrame df to the 'category' type using: df['some_col_name'] = df['some_col_name'].astype('category') ) Create a boxplot of hindfoot_length across different species ( species_id column) (HINT: There's a list of geoms available for plotnine in the docs - instead of geom_bar , which one should you use ?)","title":"Challenges"},{"location":"modules/plotting_with_ggplot/#more-geom-types","text":"ggplot(survs_df, aes(x='year', y='weight')) + \\ geom_boxplot() output Why are we not seeing mulitple boxplots, one for each year? This is because year variable is continuous in our data frame, but for this purpose we want it to be categorical. survs_df['year_fact'] = survs_df['year'].astype(\"category\") ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() output You'll notice the x-axis labels are overlapped. To flip them 90-degrees we can apply a theme so they look less cluttered. We will revisit themes later. ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() + \\ theme(axis_text_x = element_text(angle=90, hjust=1)) output To save some typing, let's define this x-axis label rotating theme as a short variable name that we can reuse: flip_xlabels = theme(axis_text_x = element_text(angle=90, hjust=1)) ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_violin() + \\ flip_xlabels output To save an image for later: plt1 = ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() + \\ xlab(\"Years\") + \\ ylab(\"Weight log2(kg)\") + \\ ggtitle(\"Boxplots, summary of species weight in each year\") ggsave(filename=\"plot1.png\", plot=plt1, device='png', dpi=300, height=25, width=25)","title":"More geom types"},{"location":"modules/plotting_with_ggplot/#challenges_1","text":"Can you log2 transform weight and plot a \"normalised\" boxplot ? Hint: use np.log2() function and name new column weight_log . Does a log2 transform make this data visualisation better ?","title":"Challenges"},{"location":"modules/plotting_with_ggplot/#faceting","text":"ggplot has a special technique called faceting that allows to split one plot into multiple plots based on a factor included in the dataset. We will use it to make one plot for a time series for each species. ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() + \\ facet_wrap(['sex']) + \\ flip_xlabels + \\ theme(axis_text_x = element_text(size=6)) output ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() + \\ theme(axis_text_x = element_text(size=4)) + \\ facet_wrap(['species_id']) + \\ flip_xlabels output The two faceted plots above are probably easier to interpret using the weight_log column we created - give it a try !","title":"Faceting"},{"location":"modules/plotting_with_ggplot/#the-layered-grammar-of-graphics","text":"ggplot(data = <DATA>) + <GEOM_FUNCTION>( mapping = aes(<MAPPINGS>), stat = <STAT>, position = <POSITION> ) + <COORDINATE_FUNCTION> + <FACET_FUNCTION>","title":"The \"Layered Grammar of Graphics\""},{"location":"modules/plotting_with_ggplot/#theming","text":"plotnine allows pre-defined 'themes' to be applied as aesthetics to the plot. A list available theme you may want to experiment with is here: https://plotnine.readthedocs.io/en/stable/api.html#themes ggplot(survs_df, aes(x='year_fact', y='weight')) + \\ geom_boxplot() + \\ theme_bw() + \\ flip_xlabels output ggplot(survs_df, aes(x='year_fact', y='weight_log')) + \\ geom_boxplot() + \\ facet_wrap(['species_id']) + \\ theme_xkcd() + \\ theme(axis_text_x = element_text(size=4, angle=90, hjust=1)) output","title":"Theming"},{"location":"modules/plotting_with_ggplot/#extra-bits-1","text":"Let's try to bin years into decades, which could be crude but might gives simple images to look at. bins = [(survs_df['year'] < 1980), (survs_df['year'] < 1990), (survs_df['year'] < 2000), (survs_df['year'] >= 2000)] labels = ['70s', '80s', '90s', 'Z'] survs_df['year_bins'] = np.select(bins, labels) plt2 = ggplot(survs_df, aes(x='year_bins', y='weight_log')) + \\ geom_boxplot() plt2 output plt2 = ggplot(survs_df, aes(x='year_bins', y='weight_log')) + \\ geom_boxplot() + \\ flip_xlabels + \\ facet_wrap(['species_id']) plt2 output","title":"Extra bits 1"},{"location":"modules/plotting_with_ggplot/#extra-bits-2","text":"This is a different way to look at your data ggplot(survs_df, aes(\"year_fact\", \"weight\")) + \\ stat_summary(fun_y = np.mean, fun_ymin=np.min, fun_ymax=np.max) + \\ theme(axis_text_x = element_text(angle=90, hjust=1)) ggplot(survs_df, aes(\"year_fact\", \"weight\")) + \\ stat_summary(fun_y = np.median, fun_ymin=np.min, fun_ymax=np.max) + \\ theme(axis_text_x = element_text(angle=90, hjust=1)) ggplot(survs_df, aes(\"year_fact\", \"weight_log\")) + \\ stat_summary(fun_y = np.mean, fun_ymin=np.min, fun_ymax=np.max) + \\ theme(axis_text_x = element_text(angle=90, hjust=1)) output","title":"Extra bits 2"},{"location":"modules/working_with_data/","text":".output_label { text-align: right; margin: -1em; padding: 0; font-size: 0.5em; color: grey } Data Analysis with Python Automating data analysis tasks in Python We can automate the process of performing data manipulations in Python. It's efficient to spend time building the code to perform these tasks because once it's built, we can use it over and over on different datasets that use a similar format. This makes our methods easily reproducible. We can also easily share our code with colleagues and they can replicate the same analysis. The Dataset For this lesson, we will be using the Portal Teaching data, a subset of the data from Ernst et al Long-term monitoring and experimental manipulation of a Chihuahuan Desert ecosystem near Portal, Arizona, USA We will be using this dataset, which can be downloaded here: surveys.csv ... but don't click to download it in your browser - we are going to use Python ! import urllib.request # You can also get this URL value by right-clicking the `surveys.csv` link above and selecting \"Copy Link Address\" url = 'https://monashdatafluency.github.io/python-workshop-base/modules/data/surveys.csv' # url = 'https://goo.gl/9ZxqBg' # or a shortened version to save typing urllib.request.urlretrieve(url, 'surveys.csv') output ('surveys.csv', ) If Jupyter is running locally on your computer, you'll now have a file surveys.csv in the current working directory. You can check by clicking on File tab on the top left of the notebook to see if the file exists. If you are running Jupyter on a remote server or cloud service (eg Colaboratory or Azure Notebooks), the file will be there instead. We are studying the species and weight of animals caught in plots in our study area. The dataset is stored as a .csv file: each row holds information for a single animal, and the columns represent: Column Description record_id Unique id for the observation month month of observation day day of observation year year of observation site_id ID of a particular plot species_id 2-letter code sex sex of animal (\"M\", \"F\") hindfoot_length length of the hindfoot in mm weight weight of the animal in grams The first few rows of our file look like this: record_id,month,day,year,site_id,species_id,sex,hindfoot_length,weight 1,7,16,1977,2,NL,M,32, 2,7,16,1977,3,NL,M,33, 3,7,16,1977,2,DM,F,37, 4,7,16,1977,7,DM,M,36, 5,7,16,1977,3,DM,M,35, 6,7,16,1977,1,PF,M,14, 7,7,16,1977,2,PE,F,, 8,7,16,1977,1,DM,M,37, 9,7,16,1977,1,DM,F,34, About Libraries A library in Python contains a set of tools (called functions) that perform tasks on our data. Importing a library is like getting a piece of lab equipment out of a storage locker and setting it up on the bench for use in a project. Once a library is set up, it can be used or called to perform many tasks. If you have noticed in the previous code import urllib.request , we are calling a request function from library urllib to download our dataset from web. Pandas in Python The dataset we have, is in table format. One of the best options for working with tabular data in Python is to use the Python Data Analysis Library (a.k.a. Pandas). The Pandas library provides data structures, produces high quality plots with matplotlib and integrates nicely with other libraries that use NumPy (which is another Python library) arrays. First, lets make sure the Pandas and matplotlib packages are installed . !pip install pandas matplotlib output Requirement already satisfied: pandas in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (0.25.0) Requirement already satisfied: matplotlib in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (3.1.1) Requirement already satisfied: python-dateutil>=2.6.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2.8.0) Requirement already satisfied: pytz>=2017.2 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2019.1) Requirement already satisfied: numpy>=1.13.3 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (1.17.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (2.4.1.1) Requirement already satisfied: cycler>=0.10 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (1.1.0) Requirement already satisfied: six>=1.5 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0) Requirement already satisfied: setuptools in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0) Python doesn't load all of the libraries available to it by default. We have to add an import statement to our code in order to use library functions. To import a library, we use the syntax import libraryName . If we want to give the library a nickname to shorten the command, we can add as nickNameHere . An example of importing the pandas library using the common nickname pd is below. import pandas as pd Each time we call a function that's in a library, we use the syntax LibraryName.FunctionName . Adding the library name with a . before the function name tells Python where to find the function. In the example above, we have imported Pandas as pd . This means we don't have to type out pandas each time we call a Pandas function. Reading CSV Data Using Pandas We will begin by locating and reading our survey data which are in CSV format. CSV stands for Comma-Separated Values and is a common way store formatted data. Other symbols my also be used, so you might see tab-separated, colon-separated or space separated files. It is quite easy to replace one separator with another, to match your application. The first line in the file often has headers to explain what is in each column. CSV (and other separators) make it easy to share data, and can be imported and exported from many applications, including Microsoft Excel. We can use Pandas' read_csv function to pull the file directly into a DataFrame . So What's a DataFrame? A DataFrame is a 2-dimensional data structure that can store data of different types (including characters, integers, floating point values, factors and more) in columns. It is similar to a spreadsheet or an SQL table or the data.frame in R. A DataFrame always has an index (0-based). An index refers to the position of an element in the data structure. # Note that pd.read_csv is used because we imported pandas as pd pd.read_csv(\"surveys.csv\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN 5 6 7 16 1977 1 PF M 14.0 NaN 6 7 7 16 1977 2 PE F NaN NaN 7 8 7 16 1977 1 DM M 37.0 NaN 8 9 7 16 1977 1 DM F 34.0 NaN 9 10 7 16 1977 6 PF F 20.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN 11 12 7 16 1977 7 DM M 38.0 NaN 12 13 7 16 1977 3 DM M 35.0 NaN 13 14 7 16 1977 8 DM NaN NaN NaN 14 15 7 16 1977 6 DM F 36.0 NaN 15 16 7 16 1977 4 DM F 36.0 NaN 16 17 7 16 1977 3 DS F 48.0 NaN 17 18 7 16 1977 2 PP M 22.0 NaN 18 19 7 16 1977 4 PF NaN NaN NaN 19 20 7 17 1977 11 DS F 48.0 NaN 20 21 7 17 1977 14 DM F 34.0 NaN 21 22 7 17 1977 15 NL F 31.0 NaN 22 23 7 17 1977 13 DM M 36.0 NaN 23 24 7 17 1977 13 SH M 21.0 NaN 24 25 7 17 1977 9 DM M 35.0 NaN 25 26 7 17 1977 15 DM M 31.0 NaN 26 27 7 17 1977 15 DM M 36.0 NaN 27 28 7 17 1977 11 DM M 38.0 NaN 28 29 7 17 1977 11 PP M NaN NaN 29 30 7 17 1977 10 DS F 52.0 NaN ... ... ... ... ... ... ... ... ... ... 35519 35520 12 31 2002 9 SF NaN 24.0 36.0 35520 35521 12 31 2002 9 DM M 37.0 48.0 35521 35522 12 31 2002 9 DM F 35.0 45.0 35522 35523 12 31 2002 9 DM F 36.0 44.0 35523 35524 12 31 2002 9 PB F 25.0 27.0 35524 35525 12 31 2002 9 OL M 21.0 26.0 35525 35526 12 31 2002 8 OT F 20.0 24.0 35526 35527 12 31 2002 13 DO F 33.0 43.0 35527 35528 12 31 2002 13 US NaN NaN NaN 35528 35529 12 31 2002 13 PB F 25.0 25.0 35529 35530 12 31 2002 13 OT F 20.0 NaN 35530 35531 12 31 2002 13 PB F 27.0 NaN 35531 35532 12 31 2002 14 DM F 34.0 43.0 35532 35533 12 31 2002 14 DM F 36.0 48.0 35533 35534 12 31 2002 14 DM M 37.0 56.0 35534 35535 12 31 2002 14 DM M 37.0 53.0 35535 35536 12 31 2002 14 DM F 35.0 42.0 35536 35537 12 31 2002 14 DM F 36.0 46.0 35537 35538 12 31 2002 15 PB F 26.0 31.0 35538 35539 12 31 2002 15 SF M 26.0 68.0 35539 35540 12 31 2002 15 PB F 26.0 23.0 35540 35541 12 31 2002 15 PB F 24.0 31.0 35541 35542 12 31 2002 15 PB F 26.0 29.0 35542 35543 12 31 2002 15 PB F 27.0 34.0 35543 35544 12 31 2002 15 US NaN NaN NaN 35544 35545 12 31 2002 15 AH NaN NaN NaN 35545 35546 12 31 2002 15 AH NaN NaN NaN 35546 35547 12 31 2002 10 RM F 15.0 14.0 35547 35548 12 31 2002 7 DO M 36.0 51.0 35548 35549 12 31 2002 5 NaN NaN NaN NaN 35549 rows \u00d7 9 columns The above command outputs a DateFrame object, which Jupyter displays as a table (snipped in the middle since there are many rows). We can see that there were 33,549 rows parsed. Each row has 9 columns. The first column is the index of the DataFrame. The index is used to identify the position of the data, but it is not an actual column of the DataFrame. It looks like the read_csv function in Pandas read our file properly. However, we haven't saved any data to memory so we can work with it.We need to assign the DataFrame to a variable. Remember that a variable is a name for a value, such as x , or data . We can create a new object with a variable name by assigning a value to it using = . Let's call the imported survey data surveys_df : surveys_df = pd.read_csv(\"surveys.csv\") Notice when you assign the imported DataFrame to a variable, Python does not produce any output on the screen. We can view the value of the surveys_df object by typing its name into the cell. surveys_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN 5 6 7 16 1977 1 PF M 14.0 NaN 6 7 7 16 1977 2 PE F NaN NaN 7 8 7 16 1977 1 DM M 37.0 NaN 8 9 7 16 1977 1 DM F 34.0 NaN 9 10 7 16 1977 6 PF F 20.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN 11 12 7 16 1977 7 DM M 38.0 NaN 12 13 7 16 1977 3 DM M 35.0 NaN 13 14 7 16 1977 8 DM NaN NaN NaN 14 15 7 16 1977 6 DM F 36.0 NaN 15 16 7 16 1977 4 DM F 36.0 NaN 16 17 7 16 1977 3 DS F 48.0 NaN 17 18 7 16 1977 2 PP M 22.0 NaN 18 19 7 16 1977 4 PF NaN NaN NaN 19 20 7 17 1977 11 DS F 48.0 NaN 20 21 7 17 1977 14 DM F 34.0 NaN 21 22 7 17 1977 15 NL F 31.0 NaN 22 23 7 17 1977 13 DM M 36.0 NaN 23 24 7 17 1977 13 SH M 21.0 NaN 24 25 7 17 1977 9 DM M 35.0 NaN 25 26 7 17 1977 15 DM M 31.0 NaN 26 27 7 17 1977 15 DM M 36.0 NaN 27 28 7 17 1977 11 DM M 38.0 NaN 28 29 7 17 1977 11 PP M NaN NaN 29 30 7 17 1977 10 DS F 52.0 NaN ... ... ... ... ... ... ... ... ... ... 35519 35520 12 31 2002 9 SF NaN 24.0 36.0 35520 35521 12 31 2002 9 DM M 37.0 48.0 35521 35522 12 31 2002 9 DM F 35.0 45.0 35522 35523 12 31 2002 9 DM F 36.0 44.0 35523 35524 12 31 2002 9 PB F 25.0 27.0 35524 35525 12 31 2002 9 OL M 21.0 26.0 35525 35526 12 31 2002 8 OT F 20.0 24.0 35526 35527 12 31 2002 13 DO F 33.0 43.0 35527 35528 12 31 2002 13 US NaN NaN NaN 35528 35529 12 31 2002 13 PB F 25.0 25.0 35529 35530 12 31 2002 13 OT F 20.0 NaN 35530 35531 12 31 2002 13 PB F 27.0 NaN 35531 35532 12 31 2002 14 DM F 34.0 43.0 35532 35533 12 31 2002 14 DM F 36.0 48.0 35533 35534 12 31 2002 14 DM M 37.0 56.0 35534 35535 12 31 2002 14 DM M 37.0 53.0 35535 35536 12 31 2002 14 DM F 35.0 42.0 35536 35537 12 31 2002 14 DM F 36.0 46.0 35537 35538 12 31 2002 15 PB F 26.0 31.0 35538 35539 12 31 2002 15 SF M 26.0 68.0 35539 35540 12 31 2002 15 PB F 26.0 23.0 35540 35541 12 31 2002 15 PB F 24.0 31.0 35541 35542 12 31 2002 15 PB F 26.0 29.0 35542 35543 12 31 2002 15 PB F 27.0 34.0 35543 35544 12 31 2002 15 US NaN NaN NaN 35544 35545 12 31 2002 15 AH NaN NaN NaN 35545 35546 12 31 2002 15 AH NaN NaN NaN 35546 35547 12 31 2002 10 RM F 15.0 14.0 35547 35548 12 31 2002 7 DO M 36.0 51.0 35548 35549 12 31 2002 5 NaN NaN NaN NaN 35549 rows \u00d7 9 columns which prints contents like above. You can also select just a few rows, so it is easier to fit on one window, you can see that pandas has neatly formatted the data to fit our screen. Here, we will be using a function called head . The head() function displays the first several lines of a file. It is discussed below. surveys_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN Exploring Our Species Survey Data Again, we can use the type function to see what kind of thing surveys_df is: type(surveys_df) output pandas.core.frame.DataFrame As expected, it's a DataFrame (or, to use the full name that Python uses to refer to it internally, a pandas.core.frame.DataFrame ). What kind of things does surveys_df contain? DataFrames have an attribute called dtypes that answers this: surveys_df.dtypes output record_id int64 month int64 day int64 year int64 site_id int64 species_id object sex object hindfoot_length float64 weight float64 dtype: object All the values in a single column have the same type. For example, months have type int64 , which is a kind of integer. Cells in the month column cannot have fractional values, but the weight and hindfoot_length columns can, because they have type float64 . The object type doesn't have a very helpful name, but in this case it represents strings (such as 'M' and 'F' in the case of sex). Useful Ways to View DataFrame objects in Python There are many ways to summarize and access the data stored in DataFrames, using attributes and methods provided by the DataFrame object. To access an attribute, use the DataFrame object name followed by the attribute name df_object.attribute . Using the DataFrame surveys_df and attribute columns , an index of all the column names in the DataFrame can be accessed with surveys_df.columns . Methods are called in a similar fashion using the syntax df_object.method() . As an example, surveys_df.head() gets the first few rows in the DataFrame surveys_df using the head() method . With a method, we can supply extra information in the parens to control behaviour. Let's look at the data using these. Challenge - DataFrames Using our DataFrame surveys_df , try out the attributes & methods below to see what they return. surveys_df.columns surveys_df.shape Take note of the output of shape - what format does it return the shape of the DataFrame in? HINT: More on tuples, here . surveys_df.head() Also, what does surveys_df.head(15) do? surveys_df.tail() Calculating Statistics From Data We've read our data into Python. Next, let's perform some quick summary statistics to learn more about the data that we're working with. We might want to know how many animals were collected in each plot, or how many of each species were caught. We can perform summary stats quickly using groups. But first we need to figure out what we want to group by. Let's begin by exploring our data: # Look at the column names surveys_df.columns output Index(['record_id', 'month', 'day', 'year', 'site_id', 'species_id', 'sex', 'hindfoot_length', 'weight'], dtype='object') Let's get a list of all the species. The pd.unique function tells us all of the unique values in the species_id column. pd.unique(surveys_df['species_id']) output array(['NL', 'DM', 'PF', 'PE', 'DS', 'PP', 'SH', 'OT', 'DO', 'OX', 'SS', 'OL', 'RM', nan, 'SA', 'PM', 'AH', 'DX', 'AB', 'CB', 'CM', 'CQ', 'RF', 'PC', 'PG', 'PH', 'PU', 'CV', 'UR', 'UP', 'ZL', 'UL', 'CS', 'SC', 'BA', 'SF', 'RO', 'AS', 'SO', 'PI', 'ST', 'CU', 'SU', 'RX', 'PB', 'PL', 'PX', 'CT', 'US'], dtype=object) Challenge - Statistics Create a list of unique site ID's found in the surveys data. Call it site_names . How many unique sites are there in the data? How many unique species are in the data? What is the difference between len(site_names) and surveys_df['site_id'].nunique() ? Groups in Pandas We often want to calculate summary statistics grouped by subsets or attributes within fields of our data. For example, we might want to calculate the average weight of all individuals per site. We can calculate basic statistics for all records in a single column using the syntax below: surveys_df['weight'].describe() output count 32283.000000 mean 42.672428 std 36.631259 min 4.000000 25% 20.000000 50% 37.000000 75% 48.000000 max 280.000000 Name: weight, dtype: float64 We can also extract one specific metric if we wish: surveys_df['weight'].min() surveys_df['weight'].max() surveys_df['weight'].mean() surveys_df['weight'].std() # only the last command shows output below - you can try the others above in new cells surveys_df['weight'].count() output 32283 But if we want to summarize by one or more variables, for example sex, we can use Pandas' .groupby method . Once we've created a groupby DataFrame, we can quickly calculate summary statistics by a group of our choice. # Group data by sex grouped_data = surveys_df.groupby('sex') The pandas function describe will return descriptive stats including: mean, median, max, min, std and count for a particular column in the data. Note Pandas' describe function will only return summary values for columns containing numeric data. # Summary statistics for all numeric columns by sex grouped_data.describe() # Provide the mean for each numeric column by sex # As above, only the last command shows output below - you can try the others above in new cells grouped_data.mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id hindfoot_length weight sex F 18036.412046 6.583047 16.007138 1990.644997 11.440854 28.836780 42.170555 M 17754.835601 6.392668 16.184286 1990.480401 11.098282 29.709578 42.995379 The groupby command is powerful in that it allows us to quickly generate summary stats. Challenge - Summary Data How many recorded individuals are female F and how many male M A) 17348 and 15690 B) 14894 and 16476 C) 15303 and 16879 D) 15690 and 17348 What happens when you group by two columns using the following syntax and then grab mean values: grouped_data2 = surveys_df.groupby(['site_id','sex']) grouped_data2.mean() Summarize weight values for each site in your data. HINT: you can use the following syntax to only create summary statistics for one column in your data by_site['weight'].describe() Quickly Creating Summary Counts in Pandas Let's next count the number of samples for each species. We can do this in a few ways, but we'll use groupby combined with a count() method . # Count the number of samples by species species_counts = surveys_df.groupby('species_id')['record_id'].count() print(species_counts) output species_id AB 303 AH 437 AS 2 BA 46 CB 50 CM 13 CQ 16 CS 1 CT 1 CU 1 CV 1 DM 10596 DO 3027 DS 2504 DX 40 NL 1252 OL 1006 OT 2249 OX 12 PB 2891 PC 39 PE 1299 PF 1597 PG 8 PH 32 PI 9 PL 36 PM 899 PP 3123 PU 5 PX 6 RF 75 RM 2609 RO 8 RX 2 SA 75 SC 1 SF 43 SH 147 SO 43 SS 248 ST 1 SU 5 UL 4 UP 8 UR 10 US 4 ZL 2 Name: record_id, dtype: int64 Or, we can also count just the rows that have the species \"DO\": surveys_df.groupby('species_id')['record_id'].count()['DO'] output 3027 Basic Math Functions If we wanted to, we could perform math on an entire column of our data. For example let's multiply all weight values by 2. A more practical use of this might be to normalize the data according to a mean, area, or some other value calculated from our data. # Multiply all weight values by 2 but does not change the original weight data surveys_df['weight']*2 output 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN ... 35544 NaN 35545 NaN 35546 28.0 35547 102.0 35548 NaN Name: weight, Length: 35549, dtype: float64 Quick & Easy Plotting Data Using Pandas We can plot our summary stats using Pandas, too. ## To make sure figures appear inside Jupyter Notebook %matplotlib inline # Create a quick bar chart species_counts.plot(kind='bar') output Animals per site plot We can also look at how many animals were captured in each site. total_count = surveys_df.groupby('site_id')['record_id'].nunique() # Let's plot that too total_count.plot(kind='bar') output Extra Plotting Challenge Create a plot of average weight across all species per plot. Create a plot of total males versus total females for the entire dataset. Create a stacked bar plot, with weight on the Y axis, and the stacked variable being sex. The plot should show total weight by sex for each plot. Some tips are below to help you solve this challenge: For more on Pandas plots, visit this link.","title":"Working with Data"},{"location":"modules/working_with_data/#data-analysis-with-python","text":"","title":"Data Analysis with Python"},{"location":"modules/working_with_data/#automating-data-analysis-tasks-in-python","text":"We can automate the process of performing data manipulations in Python. It's efficient to spend time building the code to perform these tasks because once it's built, we can use it over and over on different datasets that use a similar format. This makes our methods easily reproducible. We can also easily share our code with colleagues and they can replicate the same analysis.","title":"Automating data analysis tasks in Python"},{"location":"modules/working_with_data/#the-dataset","text":"For this lesson, we will be using the Portal Teaching data, a subset of the data from Ernst et al Long-term monitoring and experimental manipulation of a Chihuahuan Desert ecosystem near Portal, Arizona, USA We will be using this dataset, which can be downloaded here: surveys.csv ... but don't click to download it in your browser - we are going to use Python ! import urllib.request # You can also get this URL value by right-clicking the `surveys.csv` link above and selecting \"Copy Link Address\" url = 'https://monashdatafluency.github.io/python-workshop-base/modules/data/surveys.csv' # url = 'https://goo.gl/9ZxqBg' # or a shortened version to save typing urllib.request.urlretrieve(url, 'surveys.csv') output ('surveys.csv', ) If Jupyter is running locally on your computer, you'll now have a file surveys.csv in the current working directory. You can check by clicking on File tab on the top left of the notebook to see if the file exists. If you are running Jupyter on a remote server or cloud service (eg Colaboratory or Azure Notebooks), the file will be there instead. We are studying the species and weight of animals caught in plots in our study area. The dataset is stored as a .csv file: each row holds information for a single animal, and the columns represent: Column Description record_id Unique id for the observation month month of observation day day of observation year year of observation site_id ID of a particular plot species_id 2-letter code sex sex of animal (\"M\", \"F\") hindfoot_length length of the hindfoot in mm weight weight of the animal in grams The first few rows of our file look like this: record_id,month,day,year,site_id,species_id,sex,hindfoot_length,weight 1,7,16,1977,2,NL,M,32, 2,7,16,1977,3,NL,M,33, 3,7,16,1977,2,DM,F,37, 4,7,16,1977,7,DM,M,36, 5,7,16,1977,3,DM,M,35, 6,7,16,1977,1,PF,M,14, 7,7,16,1977,2,PE,F,, 8,7,16,1977,1,DM,M,37, 9,7,16,1977,1,DM,F,34,","title":"The Dataset"},{"location":"modules/working_with_data/#about-libraries","text":"A library in Python contains a set of tools (called functions) that perform tasks on our data. Importing a library is like getting a piece of lab equipment out of a storage locker and setting it up on the bench for use in a project. Once a library is set up, it can be used or called to perform many tasks. If you have noticed in the previous code import urllib.request , we are calling a request function from library urllib to download our dataset from web.","title":"About Libraries"},{"location":"modules/working_with_data/#pandas-in-python","text":"The dataset we have, is in table format. One of the best options for working with tabular data in Python is to use the Python Data Analysis Library (a.k.a. Pandas). The Pandas library provides data structures, produces high quality plots with matplotlib and integrates nicely with other libraries that use NumPy (which is another Python library) arrays. First, lets make sure the Pandas and matplotlib packages are installed . !pip install pandas matplotlib output Requirement already satisfied: pandas in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (0.25.0) Requirement already satisfied: matplotlib in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (3.1.1) Requirement already satisfied: python-dateutil>=2.6.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2.8.0) Requirement already satisfied: pytz>=2017.2 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (2019.1) Requirement already satisfied: numpy>=1.13.3 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from pandas) (1.17.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (2.4.1.1) Requirement already satisfied: cycler>=0.10 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from matplotlib) (1.1.0) Requirement already satisfied: six>=1.5 in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0) Requirement already satisfied: setuptools in /Users/perry/.virtualenvs/python-workshop-base-ufuVBSbV/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0) Python doesn't load all of the libraries available to it by default. We have to add an import statement to our code in order to use library functions. To import a library, we use the syntax import libraryName . If we want to give the library a nickname to shorten the command, we can add as nickNameHere . An example of importing the pandas library using the common nickname pd is below. import pandas as pd Each time we call a function that's in a library, we use the syntax LibraryName.FunctionName . Adding the library name with a . before the function name tells Python where to find the function. In the example above, we have imported Pandas as pd . This means we don't have to type out pandas each time we call a Pandas function.","title":"Pandas in Python"},{"location":"modules/working_with_data/#reading-csv-data-using-pandas","text":"We will begin by locating and reading our survey data which are in CSV format. CSV stands for Comma-Separated Values and is a common way store formatted data. Other symbols my also be used, so you might see tab-separated, colon-separated or space separated files. It is quite easy to replace one separator with another, to match your application. The first line in the file often has headers to explain what is in each column. CSV (and other separators) make it easy to share data, and can be imported and exported from many applications, including Microsoft Excel. We can use Pandas' read_csv function to pull the file directly into a DataFrame .","title":"Reading CSV Data Using Pandas"},{"location":"modules/working_with_data/#so-whats-a-dataframe","text":"A DataFrame is a 2-dimensional data structure that can store data of different types (including characters, integers, floating point values, factors and more) in columns. It is similar to a spreadsheet or an SQL table or the data.frame in R. A DataFrame always has an index (0-based). An index refers to the position of an element in the data structure. # Note that pd.read_csv is used because we imported pandas as pd pd.read_csv(\"surveys.csv\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN 5 6 7 16 1977 1 PF M 14.0 NaN 6 7 7 16 1977 2 PE F NaN NaN 7 8 7 16 1977 1 DM M 37.0 NaN 8 9 7 16 1977 1 DM F 34.0 NaN 9 10 7 16 1977 6 PF F 20.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN 11 12 7 16 1977 7 DM M 38.0 NaN 12 13 7 16 1977 3 DM M 35.0 NaN 13 14 7 16 1977 8 DM NaN NaN NaN 14 15 7 16 1977 6 DM F 36.0 NaN 15 16 7 16 1977 4 DM F 36.0 NaN 16 17 7 16 1977 3 DS F 48.0 NaN 17 18 7 16 1977 2 PP M 22.0 NaN 18 19 7 16 1977 4 PF NaN NaN NaN 19 20 7 17 1977 11 DS F 48.0 NaN 20 21 7 17 1977 14 DM F 34.0 NaN 21 22 7 17 1977 15 NL F 31.0 NaN 22 23 7 17 1977 13 DM M 36.0 NaN 23 24 7 17 1977 13 SH M 21.0 NaN 24 25 7 17 1977 9 DM M 35.0 NaN 25 26 7 17 1977 15 DM M 31.0 NaN 26 27 7 17 1977 15 DM M 36.0 NaN 27 28 7 17 1977 11 DM M 38.0 NaN 28 29 7 17 1977 11 PP M NaN NaN 29 30 7 17 1977 10 DS F 52.0 NaN ... ... ... ... ... ... ... ... ... ... 35519 35520 12 31 2002 9 SF NaN 24.0 36.0 35520 35521 12 31 2002 9 DM M 37.0 48.0 35521 35522 12 31 2002 9 DM F 35.0 45.0 35522 35523 12 31 2002 9 DM F 36.0 44.0 35523 35524 12 31 2002 9 PB F 25.0 27.0 35524 35525 12 31 2002 9 OL M 21.0 26.0 35525 35526 12 31 2002 8 OT F 20.0 24.0 35526 35527 12 31 2002 13 DO F 33.0 43.0 35527 35528 12 31 2002 13 US NaN NaN NaN 35528 35529 12 31 2002 13 PB F 25.0 25.0 35529 35530 12 31 2002 13 OT F 20.0 NaN 35530 35531 12 31 2002 13 PB F 27.0 NaN 35531 35532 12 31 2002 14 DM F 34.0 43.0 35532 35533 12 31 2002 14 DM F 36.0 48.0 35533 35534 12 31 2002 14 DM M 37.0 56.0 35534 35535 12 31 2002 14 DM M 37.0 53.0 35535 35536 12 31 2002 14 DM F 35.0 42.0 35536 35537 12 31 2002 14 DM F 36.0 46.0 35537 35538 12 31 2002 15 PB F 26.0 31.0 35538 35539 12 31 2002 15 SF M 26.0 68.0 35539 35540 12 31 2002 15 PB F 26.0 23.0 35540 35541 12 31 2002 15 PB F 24.0 31.0 35541 35542 12 31 2002 15 PB F 26.0 29.0 35542 35543 12 31 2002 15 PB F 27.0 34.0 35543 35544 12 31 2002 15 US NaN NaN NaN 35544 35545 12 31 2002 15 AH NaN NaN NaN 35545 35546 12 31 2002 15 AH NaN NaN NaN 35546 35547 12 31 2002 10 RM F 15.0 14.0 35547 35548 12 31 2002 7 DO M 36.0 51.0 35548 35549 12 31 2002 5 NaN NaN NaN NaN 35549 rows \u00d7 9 columns The above command outputs a DateFrame object, which Jupyter displays as a table (snipped in the middle since there are many rows). We can see that there were 33,549 rows parsed. Each row has 9 columns. The first column is the index of the DataFrame. The index is used to identify the position of the data, but it is not an actual column of the DataFrame. It looks like the read_csv function in Pandas read our file properly. However, we haven't saved any data to memory so we can work with it.We need to assign the DataFrame to a variable. Remember that a variable is a name for a value, such as x , or data . We can create a new object with a variable name by assigning a value to it using = . Let's call the imported survey data surveys_df : surveys_df = pd.read_csv(\"surveys.csv\") Notice when you assign the imported DataFrame to a variable, Python does not produce any output on the screen. We can view the value of the surveys_df object by typing its name into the cell. surveys_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN 5 6 7 16 1977 1 PF M 14.0 NaN 6 7 7 16 1977 2 PE F NaN NaN 7 8 7 16 1977 1 DM M 37.0 NaN 8 9 7 16 1977 1 DM F 34.0 NaN 9 10 7 16 1977 6 PF F 20.0 NaN 10 11 7 16 1977 5 DS F 53.0 NaN 11 12 7 16 1977 7 DM M 38.0 NaN 12 13 7 16 1977 3 DM M 35.0 NaN 13 14 7 16 1977 8 DM NaN NaN NaN 14 15 7 16 1977 6 DM F 36.0 NaN 15 16 7 16 1977 4 DM F 36.0 NaN 16 17 7 16 1977 3 DS F 48.0 NaN 17 18 7 16 1977 2 PP M 22.0 NaN 18 19 7 16 1977 4 PF NaN NaN NaN 19 20 7 17 1977 11 DS F 48.0 NaN 20 21 7 17 1977 14 DM F 34.0 NaN 21 22 7 17 1977 15 NL F 31.0 NaN 22 23 7 17 1977 13 DM M 36.0 NaN 23 24 7 17 1977 13 SH M 21.0 NaN 24 25 7 17 1977 9 DM M 35.0 NaN 25 26 7 17 1977 15 DM M 31.0 NaN 26 27 7 17 1977 15 DM M 36.0 NaN 27 28 7 17 1977 11 DM M 38.0 NaN 28 29 7 17 1977 11 PP M NaN NaN 29 30 7 17 1977 10 DS F 52.0 NaN ... ... ... ... ... ... ... ... ... ... 35519 35520 12 31 2002 9 SF NaN 24.0 36.0 35520 35521 12 31 2002 9 DM M 37.0 48.0 35521 35522 12 31 2002 9 DM F 35.0 45.0 35522 35523 12 31 2002 9 DM F 36.0 44.0 35523 35524 12 31 2002 9 PB F 25.0 27.0 35524 35525 12 31 2002 9 OL M 21.0 26.0 35525 35526 12 31 2002 8 OT F 20.0 24.0 35526 35527 12 31 2002 13 DO F 33.0 43.0 35527 35528 12 31 2002 13 US NaN NaN NaN 35528 35529 12 31 2002 13 PB F 25.0 25.0 35529 35530 12 31 2002 13 OT F 20.0 NaN 35530 35531 12 31 2002 13 PB F 27.0 NaN 35531 35532 12 31 2002 14 DM F 34.0 43.0 35532 35533 12 31 2002 14 DM F 36.0 48.0 35533 35534 12 31 2002 14 DM M 37.0 56.0 35534 35535 12 31 2002 14 DM M 37.0 53.0 35535 35536 12 31 2002 14 DM F 35.0 42.0 35536 35537 12 31 2002 14 DM F 36.0 46.0 35537 35538 12 31 2002 15 PB F 26.0 31.0 35538 35539 12 31 2002 15 SF M 26.0 68.0 35539 35540 12 31 2002 15 PB F 26.0 23.0 35540 35541 12 31 2002 15 PB F 24.0 31.0 35541 35542 12 31 2002 15 PB F 26.0 29.0 35542 35543 12 31 2002 15 PB F 27.0 34.0 35543 35544 12 31 2002 15 US NaN NaN NaN 35544 35545 12 31 2002 15 AH NaN NaN NaN 35545 35546 12 31 2002 15 AH NaN NaN NaN 35546 35547 12 31 2002 10 RM F 15.0 14.0 35547 35548 12 31 2002 7 DO M 36.0 51.0 35548 35549 12 31 2002 5 NaN NaN NaN NaN 35549 rows \u00d7 9 columns which prints contents like above. You can also select just a few rows, so it is easier to fit on one window, you can see that pandas has neatly formatted the data to fit our screen. Here, we will be using a function called head . The head() function displays the first several lines of a file. It is discussed below. surveys_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id species_id sex hindfoot_length weight 0 1 7 16 1977 2 NL M 32.0 NaN 1 2 7 16 1977 3 NL M 33.0 NaN 2 3 7 16 1977 2 DM F 37.0 NaN 3 4 7 16 1977 7 DM M 36.0 NaN 4 5 7 16 1977 3 DM M 35.0 NaN","title":"So What's a DataFrame?"},{"location":"modules/working_with_data/#exploring-our-species-survey-data","text":"Again, we can use the type function to see what kind of thing surveys_df is: type(surveys_df) output pandas.core.frame.DataFrame As expected, it's a DataFrame (or, to use the full name that Python uses to refer to it internally, a pandas.core.frame.DataFrame ). What kind of things does surveys_df contain? DataFrames have an attribute called dtypes that answers this: surveys_df.dtypes output record_id int64 month int64 day int64 year int64 site_id int64 species_id object sex object hindfoot_length float64 weight float64 dtype: object All the values in a single column have the same type. For example, months have type int64 , which is a kind of integer. Cells in the month column cannot have fractional values, but the weight and hindfoot_length columns can, because they have type float64 . The object type doesn't have a very helpful name, but in this case it represents strings (such as 'M' and 'F' in the case of sex).","title":"Exploring Our Species Survey Data"},{"location":"modules/working_with_data/#useful-ways-to-view-dataframe-objects-in-python","text":"There are many ways to summarize and access the data stored in DataFrames, using attributes and methods provided by the DataFrame object. To access an attribute, use the DataFrame object name followed by the attribute name df_object.attribute . Using the DataFrame surveys_df and attribute columns , an index of all the column names in the DataFrame can be accessed with surveys_df.columns . Methods are called in a similar fashion using the syntax df_object.method() . As an example, surveys_df.head() gets the first few rows in the DataFrame surveys_df using the head() method . With a method, we can supply extra information in the parens to control behaviour. Let's look at the data using these.","title":"Useful Ways to View DataFrame objects in Python"},{"location":"modules/working_with_data/#challenge-dataframes","text":"Using our DataFrame surveys_df , try out the attributes & methods below to see what they return. surveys_df.columns surveys_df.shape Take note of the output of shape - what format does it return the shape of the DataFrame in? HINT: More on tuples, here . surveys_df.head() Also, what does surveys_df.head(15) do? surveys_df.tail()","title":"Challenge - DataFrames"},{"location":"modules/working_with_data/#calculating-statistics-from-data","text":"We've read our data into Python. Next, let's perform some quick summary statistics to learn more about the data that we're working with. We might want to know how many animals were collected in each plot, or how many of each species were caught. We can perform summary stats quickly using groups. But first we need to figure out what we want to group by. Let's begin by exploring our data: # Look at the column names surveys_df.columns output Index(['record_id', 'month', 'day', 'year', 'site_id', 'species_id', 'sex', 'hindfoot_length', 'weight'], dtype='object') Let's get a list of all the species. The pd.unique function tells us all of the unique values in the species_id column. pd.unique(surveys_df['species_id']) output array(['NL', 'DM', 'PF', 'PE', 'DS', 'PP', 'SH', 'OT', 'DO', 'OX', 'SS', 'OL', 'RM', nan, 'SA', 'PM', 'AH', 'DX', 'AB', 'CB', 'CM', 'CQ', 'RF', 'PC', 'PG', 'PH', 'PU', 'CV', 'UR', 'UP', 'ZL', 'UL', 'CS', 'SC', 'BA', 'SF', 'RO', 'AS', 'SO', 'PI', 'ST', 'CU', 'SU', 'RX', 'PB', 'PL', 'PX', 'CT', 'US'], dtype=object)","title":"Calculating Statistics From Data"},{"location":"modules/working_with_data/#challenge-statistics","text":"Create a list of unique site ID's found in the surveys data. Call it site_names . How many unique sites are there in the data? How many unique species are in the data? What is the difference between len(site_names) and surveys_df['site_id'].nunique() ?","title":"Challenge - Statistics"},{"location":"modules/working_with_data/#groups-in-pandas","text":"We often want to calculate summary statistics grouped by subsets or attributes within fields of our data. For example, we might want to calculate the average weight of all individuals per site. We can calculate basic statistics for all records in a single column using the syntax below: surveys_df['weight'].describe() output count 32283.000000 mean 42.672428 std 36.631259 min 4.000000 25% 20.000000 50% 37.000000 75% 48.000000 max 280.000000 Name: weight, dtype: float64 We can also extract one specific metric if we wish: surveys_df['weight'].min() surveys_df['weight'].max() surveys_df['weight'].mean() surveys_df['weight'].std() # only the last command shows output below - you can try the others above in new cells surveys_df['weight'].count() output 32283 But if we want to summarize by one or more variables, for example sex, we can use Pandas' .groupby method . Once we've created a groupby DataFrame, we can quickly calculate summary statistics by a group of our choice. # Group data by sex grouped_data = surveys_df.groupby('sex') The pandas function describe will return descriptive stats including: mean, median, max, min, std and count for a particular column in the data. Note Pandas' describe function will only return summary values for columns containing numeric data. # Summary statistics for all numeric columns by sex grouped_data.describe() # Provide the mean for each numeric column by sex # As above, only the last command shows output below - you can try the others above in new cells grouped_data.mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } record_id month day year site_id hindfoot_length weight sex F 18036.412046 6.583047 16.007138 1990.644997 11.440854 28.836780 42.170555 M 17754.835601 6.392668 16.184286 1990.480401 11.098282 29.709578 42.995379 The groupby command is powerful in that it allows us to quickly generate summary stats.","title":"Groups in Pandas"},{"location":"modules/working_with_data/#challenge-summary-data","text":"How many recorded individuals are female F and how many male M A) 17348 and 15690 B) 14894 and 16476 C) 15303 and 16879 D) 15690 and 17348 What happens when you group by two columns using the following syntax and then grab mean values: grouped_data2 = surveys_df.groupby(['site_id','sex']) grouped_data2.mean() Summarize weight values for each site in your data. HINT: you can use the following syntax to only create summary statistics for one column in your data by_site['weight'].describe()","title":"Challenge - Summary Data"},{"location":"modules/working_with_data/#quickly-creating-summary-counts-in-pandas","text":"Let's next count the number of samples for each species. We can do this in a few ways, but we'll use groupby combined with a count() method . # Count the number of samples by species species_counts = surveys_df.groupby('species_id')['record_id'].count() print(species_counts) output species_id AB 303 AH 437 AS 2 BA 46 CB 50 CM 13 CQ 16 CS 1 CT 1 CU 1 CV 1 DM 10596 DO 3027 DS 2504 DX 40 NL 1252 OL 1006 OT 2249 OX 12 PB 2891 PC 39 PE 1299 PF 1597 PG 8 PH 32 PI 9 PL 36 PM 899 PP 3123 PU 5 PX 6 RF 75 RM 2609 RO 8 RX 2 SA 75 SC 1 SF 43 SH 147 SO 43 SS 248 ST 1 SU 5 UL 4 UP 8 UR 10 US 4 ZL 2 Name: record_id, dtype: int64 Or, we can also count just the rows that have the species \"DO\": surveys_df.groupby('species_id')['record_id'].count()['DO'] output 3027","title":"Quickly Creating Summary Counts in Pandas"},{"location":"modules/working_with_data/#basic-math-functions","text":"If we wanted to, we could perform math on an entire column of our data. For example let's multiply all weight values by 2. A more practical use of this might be to normalize the data according to a mean, area, or some other value calculated from our data. # Multiply all weight values by 2 but does not change the original weight data surveys_df['weight']*2 output 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN ... 35544 NaN 35545 NaN 35546 28.0 35547 102.0 35548 NaN Name: weight, Length: 35549, dtype: float64","title":"Basic Math Functions"},{"location":"modules/working_with_data/#quick-easy-plotting-data-using-pandas","text":"We can plot our summary stats using Pandas, too. ## To make sure figures appear inside Jupyter Notebook %matplotlib inline # Create a quick bar chart species_counts.plot(kind='bar') output","title":"Quick &amp; Easy Plotting Data Using Pandas"},{"location":"modules/working_with_data/#animals-per-site-plot","text":"We can also look at how many animals were captured in each site. total_count = surveys_df.groupby('site_id')['record_id'].nunique() # Let's plot that too total_count.plot(kind='bar') output","title":"Animals per site plot"},{"location":"modules/working_with_data/#extra-plotting-challenge","text":"Create a plot of average weight across all species per plot. Create a plot of total males versus total females for the entire dataset. Create a stacked bar plot, with weight on the Y axis, and the stacked variable being sex. The plot should show total weight by sex for each plot. Some tips are below to help you solve this challenge: For more on Pandas plots, visit this link.","title":"Extra Plotting Challenge"},{"location":"modules/notebooks/wip/","text":"These are work-in-progress modules that don't yet get rendered to Markdown or the main site. Once these modules take shape they can be moved to workshops/docs/modules/notebooks and integrated.","title":"Home"}]}